{
  "hash": "7db1134571cb055ec5e3b24b690dc272",
  "result": {
    "markdown": "# Geographic data I/O {#read-write}\n\n## Prerequisites\n\nThis chapter requires the following packages, used in previous chapters:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport fiona\nimport geopandas as gpd\nimport shapely\nimport rasterio\n```\n:::\n\n\n\n\n## Introduction\n\nThis chapter is about reading and writing geographic data.\nGeographic data import is essential for geocomputation: real-world applications are impossible without data. \nData output is also vital, enabling others to use valuable new or improved datasets resulting from your work. \nTaken together, these processes of import/output can be referred to as data I/O.\n\nGeographic data I/O is often done with few lines of code at the beginning and end of projects. \nIt is often overlooked as a simple one step process. \nHowever, mistakes made at the outset of projects (e.g. using an out-of-date or in some way faulty dataset) can lead to large problems later down the line, so it is worth putting considerable time into identifying which datasets are available, where they can be found and how to retrieve them. \nThese topics are covered in @sec-retrieving-open-data, which describes various geoportals, which collectively contain many terabytes of data, and how to use them. \nTo further ease data access, a number of packages for downloading geographic data have been developed, as described in @sec-geographic-data-packages.\n\nThere are many geographic file formats, each of which has pros and cons, described in @sec-file-formats. \nThe process of reading and writing files in formats efficiently is covered in Sections @sec-data-input and @sec-data-output, respectively. \nThe final Section @sec-visual-outputs demonstrates methods for saving visual outputs (maps), in preparation for @sec-map-making on visualization.\n\n## Retrieving open data {#sec-retrieving-open-data}\n\n## Geographic data packages {#sec-geographic-data-packages}\n\n## Geographic web services\n\n## File formats {#sec-file-formats}\n\nGeographic datasets are usually stored as files or in spatial databases. \nFile formats can either store vector or raster data, while spatial databases such as [PostGIS](https://postgis.net/) can store both. \nThe large variety of file formats may seem bewildering, but there has been much consolidation and standardization since the beginnings of GIS software in the 1960s when the first widely distributed program ([SYMAP](https://news.harvard.edu/gazette/story/2011/10/the-invention-of-gis/)) for spatial analysis was created at Harvard University [@coppock_history_1991].\n\nGDAL (which should be pronounced \"goo-dal\", with the double \"o\" making a reference to object-orientation), the Geospatial Data Abstraction Library, has resolved many issues associated with incompatibility between geographic file formats since its release in 2000. \nGDAL provides a unified and high-performance interface for reading and writing of many raster and vector data formats. \nMany open and proprietary GIS programs, including GRASS, ArcGIS and QGIS, use GDAL behind their GUIs for doing the legwork of ingesting and spitting out geographic data in appropriate formats.\n\nGDAL provides access to more than 200 vector and raster data formats. \n@tbl-file-formats presents some basic information about selected and often used spatial file formats.\n\nName  | Extension  | Info  | Type  | Model |\n|-----|----|----------|-----|-----|\nESRI Shapefile  | `.shp` (the main file)  | Popular format consisting of at least three files. No support for: files > 2GB;mixed types; names > 10 chars; cols > 255.  | Vector  | Partially open |\nGeoJSON  | `.geojson`  | Extends the JSON exchange format by including a subset of the simple feature representation; mostly used for storing coordinates in longitude and latitude; it is extended by the TopoJSON format  | Vector  | Open |\nKML  | `.kml`  | XML-based format for spatial visualization, developed for use with Google Earth. Zipped KML file forms the KMZ format.  | Vector  | Open |\nGPX  | `.gpx`  | XML schema created for exchange of GPS data.  | Vector  | Open |\nFlatGeobuf  | `.fgb`  | Single file format allowing for quick reading and writing of vector data. Has streaming capabilities.  | Vector  | Open |\nGeoTIFF  | `.tif/.tiff`  | Popular raster format. A TIFF file containing additional spatial metadata.  | Raster  | Open |\nArc ASCII  | `.asc`  | Text format where the first six lines represent the raster header, followed by the raster cell values arranged in rows and columns.  | Raster  | Open |\nSQLite/SpatiaLite  | `.sqlite`  | Standalone relational database, SpatiaLite is the spatial extension of SQLite.  | Vector and raster  | Open |\nESRI FileGDB  | `.gdb`  | Spatial and nonspatial objects created by ArcGIS. Allows: multiple feature classes; topology. Limited support from GDAL.  | Vector and raster  | Proprietary |\nGeoPackage  | `.gpkg`  | Lightweight database container based on SQLite allowing an easy and platform-independent exchange of geodata  | Vector and (very limited) raster  | Open |\n: Commonly used spatial data file formats {#tbl-file-formats}\n\nAn important development ensuring the standardization and open-sourcing of file formats was the founding of the Open Geospatial Consortium ([OGC](http://www.opengeospatial.org/)) in 1994. \nBeyond defining the simple features data model (see @sec-simple-features), the OGC also coordinates the development of open standards, for example as used in file formats such as KML and GeoPackage. \nOpen file formats of the kind endorsed by the OGC have several advantages over proprietary formats: the standards are published, ensure transparency and open up the possibility for users to further develop and adjust the file formats to their specific needs.\n\nESRI Shapefile is the most popular vector data exchange format; however, it is not an open format (though its specification is open). \nIt was developed in the early 1990s and has a number of limitations. \nFirst of all, it is a multi-file format, which consists of at least three files. \nIt only supports 255 columns, column names are restricted to ten characters and the file size limit is 2 GB. \nFurthermore, ESRI Shapefile does not support all possible geometry types, for example, it is unable to distinguish between a polygon and a multipolygon. \nDespite these limitations, a viable alternative had been missing for a long time. \nIn the meantime, [GeoPackage](https://www.geopackage.org/) emerged, and seems to be a more than suitable replacement candidate for ESRI Shapefile. \nGeoPackage is a format for exchanging geospatial information and an OGC standard. \nThe GeoPackage standard describes the rules on how to store geospatial information in a tiny SQLite container. \nHence, GeoPackage is a lightweight spatial database container, which allows the storage of vector and raster data but also of non-spatial data and extensions. \nAside from GeoPackage, there are other geospatial data exchange formats worth checking out (@tbl-file-formats).\n\nThe GeoTIFF format seems to be the most prominent raster data format. \nIt allows spatial information, such as the CRS definition and the transformation matrix (see @sec-using-rasterio), to be embedded within a TIFF file. \nSimilar to ESRI Shapefile, this format was firstly developed in the 1990s, but as an open format. \nAdditionally, GeoTIFF is still being expanded and improved. \nOne of the most significant recent addition to the GeoTIFF format is its variant called COG (Cloud Optimized GeoTIFF). \nRaster objects saved as COGs can be hosted on HTTP servers, so other people can read only parts of the file without downloading the whole file (see Sections 8.6.2 and 8.7.2...).\n\nThere is also a plethora of other spatial data formats that we do not explain in detail or mention in @tbl-file-formats due to the book limits. \nIf you need to use other formats, we encourage you to read the GDAL documentation about [vector](https://gdal.org/drivers/vector/index.html) and [raster](https://gdal.org/drivers/raster/index.html) drivers. \nAdditionally, some spatial data formats can store other data models (types) than vector or raster. \nIt includes LAS and LAZ formats for storing lidar point clouds, and NetCDF and HDF for storing multidimensional arrays.\n\nFinally, spatial data is also often stored using tabular (non-spatial) text formats, including CSV files or Excel spreadsheets. \nThis can be convenient to share spatial datasets with people who, or software that, struggle with spatial data formats.\n\n## Data input (I) {#sec-data-input}\n\nExecuting commands such as `geopandas.read_file` (the main function we use for loading vector data) or `rasterio.open`+`.read` (the main functions used for loading raster data) silently sets off a chain of events that reads data from files. \nMoreover, there are many Python packages containing a wide range of geographic data or providing simple access to different data sources. \nAll of them load the data into the Python environment or, more precisely, assign objects to your workspace, stored in RAM and accessible within the Python session.\n\n### Vector data\n\nSpatial vector data comes in a wide variety of file formats. \nMost popular representations such as `.shp`, `.geojson`, and `.gpkg` files can be imported and exported with `geopandas` functions `read_file` and `to_file` (covered in Section \\@ref(sec-data-output)), respectively.\n\n`geopandas` uses GDAL to read and write data, via `fiona` (the [default](https://github.com/geopandas/geopandas/issues/2217)) or `pyogrio` packages (a recently developed alternative to `fiona`). \nAfter `fiona` is imported, the command `fiona.supported_drivers` can be used to list drivers available to GDAL, including whether they can (`r`), append (`a`), or write (`w`) data, or all three:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfiona.supported_drivers\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n{'DXF': 'rw',\n 'CSV': 'raw',\n 'OpenFileGDB': 'raw',\n 'ESRIJSON': 'r',\n 'ESRI Shapefile': 'raw',\n 'FlatGeobuf': 'raw',\n 'GeoJSON': 'raw',\n 'GeoJSONSeq': 'raw',\n 'GPKG': 'raw',\n 'GML': 'rw',\n 'OGR_GMT': 'rw',\n 'GPX': 'rw',\n 'MapInfo File': 'raw',\n 'DGN': 'raw',\n 'S57': 'r',\n 'SQLite': 'raw',\n 'TopoJSON': 'r'}\n```\n:::\n:::\n\n\nOther, less common, drivers can be [\"activated\"](https://geopandas.org/en/stable/docs/user_guide/io.html) by manually supplementing `fiona.supported_drivers`.\nThe first argument of the `geopandas` versatile data import function `gpd.read_file` is `filename`, which is typically a string, but can also be a file connection.\nThe content of a string could vary between different drivers.\n In most cases, as with the ESRI Shapefile (`.shp`) or the GeoPackage format (`.gpkg`), the dsn would be a file name, such as `geodata.gpkg`.\n The driver is automatically selected based on the file extension, as demosntrated for a `.gpkg` file below:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ngpd.read_file('data/world.gpkg')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso_a2</th>\n      <th>name_long</th>\n      <th>continent</th>\n      <th>...</th>\n      <th>lifeExp</th>\n      <th>gdpPercap</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FJ</td>\n      <td>Fiji</td>\n      <td>Oceania</td>\n      <td>...</td>\n      <td>69.960000</td>\n      <td>8222.253784</td>\n      <td>MULTIPOLYGON (((-180.00000 -16....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TZ</td>\n      <td>Tanzania</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>64.163000</td>\n      <td>2402.099404</td>\n      <td>MULTIPOLYGON (((33.90371 -0.950...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EH</td>\n      <td>Western Sahara</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MULTIPOLYGON (((-8.66559 27.656...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>XK</td>\n      <td>Kosovo</td>\n      <td>Europe</td>\n      <td>...</td>\n      <td>71.097561</td>\n      <td>8698.291559</td>\n      <td>MULTIPOLYGON (((20.59025 41.855...</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>TT</td>\n      <td>Trinidad and Tobago</td>\n      <td>North America</td>\n      <td>...</td>\n      <td>70.426000</td>\n      <td>31181.821196</td>\n      <td>MULTIPOLYGON (((-61.68000 10.76...</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>SS</td>\n      <td>South Sudan</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>55.817000</td>\n      <td>1935.879400</td>\n      <td>MULTIPOLYGON (((30.83385 3.5091...</td>\n    </tr>\n  </tbody>\n</table>\n<p>177 rows × 11 columns</p>\n</div>\n```\n:::\n:::\n\n\nFor some drivers, such as a File Geodatabase (`OpenFileGDB`), `filename` could be provided as a folder name.\nGeoJSON string can also be read from a character string:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ngpd.read_file('{\"type\":\"Point\",\"coordinates\":[34.838848,31.296301]}')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>POINT (34.83885 31.29630)</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAlternatively, the `gpd.read_postgis` function can be used to read a vector layer from a PostGIS database.\n\nSome vector formats, such as GeoPackage, can store multiple data layers. \nBy default, `gpd.read_file` automatically reads the first layer of the file specified in `filename`. \nHowever, using the `layer` argument you can specify any other layer.\n\nThe `gpd.read_file` function also allows for reading just parts of the file into RAM with two possible mechanisms. \nThe first one is related to the `where` argument, which allows specifying what part of the data to read using an SQL `WHERE` expression. \nAn example below extracts data for Tanzania only (Figure ...). \nIt is done by specifying that we want to get all rows for which `name_long` equals to `\"Tanzania\"`:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntanzania = gpd.read_file('data/world.gpkg', where='name_long=\"Tanzania\"')\ntanzania\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso_a2</th>\n      <th>name_long</th>\n      <th>continent</th>\n      <th>...</th>\n      <th>lifeExp</th>\n      <th>gdpPercap</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TZ</td>\n      <td>Tanzania</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>64.163</td>\n      <td>2402.099404</td>\n      <td>MULTIPOLYGON (((33.90371 -0.950...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 11 columns</p>\n</div>\n```\n:::\n:::\n\n\nIf you do not know the names of the available columns, a good approach is to just read one row of the data using the `rows` argument, which can be used to read the first N rows, then use the `.columns` property to examine the column names:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ngpd.read_file('data/world.gpkg', rows=1).columns\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nIndex(['iso_a2', 'name_long', 'continent', 'region_un', 'subregion', 'type',\n       'area_km2', 'pop', 'lifeExp', 'gdpPercap', 'geometry'],\n      dtype='object')\n```\n:::\n:::\n\n\nThe second mechanism uses the `mask` argument to filter data based on intersection with an existing geometry. \nThis argument expects a geometry (`GeoDataFrame`, `GeoSeries`, or `shapely`) representing the area where we want to extract the data. \nLet's try it using a small example---we want to read polygons from our file that intersect with the buffer of 50,000 $m$ of Tanzania's borders. \nTo do it, we need to (a) transform the geometry to a projected CRS (such as EPSG:32736), (b) prepare our \"filter\" by creating the buffer (@sec-buffers), and (c) transform back to the original CRS to be used as a mask:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntanzania_buf = tanzania.to_crs(32736).buffer(50000).to_crs(4326)\ntanzania_buf.iloc[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](08-read-write-plot_files/figure-html/cell-9-output-1.svg){}\n:::\n:::\n\n\nNow, we can apply this \"filter\" using the `mask` argument.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntanzania_neigh = gpd.read_file('data/world.gpkg', mask=tanzania_buf)\ntanzania_neigh\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso_a2</th>\n      <th>name_long</th>\n      <th>continent</th>\n      <th>...</th>\n      <th>lifeExp</th>\n      <th>gdpPercap</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MZ</td>\n      <td>Mozambique</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>57.099</td>\n      <td>1079.823866</td>\n      <td>MULTIPOLYGON (((34.55999 -11.52...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ZM</td>\n      <td>Zambia</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>60.775</td>\n      <td>3632.503753</td>\n      <td>MULTIPOLYGON (((30.74001 -8.340...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MW</td>\n      <td>Malawi</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>61.932</td>\n      <td>1090.367208</td>\n      <td>MULTIPOLYGON (((32.75938 -9.230...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BI</td>\n      <td>Burundi</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>56.688</td>\n      <td>803.172837</td>\n      <td>MULTIPOLYGON (((30.46967 -2.413...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>UG</td>\n      <td>Uganda</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>59.224</td>\n      <td>1637.275081</td>\n      <td>MULTIPOLYGON (((33.90371 -0.950...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RW</td>\n      <td>Rwanda</td>\n      <td>Africa</td>\n      <td>...</td>\n      <td>66.188</td>\n      <td>1629.868866</td>\n      <td>MULTIPOLYGON (((30.41910 -1.134...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9 rows × 11 columns</p>\n</div>\n```\n:::\n:::\n\n\nOur result, shown in @fig-read-shp-query, contains Tanzania and every country within its 50,000 $m$ buffer. \nNote that the last two expressions are used to add text labels with the `name_long` of each country, placed at the country centroid:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\ntanzania.plot(ax=axes[0], color='lightgrey', edgecolor='grey')\ntanzania_neigh.plot(ax=axes[1], color='lightgrey', edgecolor='grey')\ntanzania_buf.plot(ax=axes[1], color='none', edgecolor='red')\naxes[0].set_title('where')\naxes[1].set_title('mask')\ntanzania.apply(lambda x: axes[0].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\ntanzania_neigh.apply(lambda x: axes[1].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\n```\n\n::: {.cell-output .cell-output-display}\n![Reading a subset of the vector data using a `where` query (left) and a `mask` (right)](08-read-write-plot_files/figure-html/fig-read-shp-query-output-1.png){#fig-read-shp-query width=726 height=394}\n:::\n:::\n\n\nOften we need to read CSV files (or other tabular formats) which have x and y coordinate columns, and turn them into a `GeoDataFrame` with point geometries. \nTo do that, we can import the file using `pandas` (e.g., `pd.read_csv` or `pd.read_excel`), then go from `DataFrame` to `GeoDataFrame` using the `gpd.points_from_xy` function, as shown earlier in the book (See @sec-vector-layer-from-scratch and @sec-spatial-joining). \nFor example, the table `cycle_hire_xy.csv`, where the coordinates are stored in the `X` and `Y` columns in EPSG:4326, can be imported, converted to a `GeoDataFrame`, and plotted, as follows:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ncycle_hire = pd.read_csv('data/cycle_hire_xy.csv')\ngeom = gpd.points_from_xy(cycle_hire['X'], cycle_hire['Y'], crs=4326)\ngeom = gpd.GeoSeries(geom)\ncycle_hire = gpd.GeoDataFrame(data=cycle_hire, geometry=geom)\ncycle_hire.plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](08-read-write-plot_files/figure-html/cell-12-output-1.png){width=440 height=264}\n:::\n:::\n\n\nInstead of columns describing 'XY' coordinates, a single column can also contain the geometry information. \nWell-known text (WKT), well-known binary (WKB), and the GeoJSON formats are examples of this. \nFor instance, the `world_wkt.csv` file has a column named WKT representing polygons of the world's countries. \nTo import and convert it to a `GeoDataFrame`, we can apply the `shapely.wkt.loads` function (@sec-geometries) on WKT strings, to convert them into `shapely` geometries:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nworld_wkt = pd.read_csv('data/world_wkt.csv')\nworld_wkt['geometry'] = world_wkt['WKT'].apply(shapely.wkt.loads)\nworld_wkt = gpd.GeoDataFrame(world_wkt)\nworld_wkt.plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](08-read-write-plot_files/figure-html/cell-13-output-1.png){width=429 height=221}\n:::\n:::\n\n\n::: {.callout-note}\nNot all of the supported vector file formats store information about their coordinate reference system. In these situations, it is possible to add the missing information using the `.set_crs` function. Please refer also to @sec-querying-and-setting-coordinate-systems for more information. \n:::\n\nAs a final example, we will show how `geopandas` also reads KML files. \nA KML file stores geographic information in XML format---a data format for the creation of web pages and the transfer of data in an application-independent way (Nolan and Lang 2014 ...). Here, we access a KML file from the web. First, we need to \"activate\" the `KML` driver, which isn't available by default (see above):\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfiona.supported_drivers['KML'] = 'r'\n```\n:::\n\n\nThis file contains more than one layer. To list the available layers, we can use the `fiona.listlayers` function: \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nu = 'https://developers.google.com/kml/documentation/KML_Samples.kml'\nfiona.listlayers(u)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n['Placemarks',\n 'Highlighted Icon',\n 'Paths',\n 'Google Campus',\n 'Extruded Polygon',\n 'Absolute and Relative']\n```\n:::\n:::\n\n\nFinally, we can choose the first layer `Placemarks` and read it, using `gpd.read_file` with an additional `layer` argument:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ngpd.read_file(u, layer='Placemarks')\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Description</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Simple placemark</td>\n      <td>Attached to the ground. Intelli...</td>\n      <td>POINT Z (-122.08220 37.42229 0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Floating placemark</td>\n      <td>Floats a defined distance above...</td>\n      <td>POINT Z (-122.08407 37.42200 50...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Extruded placemark</td>\n      <td>Tethered to the ground by a cus...</td>\n      <td>POINT Z (-122.08577 37.42157 50...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Raster data\n\nSimilar to vector data, raster data comes in many file formats with some of them supporting multilayer files. \n`rasterio.open` is used to create a file connection to a raster file, which can be subsequently used to read the metadata and/or the values, as shown previously (@sec-using-rasterio). \nFor example: \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nsrc = rasterio.open('data/srtm.tif')\nsrc\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<open DatasetReader name='data/srtm.tif' mode='r'>\n```\n:::\n:::\n\n\nAll of the previous examples read spatial information from files stored on your hard drive. \nHowever, GDAL also allows reading data directly from online resources, such as HTTP/HTTPS/FTP web resources. \nThe only thing we need to do is to add a `/vsicurl/` prefix before the path to the file. \nLet's try it by connecting to the global monthly snow probability at 500 m resolution for the period 2000-2012 (T. Hengl 2021 add reference...). \nSnow probability for December is stored as a Cloud Optimized GeoTIFF (COG) file (see @sec-file-formats). \nTo read an online file, we just need to provide its URL together with the `/vsicurl/` prefix:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nurl = \"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif\"\nsrc = rasterio.open(url)\nsrc\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n<open DatasetReader name='/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif' mode='r'>\n```\n:::\n:::\n\n\nIn the example above `rasterio.open` creates a connection to the file without obtaining any values, as we did for the local `srtm.tif` file.\nThe values can read, into an `ndarray`, using the `.read` method of the file connection (@sec-using-rasterio). \nThis allows us also to just read a small portion of the data without downloading the entire file. \nThis is very useful when working with large datasets hosted online from resource-constrained computing environments such as laptops.\n\nAnother option is to extract raster values at particular points, directly from the file connection, using the `.sample` method (see @sec-spatial-subsetting). \nFor example, we can get the snow probability for December in Reykjavik (70%) by specifying its coordinates and applying `.sample`:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nvalues = src.sample([(-21.94, 64.15)])\nlist(values)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n[array([70], dtype=uint8)]\n```\n:::\n:::\n\n\nThe example above efficiently extracts and downloads a single value instead of the entire GeoTIFF file, saving valuable resources.\nThe `/vsicurl/` prefix **also works for vector file formats**, enabling you to import datasets from online storage with `geopandas` just by adding it before the vector file URL.\n\nImportantly, `/vsicurl/` is not the only prefix provided by GDAL---many more exist, such as `/vsizip/` to read spatial files from ZIP archives without decompressing them beforehand or `/vsis3/` for on-the-fly reading files available in AWS S3 buckets. You can learn more about it at <https://gdal.org/user/virtual_file_systems.html>.\n\n## Data output (O) {#sec-data-output}\n\nWriting geographic data allows you to convert from one format to another and to save newly created objects for permanent storage. \nDepending on the data type (vector or raster), object class (e.g., `GeoDataFrame`), and type and amount of stored information (e.g., object size, range of values), it is important to know how to store spatial files in the most efficient way. \nThe next two sections will demonstrate how to do this.\n\n### Vector data\n\n...\n\n### Raster data {#sec-data-output-raster}\n\n...\n\n## Visual outputs {#sec-visual-outputs}\n\n## Exercises\n\n",
    "supporting": [
      "08-read-write-plot_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}