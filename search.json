[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Geocomputation with Python",
    "section": "Welcome",
    "text": "Welcome\nThis is the online home of Geocomputation with Python, a book on reproducible geographic data analysis with open source software.\nInspired by the Free and Open Source Software for Geospatial (FOSS4G) movement this is an open source book. Find the code underlying the geocompy project on GitHub, ensuring that the content is reproducible, transparent, and accessible. Making the book open source allows you or anyone else, to interact with the project by opening issues, making typo fixes and more, for the benefit of everyone.\nThe book’s website is built by GitHub Actions, which runs the code every time we make a change, ensuring code correctness and reproducibility. The current build status as follows:\n\n\n\n\n\nYou can run the code in the book using GitHub CodeSpaces as follows (requires a GitHub account):\n\n\n\n\n\nFor details on reproducing the book, see the README in the project’s GitHub repo: https://github.com/geocompx/geocompy."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Geocomputation with Python (geocompy) is motivated by the need for an introductory, yet rigorous and up-to-date, resource geographic data with the most popular programming language in the world. A unique selling point of the book is its cohesive and joined-up coverage of both vector and raster geographic data models and consistent learning curve. We aim to minimize surprises, with each section and chapter building on the previous. If you’re just starting out with Python for working with geographic data, this book is an excellent place to start.\nThere are many resources on Python on ‘GeoPython’ but none that fill this need for an introductory resource that provides strong foundations for future work. We want to avoid reinventing the wheel and provide something that fills an ‘ecological niche’ in the wider free and open source software for geospatial (FOSS4G) ecosystem. Key features include:\n\nDoing basic operations well\nIntegration of vector and raster datasets and operations\nClear explanation of each line of code in the book to minimize surprises\nExcercises at the end of each chapter with reproducible and open solutions\nProvision of lucid example datasets and meaningful operations to illustrate the applied nature of geographic research\n\nThis book is complementary with, and adds value to, other projects in the ecosystem, as highlighted in the following comparison between Geocomputation with Python and related GeoPython books:\n\nLearning Geospatial Analysis with Python and Geoprocessing with Python are books in this space that focus on processing spatial data using low-level Python interfaces for GDAL, such as the gdal, gdalnumeric, and ogr packages from osgeo. This approach requires writing more lines of code. We believe our approach is more “Pythonic” and future-proof, in light of development of packages such as geopandas and rasterio.\nIntroduction to Python for Geographic Data Analysis (in progress) seeks to provide a general introduction to ‘GIS in Python’, with parts focusing on Python essentials, using Python with GIS, and case studies. Compared with this book, which is also open source, and is hosted at pythongis.org, Geocomputation with Python has a narrower scope (not covering spatial network analysis, for example) and more coverage of raster data processing and raster-vector interoperability.\nGeographic Data Science with Python is an ambitious project with chapters dedicated to advanced topics, with Chapter 4 on Spatial Weights getting into complex topics relatively early, for example.\nPython for Geospatial Data Analysis introduces a wide range of approaches to working with geospatial data using Python, including automation of proprietary and open-source GIS software, as well as standalone open source Python packages (which is what we focus on and explain comprehensively in our book). Geocompy is shorter, simpler and more introductory, and cover raster and vector data with equal importance (1 to 4).\n\nAnother unique feature of the book is that it is part of a wider community. Geocomputation with Python is a sister project of Geocomputation with R, a book on geographic data analysis, visualization, and modeling using the R programming language that has 60+ contributors and an active community, not least in the associated Discord group. Links with the vibrant ‘R-spatial’ community, and other communities such as GeoRust and JuliaGeo, will lead to many opportunities for mutual benefit across open source ecosystems."
  },
  {
    "objectID": "02-spatial-data.html#introduction",
    "href": "02-spatial-data.html#introduction",
    "title": "1  Geographic data in Python",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThis chapter introduces key Python packages and data structures for working with the two major types of spatial data, namely:\n\nshapely and geopandas — for working with vector layers\nrasterio and xarray — for working with rasters\n\nAs we will see in the example code presented later in this chapter, shapely and geopandas are related:\n\nshapely is a “low-level” package for working with individual vector geometry objects\ngeopandas is a “high-level” package for working with geometry columns (GeoSeries objects), which internally contain shapely geometries, and vector layers (GeoDataFrame objects)\n\nThe geopandas ecosystem provides a comprehensive approach for working with vector layers in Python, with many packages building on it. This is not the case for raster data, however: there are several partially overlapping packages for working with raster data, each with its own advantages and disadvantages. In this book we focus on the most prominent one:\n\nrasterio — a spatial-oriented package, focused on “simple” raster formats (such as GeoTIFF), representing a raster using a combination of a numpy array, and a metadata object (dict) specifying the spatial referencing of the array\n\nAnother raster-related package worth mentioning is xarray. It is a general-purpose package for working with labeled arrays, thus advantageous for processing “complex” raster format (such as NetCDF), representing a raster using its own native classes, namely xarray.Dataset and xarray.DataArray\nThis chapter will briefly explain the fundamental geographic data models: vector and raster. Before demonstrating their implementation in Python, we will introduce the theory behind each data model and the disciplines in which they predominate.\nThe vector data model represents the world using points, lines, and polygons. These have discrete, well-defined borders, meaning that vector datasets usually have a high level of precision (but not necessarily accuracy).  The raster data model divides the surface up into cells of constant size. Raster datasets are the basis of background images used in web-mapping and have been a vital source of geographic data since the origins of aerial photography and satellite-based remote sensing devices. Rasters aggregate spatially specific features to a given resolution, meaning that they are consistent over space and scalable (many worldwide raster datasets are available).\nWhich to use? The answer likely depends on your domain of application, and the datasets you have access to:\n\nVector datasets and methods dominate the social sciences because human settlements and and processes (e.g. transport infrastructure) tend to have discrete borders\nRaster datasets and methods dominate many environmental sciences because of the reliance on remote sensing data\n\nThere is much overlap in some fields and raster and vector datasets can be used together: ecologists and demographers, for example, commonly use both vector and raster data. Furthermore, it is possible to convert between the two forms  Whether your work involves more use of vector or raster datasets, it is worth understanding the underlying data models before using them, as discussed in subsequent chapters. This book focusses on approaches that build on geopandas and rasterio packages to work with vector data and raster datasets, respectively."
  },
  {
    "objectID": "02-spatial-data.html#sec-vector-data",
    "href": "02-spatial-data.html#sec-vector-data",
    "title": "1  Geographic data in Python",
    "section": "1.2 Vector data",
    "text": "1.2 Vector data\nThe geographic vector data model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g., the location of a bus stop), or they can be linked together to form more complex geometries such as lines and polygons. Most point geometries contain only two dimensions (3-dimensional CRSs contain an additional \\(z\\) value, typically representing height above sea level).\nIn this system, London, for example, can be represented by the coordinates (-0.1, 51.5). This means that its location is -0.1 degrees east and 51.5 degrees north of the origin. The origin, in this case, is at 0 degrees longitude (the Prime Meridian) and 0 degree latitude (the Equator) in a geographic (‘lon/lat’) CRS.  The same point could also be approximated in a projected CRS with ‘Easting/Northing’ values of (530000, 180000) in the British National Grid, meaning that London is located 530 \\(km\\) East and 180 \\(km\\) North of the origin of the CRS.   The location of National Grid’s origin, in the sea beyond South West Peninsular, ensures that most locations in the UK have positive Easting and Northing values.\n\ngeopandas provides classes for geographic vector data and a consistent command-line interface for reproducible geographic data analysis in Python. geopandas provides an interface to three mature libraries for geocomputation which, in combination, represent a strong foundation on which many geographic applications (including QGIS and R’s spatial ecosystem) builds:\n\nGDAL, for reading, writing, and manipulating a wide range of geographic data formats, covered in Chapter 8\nPROJ, a powerful library for coordinate system transformations, which underlies the content covered in Chapter 7\nGEOS, a planar geometry engine for operations such as calculating buffers and centroids on data with a projected CRS, covered in Chapter 5\n\nTight integration with these geographic libraries makes reproducible geocomputation possible: an advantage of using a higher level language such as Python to access these libraries is that you do not need to know the intricacies of the low level components, enabling focus on the methods rather than the implementation. This section introduces geopandas classes in preparation for subsequent chapters (Chapters 5 and 8 cover the GEOS and GDAL interface, respectively).\n\n1.2.1 Vector data classes\nThe main classes for working with geographic vector data in Python are hierarchical, meaning the highest level ‘vector layer’ class is composed of simpler ‘geometry column’ and individual ‘geometry’ components. This section introduces them in order, starting with the highest level class. For many applications, the high level vector layer class, which are essentially a data frame with geometry columns, are all that’s needed. However, it’s important to understand the structure of vector geographic objects and their component pieces for more advanced applications. The three main vector geographic data classes in Python are:\n\nGeoDataFrame, a class representing vector layers, with a geometry column (class GeoSeries) as one of the columns\nGeoSeries, a class that is used to represent the geometry column in GeoDataFrame objects\nshapely geometry objects which represent individual geometries, such as a point or a polygon\n\nThe first two classes (GeoDataFrame and GeoSeries) are defined in geopandas. The third class is defined in the shapely package, which deals with individual geometries, and is a main dependency of the geopandas package.\n\n\n1.2.2 Vector layers\nThe most commonly used geographic vector data structure is the vector layer. There are several approaches for working with vector layers in Python, ranging from low-level packages (e.g., osgeo, fiona) to the relatively high-level geopandas package that is the focus of this section. Before writing and running code for creating and working with geographic vector objects, we therefore import geopandas (by convention as gpd for more concise code) and shapely:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport shapely\n\nWe also limit the maximum number of printed rows to four, to save space, using the 'display.max_rows' option of pandas:\n\npd.set_option('display.max_rows', 4)\n\nProjects often start by importing an existing vector layer saved as an ESRI Shapefile (.shp), a GeoPackage (.gpkg) file, or other geographic file format. The function read_file() in the following line of code imports a GeoPackage file named world.gpkg located in the data directory of Python’s working directory into a GeoDataFrame named gdf:\n\ngdf = gpd.read_file('data/world.gpkg')\n\nAs result is an object of type (class) GeoDataFrame with 177 rows (features) and 11 columns, as shown in the output of the following code:\n\ntype(gdf)\ngdf.shape\n\n(177, 11)\n\n\nThe GeoDataFrame class is an extension of the DataFrame class from the popular pandas package. This means we can treat a vector layer as a table, and process it using the ordinary, i.e., non-spatial, established function methods. For example, standard data frame subsetting methods can be used. The code below creates a subset of the gdf dataset containing only the country name and the geometry:\n\ngdf = gdf[['name_long', 'geometry']]\ngdf\n\n\n\n\n\n\n\n\nname_long\ngeometry\n\n\n\n\n0\nFiji\nMULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n\n\n1\nTanzania\nMULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n\n\n...\n...\n...\n\n\n175\nTrinidad and Tobago\nMULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n\n\n176\nSouth Sudan\nMULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n\n\n\n\n177 rows × 2 columns\n\n\n\nThe following expression creates a subset based on a condition, such as equality of the value in the 'name_long' column to the string 'Egypt':\n\ngdf[gdf['name_long'] == 'Egypt']\n\n\n\n\n\n\n\n\nname_long\ngeometry\n\n\n\n\n163\nEgypt\nMULTIPOLYGON (((36.86623 22.00000, 36.69069 22...\n\n\n\n\n\n\n\nFinally, to get a sense of the spatial component of the vector layer, it can be plotted using the .plot method (Figure 1.1):\n\ngdf.plot();\n\n\n\n\nFigure 1.1: Basic plot of a GeoDataFrame\n\n\n\n\nor using .explore to get an interactive plot (Figure 1.2):\n\ngdf.explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 1.2: Basic interactive map with .explore\n\n\n\nAnd consequently, a subset can be plotted using:\n\ngdf[gdf['name_long'] == 'Egypt'].explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 1.3: Interactive map of a GeoDataFrame subset\n\n\n\n\n\n1.2.3 Geometry columns\nA vital column in a GeoDataFrame is the geometry column, of class GeoSeries. The geometry column contains the geometric part of the vector layer. In the case of the gdf object, the geometry column contains 'MultiPolygon's associated with each country:\n\ngdf['geometry']\n\n0      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n1      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n                             ...                        \n175    MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n176    MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\nName: geometry, Length: 177, dtype: geometry\n\n\nThe geometry column also contains the spatial reference information, if any:\n\ngdf['geometry'].crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nMany geometry operations, such as calculating the centroid, buffer, or bounding box of each feature involve just the geometry. Applying this type of operation on a GeoDataFrame is therefore basically a shortcut to applying it on the GeoSeries object in the geometry column. The two following commands therefore return exactly the same result, a GeoSeries with country centroids:\n\ngdf.centroid\n\n/tmp/ipykernel_177/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\n\ngdf['geometry'].centroid\n\n/tmp/ipykernel_177/3996546279.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['geometry'].centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\nNote that .centroid, and other similar operators in geopandas such as .buffer (Section 4.3.3) or .convex_hull, return only the geometry (i.e., a GeoSeries), not a GeoDataFrame with the original attribute data. In case we want the latter, we can create a copy of the GeoDataFrame and then “overwrite” its geometry (or, we can overwrite the geometries directly in case we don’t need the original ones, as in gdf['geometry']=gdf.centroid):\n\ngdf2 = gdf.copy()\ngdf2['geometry'] = gdf.centroid\ngdf2\n\n/tmp/ipykernel_177/4249144945.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf2['geometry'] = gdf.centroid\n\n\n\n\n\n\n\n\n\nname_long\ngeometry\n\n\n\n\n0\nFiji\nPOINT (163.85312 -17.31631)\n\n\n1\nTanzania\nPOINT (34.75299 -6.25773)\n\n\n...\n...\n...\n\n\n175\nTrinidad and Tobago\nPOINT (-61.33037 10.42824)\n\n\n176\nSouth Sudan\nPOINT (30.19862 7.29289)\n\n\n\n\n177 rows × 2 columns\n\n\n\nAnother useful property of the geometry column is the geometry type, as shown in the following code. Note that the types of geometries contained in a geometry column (and, thus, a vector layer) are not necessarily the same for every row. Accordingly, the .type property returns a Series (of type string), rather than a single value:\n\ngdf['geometry'].type\n\n0      MultiPolygon\n1      MultiPolygon\n           ...     \n175    MultiPolygon\n176    MultiPolygon\nLength: 177, dtype: object\n\n\nTo summarize the occurrence of different geometry types in a geometry column, we can use the pandas method called value_counts:\n\ngdf['geometry'].type.value_counts()\n\nMultiPolygon    177\ndtype: int64\n\n\nIn this case, we see that the gdf layer contains only 'MultiPolygon' geometries. It is possible to have multiple geometry types in a single GeoSeries and a GeoDataFrame can have multiple GeoSeries:\n\ngdf['centroids'] = gdf.centroid\ngdf['polygons'] = gdf.geometry\ngdf\n\n/tmp/ipykernel_177/104973583.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['centroids'] = gdf.centroid\n\n\n\n\n\n\n\n\n\nname_long\ngeometry\ncentroids\npolygons\n\n\n\n\n0\nFiji\nMULTIPOLYGON (((-180.00000 -16.55522, -179.917...\nPOINT (163.85312 -17.31631)\nMULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n\n\n1\nTanzania\nMULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\nPOINT (34.75299 -6.25773)\nMULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n\n\n...\n...\n...\n...\n...\n\n\n175\nTrinidad and Tobago\nMULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\nPOINT (-61.33037 10.42824)\nMULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n\n\n176\nSouth Sudan\nMULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\nPOINT (30.19862 7.29289)\nMULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n\n\n\n\n177 rows × 4 columns\n\n\n\nTo switch the geometry column from one `GeoSeries column to another, we use set_geometry:\n\ngdf.set_geometry('centroids', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.set_geometry('polygons', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n1.2.4 The Simple Features standard\nGeometries are the basic building blocks of vector layers. Although the Simple Features standard defines about 20 types of geometries, we will focus on the seven most commonly used types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. Find the whole list of possible feature types in the PostGIS manual. \nWell-known binary (WKB) and well-known text (WKT) are the standard encodings for simple feature geometries.  WKB representations are usually hexadecimal strings easily readable for computers. This is why GIS and spatial databases use WKB to transfer and store geometry objects. WKT, on the other hand, is a human-readable text markup description of simple features. Both formats are exchangeable, and if we present one, we will naturally choose the WKT representation.\nThe foundation of each geometry type is the point. A point is simply a coordinate in 2D, 3D, or 4D space such as: \nPOINT (5 2)\nA linestring is a sequence of points with a straight line connecting the points, for example: \nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\nA polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates (see right panel in Figure …).\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nSo far we have created geometries with only one geometric entity per feature. However, the Simple Features standard allows multiple geometries to exist within a single feature, using “multi” versions of each geometry type, as illustrated below:\nMULTIPOINT (5 2, 1 3, 3 4, 3 2)\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))\n\nFinally, a geometry collection can contain any combination of geometries including (multi)points and linestrings: \nGEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))\n\n\n1.2.5 Geometries\nEach element in the geometry column is a geometry object, of class shapely. For example, here is one specific geometry selected by implicit index (that of Canada):\n\ngdf['geometry'].iloc[3]\n\n\n\n\nWe can also select a specific geometry based on the 'name_long' attribute:\n\ngdf[gdf['name_long'] == 'Egypt']['geometry'].iloc[0]\n\n\n\n\nThe shapely package is compatible with the Simple Features standard. Accordingly, seven types of geometries are supported. The following section demonstrates creating a shapely geometry of each type from scratch. In the first example (a 'Point') we demonstrate two types of inputs for geometry creation:\n\na list of coordinates\na string in the WKT format and\n\nIn the examples for the remaining geometries we use the former approach.\nCreating a 'Point' geometry from a list of coordinates uses the shapely.Point function:\n\npoint = shapely.Point([5, 2])\npoint\n\n\n\n\nAlternatively, we can use the shapely.from_wkt to transform a WKT string to a shapely geometry object. Here is an example of creating the same 'Point' geometry from WKT:\n\npoint = shapely.from_wkt('POINT (5 2)')\npoint\n\n\n\n\nHere is an example of a 'MultiPoint' geometry from a list of coordinate tuples:\n\nmultipoint = shapely.MultiPoint([(5,2), (1,3), (3,4), (3,2)])\nmultipoint\n\n\n\n\nHere is an example of a 'LineString' geometry from a list of coordinate tuples:\n\nlinestring = shapely.LineString([(1,5), (4,4), (4,1), (2,2), (3,2)])\nlinestring\n\n\n\n\nHere is an example of a 'MultiLineString' geometry. Note that there is one list of coordinates for each line in the MultiLineString:\n\nlinestring = shapely.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\nlinestring\n\n\n\n\nHere is an example of a 'Polygon' geometry. Note that there is one list of coordinates that defines the exterior outer hull of the polygon, followed by a list of lists of coordinates that define the potential holes in the polygon:\n\npolygon = shapely.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)], [[(2,4), (3,4), (3,3), (2,3), (2,4)]])\npolygon\n\n\n\n\nHere is an example of a 'MultiPolygon' geometry:\n\nmultipolygon = shapely.MultiPolygon([\n    shapely.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)]), \n    shapely.Polygon([(0,2), (1,2), (1,3), (0,3), (0,2)])\n])\nmultipolygon\n\n\n\n\nAnd, finally, here is an example of a 'GeometryCollection' geometry:\n\nmultipoint = shapely.GeometryCollection([\n    shapely.MultiPoint([(5,2), (1,3), (3,4), (3,2)]),\n    shapely.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\n])\nmultipoint\n\n\n\n\nshapely geometries act as atomic units of vector data, as spatial operations on a geometry return a single new geometry. For example, the following expression calculates the difference between the buffered multipolygon (using distance of 0.2) and itself:\n\nmultipolygon.buffer(0.2).difference(multipolygon)\n\n\n\n\nAs demonstrated above, a shapely geometry object is automatically evaluated to a small image of the geometry (when using an interface capable of displaying it, such as a Jupyter Notebook). To print the WKT string instead, we can use the print function:\n\nprint(linestring)\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\n\nWe can determine the geometry type using the .geom_type property, which returns a string:\n\nlinestring.geom_type\n\n'MultiLineString'\n\n\nFinally, it is important to note that raw coordinates of shapely geometries are accessible through a combination of the .coords, .geoms, .exterior, and .interiors, properties (depending on the geometry type). These access methods are helpful when we need to develop our own spatial operators for specific tasks. For example, the following expression returns the list of all coordinates of the polygon geometry exterior:\n\nlist(polygon.exterior.coords)\n\n[(1.0, 5.0), (2.0, 2.0), (4.0, 1.0), (4.0, 4.0), (1.0, 5.0)]\n\n\n\n\n1.2.6 Vector layer from scratch\nIn the previous sections we started with a vector layer (GeoDataFrame), from an existing Shapefile, and “decomposed” it to extract the geometry column (GeoSeries, Section 1.2.3) and separate geometries (shapely, see Section 1.2.5). In this section, we will demonstrate the opposite process, constructing a GeoDataFrame from shapely geometries, combined into a GeoSeries. This will:\n\nHelp you better understand the structure of a GeoDataFrame, and\nMay come in handy when you need to programmatically construct simple vector layers, such as a line between two given points, etc.\n\nVector layers consist of two main parts: geometries and non-geographic attributes. Figure 1.4 shows how a GeoDataFrame object is created—geometries come from a GeoSeries object (which consists of shapely geometries), while attributes are taken from Series objects.\n\n\n\nFigure 1.4: Creating a GeoDataFrame from scratch\n\n\nThe final result, a vector layer (GeoDataFrame) is therefore a hierarchical structure (Figure 1.5), containing the geometry column (GeoSeries), which in turn contains geometries (shapely). Each of the “internal” components can be accessed, or “extracted”, which is sometimes necessary, as we will see later on.\n\n\n\nFigure 1.5: Structure of a GeoDataFrame\n\n\nNon-geographic attributes represent the name of the feature or other attributes such as measured values, groups, and other things. To illustrate attributes, we will represent a temperature of 25°C in London on June 21st, 2017. This example contains a geometry (the coordinates), and three attributes with three different classes (place name, temperature and date). Objects of class GeoDataFrame represent such data by combining the attributes (Series) with the simple feature geometry column (GeoSeries). First, we create a point geometry, which we know how to do from Section 1.2.5:\n\nlnd_point = shapely.Point(0.1, 51.5)\nlnd_point\n\n\n\n\nNext, we create a GeoSeries (of length 1), containing the point. Note that a GeoSeries stores a CRS definition, in this case WGS84 (defined using its EPSG code 4326). Also note that the shapely geometries go into a list, to illustrate that there can be more than one (unlike in this example):\n\nlnd_geom = gpd.GeoSeries([lnd_point], crs=4326)\nlnd_geom\n\n0    POINT (0.10000 51.50000)\ndtype: geometry\n\n\nNext, we combine the GeoSeries with other attributes into a dict. The geometry column is a GeoSeries, named geometry. The other attributes (if any) may be defined using list or Series objects. Here, for simplicity, we use the list option for defining the three attributes name, temperature, and date. Again, note that the list can be of length &gt;1, in case we are creating a layer with more than one feature:\n\nd = {\n  'name': ['London'],\n  'temperature': [25],\n  'date': ['2017-06-21'],\n  'geometry': lnd_geom\n}\n\nFinally, the dict can be coverted to a GeoDataFrame:\n\nlnd_layer = gpd.GeoDataFrame(d)\nlnd_layer\n\n\n\n\n\n\n\n\nname\ntemperature\ndate\ngeometry\n\n\n\n\n0\nLondon\n25\n2017-06-21\nPOINT (0.10000 51.50000)\n\n\n\n\n\n\n\nWhat just happened? First, the coordinates were used to create the simple feature geometry (shapely). Second, the geometry was converted into a simple feature geometry column (GeoSeries), with a CRS. Third, attributes were combined with GeoSeries. This results in an GeoDataFrame object, named lnd_layer.\nJust to illustrate how does creating a layer with more than one feature looks like, here is an example where we create a layer with two points, London and Paris:\n\nlnd_point = shapely.Point(0.1, 51.5)\nparis_point = shapely.Point(2.3, 48.9)\ntowns_geom = gpd.GeoSeries([lnd_point, paris_point], crs=4326)\nd = {\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'geometry': towns_geom\n}\ntowns_layer = gpd.GeoDataFrame(d)\ntowns_layer\n\n\n\n\n\n\n\n\nname\ntemperature\ndate\ngeometry\n\n\n\n\n0\nLondon\n25\n2017-06-21\nPOINT (0.10000 51.50000)\n\n\n1\nParis\n27\n2017-06-21\nPOINT (2.30000 48.90000)\n\n\n\n\n\n\n\nThe following expression creates an interactive map with the result:\n\ntowns_layer.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAlternatively, we can first create a pandas.DataFrame and turn it into a GeoDataFrame like this:\n\ntowns_table = pd.DataFrame({\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'x': [0.1, 2.3],\n  'y': [51.5, 48.9]\n})\ntowns_geom = gpd.points_from_xy(towns_table['x'], towns_table['y'])\ntowns_layer = gpd.GeoDataFrame(towns_table, geometry=towns_geom, crs=4326)\n\nwhich gives the same result towns_layer:\n\ntowns_layer\n\n\n\n\n\n\n\n\nname\ntemperature\ndate\nx\ny\ngeometry\n\n\n\n\n0\nLondon\n25\n2017-06-21\n0.1\n51.5\nPOINT (0.10000 51.50000)\n\n\n1\nParis\n27\n2017-06-21\n2.3\n48.9\nPOINT (2.30000 48.90000)\n\n\n\n\n\n\n\nThis approach is particularly useful when we need to read data from a CSV file, e.g., using pandas.read_csv, and want to turn the resulting DataFrame into a GeoDataFrame.\n\n\n1.2.7 Derived numeric properties\nVector layers are characterized by two essential derived numeric properties:\n\nLength (.length)—Applicable to lines\nArea (.area)—Applicable to polygons\n\nArea and length can be calculated for any data structures discussed above, either a shapely geometry, in which case the returned value is a number:\n\nlinestring.length\n\n11.63441361516796\n\n\n\nmultipolygon.area\n\n8.0\n\n\nor for GeoSeries or DataFrame, in which case the returned value is a numeric Series:\n\ngdf.area\n\n/tmp/ipykernel_177/138307179.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.area\n\n\n0       1.639506\n1      76.301964\n         ...    \n175     0.639000\n176    51.196106\nLength: 177, dtype: float64\n\n\nLike all numeric calculations in geopandas, the results assume a planar CRS and are returned in its native units. This means that length and area measurements for geometries in WGS84 (crs=4326) are returned in decimal degrees and essentially meaningless, thus the warning in the above command.\nTo obtain true length and area measurements, the geometries first need to be transformed to a projected CRS (see Section 6.8) applicable to the area of interest. For example, the area of Slovenia can be calculated in the UTM zone 33N CRS (crs=32633). The result is in the CRS units, namely \\(m^2\\):\n\ngdf[gdf['name_long'] == 'Slovenia'].to_crs(32633).area\n\n150    1.910410e+10\ndtype: float64"
  },
  {
    "objectID": "02-spatial-data.html#raster-data",
    "href": "02-spatial-data.html#raster-data",
    "title": "1  Geographic data in Python",
    "section": "1.3 Raster data",
    "text": "1.3 Raster data\n\n1.3.1 Introduction\nAs mentioned above, working with rasters in Python is less organized around one comprehensive package (compared to the case for vector layers and geopandas). Instead, several packages provide alternative subsets of method for working with raster data.\nThe two most notable approaches for working with rasters in Python are provided by the rasterio and xarray packages. As we will see shortly, they differ in their scope and underlying data models. Specifically, rasterio represents rasters as numpy arrays associated with a separate object holding the spatial metadata. The xarray package, however, represents rasters with the native DataArray object, which is an extension of numpy array designed to hold axis labels and attributes, in the same object, together with the array of raster values.\nBoth packages are not exhaustive in the same way geopandas is. For example, when working with rasterio, on the one hand, more packages may be needed to accomplish common tasks such as zonal statistics (package rasterstats) or calculating topographic indices (package richdem). On the other hand, xarray was extended to accommodate spatial operators missing from the core package itself, with the rioxarray and xarray-spatial packages.\nIn the following two sections, we introduce rasterio, which is the raster-related package we are going to work with through the rest of the book.\n\n\n1.3.2 Using rasterio\nTo work with the rasterio package, we first need to import it. We also import numpy, since the underlying raster data are stored in numpy arrays. To effectively work with those, we expose all numpy functions. Finally, we import the rasterio.plot sub-module for its show function for quick visualization of rasters.\n\nimport numpy as np\nimport rasterio\nimport rasterio.plot\nimport subprocess\n\nRasters are typically imported from existing files. When working with rasterio, importing a raster is actually a two-step process:\n\nFirst, we open a raster file “connection” using rasterio.open\nSecond, we read raster values from the connection using the .read method\n\nThis separation is analogous to basic Python functions for reading from files, such as open and .readline to read from a text file. The rationale is that we do not always want to read all information from the file into memory, which is particularly important as rasters size can be larger than RAM size. Accordingly, the second step (.read) is selective. For example, we may want to read just one raster band rather than reading all bands.\nIn the first step, we pass a file path to the rasterio.open function to create a DatasetReader file connection. For this example, we use a single-band raster representing elevation in Zion National Park:\n\nsrc = rasterio.open('data/srtm.tif')\nsrc\n\n&lt;open DatasetReader name='data/srtm.tif' mode='r'&gt;\n\n\nTo get a first impression of the raster values, we can plot the raster using the show function (Figure 1.6):\n\nrasterio.plot.show(src);\n\n\n\n\nFigure 1.6: Basic plot of a raster, the data are coming from a rasterio file connection\n\n\n\n\nThe DatasetReader contains the raster metadata, that is, all of the information other than the raster values. Let us examine it:\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nImportantly, we can see:\n\nThe raster data type (dtype)\nRaster dimensions (width, height, and count, i.e., number of layers)\nRaster Coordinate Reference System (crs)\nThe raster affine transformation matrix (transform)\n\nThe last item (i.e., transform) deserves more attention. To position a raster in geographical space, in addition to the CRS we must specify the raster origin (\\(x_{min}\\), \\(y_{max}\\)) and resolution (\\(delta_{x}\\), \\(delta_{y}\\)). In the transform matrix notation, these data items are stored as follows:\nAffine(delta_x, 0.0, x_min,\n       0.0, delta_y, y_max)\nNote that, by convention, raster y-axis origin is set to the maximum value (\\(y_{max}\\)) rather than the minimum, and, accordingly, the y-axis resolution (\\(delta_{y}\\)) is negative.\nFinally, the .read method of the DatasetReader is used to read the actual raster values. Importantly, we can read:\n\nA particular layer, passing a numeric index (as in .read(1))\nA subset of layers, passing a list of indices (as in .read([1,2]))\nAll layers (as in .read())\n\nNote that the layer indices start from 1, contrary to the Python convention of the first index being 0.\nThe resulting object is a numpy array, with either two or three dimensions:\n\nThree dimensions, when reading more than one layer (e.g., .read() or .read([1,2])). In such case, the dimensions pattern is (layers, rows, columns)\nTwo dimensions, when reading one specific layer (e.g., .read(1))\n\nLet’s read the first (and only) layer from the srtm.tif raster, using the file connection object src, for example:\n\nsrc.read(1)\n\narray([[1728, 1718, 1715, ..., 2654, 2674, 2685],\n       [1737, 1727, 1717, ..., 2649, 2677, 2693],\n       [1739, 1734, 1727, ..., 2644, 2672, 2695],\n       ...,\n       [1326, 1328, 1329, ..., 1777, 1778, 1775],\n       [1320, 1323, 1326, ..., 1771, 1770, 1772],\n       [1319, 1319, 1322, ..., 1768, 1770, 1772]], dtype=uint16)\n\n\nThe result is a two-dimensional numpy array.\nThe relation between a rasterio file connection and the derived properties is summarized in Figure 1.7. The file connection (created with rasterio.open) gives access to the two components of raster data: the metadata (via the .meta property) and the values (via the .read method).\n\n\n\nFigure 1.7: Creating a GeoDataFrame from scratch\n\n\n\n\n1.3.3 Raster from scratch\nIn this section, we are going to demonstrate creation of rasters from scratch. We are going to create two small rasters, elev and grain, which we are going to use in examples later on in the book. Unlike creating a vector layer, creating a raster from scratch is rarely needed in practive because aligning a raster with the right spatial extent is difficult to do programmatically (GIS software is a better fit for the job). Nevertheless, the examples will be useful to become more familiar with the rasterio data structures.\nA raster is basically an array combined with georeferencing information, namely:\n\nA transformation matrix (linking pixel indices with coordinates)\nA CRS definition\n\nTherefore, to create a raster, we first need to have an array with the values, then supplement it with the georeferencing information. Let’s create the arrays elev and grain. The elev array is a 6 by 6 array with sequential values from 1 to 36. It can be created as follows:\n\nelev = np.arange(1, 37, dtype=np.uint8).reshape(6, 6)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThe grain array represents a categorical raster with values 0, 1, 2, corresponding to categories “clay”, “silt”, “sand”, respectively. We will create if from a specific arrangement of pixel values, as follows:\n\nv = [\n  1, 0, 1, 2, 2, 2, \n  0, 2, 0, 0, 2, 1, \n  0, 2, 2, 0, 0, 2, \n  0, 0, 1, 1, 1, 1, \n  1, 1, 1, 2, 1, 1, \n  2, 1, 2, 2, 0, 2\n]\ngrain = np.array(v, dtype=np.uint8).reshape(6, 6)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nNote that in both cases we are using the uint8 (unsigned integer in 8 bits, i.e., 0-255) data type, which is minimally sufficient to represent all possible values of the given rasters. This is the recommended approach for a minimal memory footprint.\nWhat is missing is the raster transform (see Section 1.3.2). In this case, since the rasters are arbitrary, we also set up an arbitrary transformation matrix, where:\n\nthe origin (\\(x_{min}\\), \\(y_{max}\\)) is at -1.5,1.5, and\nand resolution (\\(delta_{x}\\), \\(delta_{y}\\)) is 0.5,-0.5.\n\nIn terms of code, we do that as follows, using rasterio.transform.from_origin:\n\nnew_transform = rasterio.transform.from_origin(\n    west=-1.5, \n    north=1.5, \n    xsize=0.5, \n    ysize=0.5\n)\nnew_transform\n\nAffine(0.5, 0.0, -1.5,\n       0.0, -0.5, 1.5)\n\n\nNote that, confusingly, \\(delta_{y}\\) is defined in rasterio.transform.from_origin using a positive value (0.5), even though it is eventuially negative (-0.5)!\nThe raster can now be plotted in its coordinate system, passing the array elev along with the transformation matrix new_transform to show (Figure 1.8):\n\nrasterio.plot.show(elev, transform=new_transform);\n\n\n\n\nFigure 1.8: Plot of the elev raster, which was created from scratch\n\n\n\n\nThe grain raster can be plotted the same way, as we are going to use the same transformation matrix for it as well (Figure 1.9):\n\nrasterio.plot.show(grain, transform=new_transform);\n\n\n\n\nFigure 1.9: Plot of the grain raster, which was created from scratch\n\n\n\n\nAt this point, we can work with the raster using rasterio:\n\nPassing the transformation matrix wherever true raster pixel coordinates are important (such as in function show above)\nKeeping in mind that any other layer we use in the analysis is in the same CRS of those coordinates\n\nFinally, to export the raster for permanent storage, along with the CRS definition, we need to go through the following steps:\n\nCreate a raster file connection (where we set the transform and the CRS, among other settings)\nWrite the array with raster values into the connection\nClose the connection\n\nIn the case of elev, we do it as follows:\n\nnew_dataset = rasterio.open(\n    'output/elev.tif', 'w', \n    driver='GTiff',\n    height=elev.shape[0],\n    width=elev.shape[1],\n    count=1,\n    dtype=elev.dtype,\n    crs=4326,\n    transform=new_transform\n)\nnew_dataset.write(elev, 1)\nnew_dataset.close()\n\nNote that the CRS we (arbitrarily) set for the elev raster is WGS84, defined using crs=4326 according to the EPSG code.\nHere is how we export the grain raster as well, using almost the exact same code just with elev replaced with grain:\n\nnew_dataset = rasterio.open(\n    'output/grain.tif', 'w', \n    driver='GTiff',\n    height=grain.shape[0],\n    width=grain.shape[1],\n    count=1,\n    dtype=grain.dtype,\n    crs=4326,\n    transform=new_transform\n)\nnew_dataset.write(grain, 1)\nnew_dataset.close()\n\nDon’t worry if the raster export code is unclear. We will elaborate on the details of raster output in Section 7.8.2.\nAs a result, the files elev.tif and grain.tif are written into the output directory. These are identical to the elev.tif and grain.tif files in the data directory which we use later on in the examples (for example, Section 2.4.1).\nNote that the transform matrices and dimensions of elev and grain are identical. This means that the rasters are overlapping, and can be combined into one two-band raster, combined in raster algebra operations (Section 3.4.2), etc."
  },
  {
    "objectID": "02-spatial-data.html#sec-coordinate-reference-systems",
    "href": "02-spatial-data.html#sec-coordinate-reference-systems",
    "title": "1  Geographic data in Python",
    "section": "1.4 Coordinate Reference Systems",
    "text": "1.4 Coordinate Reference Systems\nVector and raster spatial data types share concepts intrinsic to spatial data. Perhaps the most fundamental of these is the Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other bodies). CRSs are either geographic or projected, as introduced at the beginning of this chapter (see …). This section explains each type, laying the foundations for Chapter 6, which provides a deep dive into setting, transforming, and querying CRSs.\n\n1.4.1 Geographic coordinate systems\nGeographic coordinate systems identify any location on the Earth’s surface using two values—longitude and latitude (see left panel of Figure 1.10). Longitude is a location in the East-West direction in angular distance from the Prime Meridian plane, while latitude is an angular distance North or South of the equatorial plane. Distances in geographic CRSs are therefore not measured in meters. This has important consequences, as demonstrated in Section 7.\nA spherical or ellipsoidal surface represents the surface of the Earth in geographic coordinate systems. Spherical models assume that the Earth is a perfect sphere of a given radius—they have the advantage of simplicity, but, at the same time, they are inaccurate: the Earth is not a sphere! Ellipsoidal models are defined by two parameters: the equatorial radius and the polar radius. These are suitable because the Earth is compressed: the equatorial radius is around 11.5 km longer than the polar radius. \nEllipsoids are part of a broader component of CRSs: the datum. This contains information on what ellipsoid to use and the precise relationship between the Cartesian coordinates and location on the Earth’s surface. There are two types of datum—geocentric (such as WGS84) and local (such as NAD83). You can see examples of these two types of datums in Figure …. Black lines represent a geocentric datum, whose center is located in the Earth’s center of gravity and is not optimized for a specific location. In a local datum, shown as a purple dashed line, the ellipsoidal surface is shifted to align with the surface at a particular location. These allow local variations on Earth’s surface, such as large mountain ranges, to be accounted for in a local CRS. \n\n\n1.4.2 Projected coordinate reference systems\nAll projected CRSs are based on a geographic CRS, described in the previous section, and rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS. Projected CRSs are based on Cartesian coordinates on an implicitly flat surface (see right panel of Figure 1.10).  They have an origin, x and y axes, and a linear unit of measurement such as meters.\nThis transition cannot be done without adding some deformations. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve: equal-area preserves area, azimuthal preserves direction, equidistant preserves distance, and conformal preserves local shape.\nThere are three main groups of projection types: conic, cylindrical, and planar (azimuthal). In a conic projection, the Earth’s surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines in this projection. Therefore, it is best suited for maps of mid-latitude areas. A cylindrical projection maps the surface onto a cylinder. This projection could also be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are used most often when mapping the entire world. A planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions.\nLike most open-source geospatial software, the geopandas and rasterio packages use the PROJ software for CRS definition and calculations. The pyproj package is a low-level interface to PROJ. Using its functions, we can examine the list of supported projections:\n\nimport pyproj\nepsg_codes = pyproj.get_codes('EPSG', 'CRS')  ## List of supported EPSG codes\nepsg_codes[:5]  ## print first five\n\n['2000', '20000', '20001', '20002', '20003']\n\n\n\npyproj.CRS.from_epsg(4326)  ## Printout of WGS84 CRS (EPSG:4326)\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nA quick summary of different projections, their types, properties, and suitability can be found in “Map Projections” (1993) and at https://www.geo-projections.com/. We will expand on CRSs and explain how to project from one CRS to another in Chapter 7. But, for now, it is sufficient to know:\n\nThat coordinate systems are a key component of geographic objects\nKnowing which CRS your data is in, and whether it is in geographic (lon/lat) or projected (typically meters), is important and has consequences for how Python handles spatial and geometry operations\nCRSs of geopandas (vector layer or geometry column) and rasterio (raster) objects can be queried with the .crs property\n\nHere is a demonstration of the last bullet point, where we import a vector layer and figure out its CRS (in this case, a projected CRS, namely UTM Zone 12):\n\nzion = gpd.read_file('data/zion.gpkg')\nzion.crs\n\n&lt;Bound CRS: PROJCS[\"UTM Zone 12, Northern Hemisphere\",GEOGCS[\" ...&gt;\nName: UTM Zone 12, Northern Hemisphere\nAxis Info [cartesian]:\n- [east]: Easting (Meter)\n- [north]: Northing (Meter)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: Transformation from GRS 1980(IUGG, 1980) to WGS84\n- method: Position Vector transformation (geog2D domain)\nDatum: unknown\n- Ellipsoid: GRS80\n- Prime Meridian: Greenwich\nSource CRS: UTM Zone 12, Northern Hemisphere\n\n\nAnd here is an illustration of the layer in the original projected CRS and in a geographic CRS (Figure 1.10):\n\n# WGS84\nzion.to_crs(4326).plot(edgecolor='black', color='lightgrey').grid()\n# NAD83 / UTM zone 12N\nzion.plot(edgecolor='black', color='lightgrey').grid();\n\n\n\n\n\n\n\n(a) geographic (WGS84)\n\n\n\n\n\n\n\n(b) projected (NAD83 / UTM zone 12N)\n\n\n\n\nFigure 1.10: Examples of Coordinate Refrence Systems (CRS) for a vector layer\n\n\n\nWe are going to elaborate on reprojection from one CRS to another (.to_crs in the above code section) in Chapter 6."
  },
  {
    "objectID": "02-spatial-data.html#units",
    "href": "02-spatial-data.html#units",
    "title": "1  Geographic data in Python",
    "section": "1.5 Units",
    "text": "1.5 Units\nAn essential feature of CRSs is that they contain information about spatial units. Clearly, it is vital to know whether a house’s measurements are in feet or meters, and the same applies to maps. It is a good cartographic practice to add a scale bar or some other distance indicator onto maps to demonstrate the relationship between distances on the page or screen and distances on the ground. Likewise, it is important for the user to be aware of the units in which the geometry coordinates are, to ensure that subsequent calculations are done in the right context.\nPython spatial data structures in geopandas and rasterio do not natively support the concept of measurement. The coordinates of a vector layer or a raster are plain numbers, referring to an arbitrary plane. For example, according to the .transform matrix of srtm.tif we can see that the raster resolution is 0.000833 and that its CRS is WGS84 (EPSG: 4326). We may know (or can find out) that the units of WGS84 are decimal degrees. However, that information is not encoded in any numeric calculation.\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nConsequently, we need to be aware of the CRS units we are working with. Typically, these are decimal degrees, in a geographic CRS, or \\(m\\), in a projected CRS, although there are exceptions. Geometric calculations such as length, area, or distance, return plain numbers in the same units of the CRS (such as \\(m\\) or \\(m^2\\)). It is up to the user to determine which units the result is given in and treat the result accordingly. For example, if the area output was in \\(m^2\\) and we need the result in \\(km^2\\), then we need to divide the result by \\(1000^2\\)."
  },
  {
    "objectID": "02-spatial-data.html#exercises",
    "href": "02-spatial-data.html#exercises",
    "title": "1  Geographic data in Python",
    "section": "1.6 Exercises",
    "text": "1.6 Exercises\n…"
  },
  {
    "objectID": "03-attribute-operations.html#prerequisites",
    "href": "03-attribute-operations.html#prerequisites",
    "title": "2  Attribute data operations",
    "section": "2.1 Prerequisites",
    "text": "2.1 Prerequisites\nLet’s import the required packages:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\n\nand load the sample data for this chapter:\n\n\nAttempting to get the data\n\n\n\nworld = gpd.read_file('data/world.gpkg')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_multi_rast = rasterio.open('data/landsat.tif')"
  },
  {
    "objectID": "03-attribute-operations.html#introduction",
    "href": "03-attribute-operations.html#introduction",
    "title": "2  Attribute data operations",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nAttribute data is non-spatial information associated with geographic (geometry) data. A bus stop provides a simple example: its position would typically be represented by latitude and longitude coordinates (geometry data), in addition to its name. The Elephant & Castle / New Kent Road stop in London, for example has coordinates of -0.098 degrees longitude and 51.495 degrees latitude which can be represented as POINT (-0.098 51.495) in the Simple Feature representation described in Chapter 1. Attributes such as the name attribute of the POINT feature (to use Simple Features terminology) are the topic of this chapter.\nAnother example is the elevation value (attribute) for a specific grid cell in raster data. Unlike the vector data model, the raster data model stores the coordinate of the grid cell indirectly, meaning the distinction between attribute and spatial information is less clear. To illustrate the point, think of a pixel in the 3rd row and the 4th column of a raster matrix. Its spatial location is defined by its index in the matrix: move from the origin four cells in the x direction (typically east and right on maps) and three cells in the y direction (typically south and down). The raster’s resolution defines the distance for each x- and y-step which is specified in a header. The header is a vital component of raster datasets which specifies how pixels relate to geographic coordinates (see also Chapter @spatial-operations).\nThis chapter teaches how to manipulate geographic objects based on attributes such as the names of bus stops in a vector dataset and elevations of pixels in a raster dataset. For vector data, this means techniques such as subsetting and aggregation (see Section 2.3.1 and Section 2.3.2). Section 2.3.3 and Section 2.3.4 demonstrate how to join data onto simple feature objects using a shared ID and how to create new variables, respectively. Each of these operations has a spatial equivalent: [ operator for subsetting a (Geo)DataFrame using a boolean Series, for example, is applicable both for subsetting objects based on their attribute and spatial relations derived using methods such as .intersects; you can also join attributes in two geographic datasets using spatial joins. This is good news: skills developed in this chapter are cross-transferable. Chapter 3 extends the methods presented here to the spatial world.\nAfter a deep dive into various types of vector attribute operations in the next section, raster attribute data operations are covered in Section 2.4, which demonstrates how to create raster layers containing continuous and categorical attributes and extracting cell values from one or more layer (raster subsetting). Section 2.4.2 provides an overview of ‘global’ raster operations which can be used to summarize entire raster datasets."
  },
  {
    "objectID": "03-attribute-operations.html#vector-attribute-manipulation",
    "href": "03-attribute-operations.html#vector-attribute-manipulation",
    "title": "2  Attribute data operations",
    "section": "2.3 Vector attribute manipulation",
    "text": "2.3 Vector attribute manipulation\nAs mentioned in Section 1.2.2, vector layers (GeoDataFrame, from package geopandas) are basically extended tables (DataFrame from package pandas), the difference being that a vector layer has a geometry column. Since GeoDataFrame extends DataFrame, all ordinary table-related operations from package pandas are supported for vector layers as well, as shown below.\n\n2.3.1 Vector attribute subsetting\npandas supports several subsetting interfaces, though the most recommended ones are:\n\n.loc, which uses pandas indices, and\n.iloc, which uses (implicit) numpy-style numeric indices.\n\nIn both cases the method is followed by square brackets, and two indices, separated by a comma. Each index can comprise:\n\nA specific value, as in 1\nA slice, as in 0:3\nA list, as in [0,2,4]\n:—indicating “all” indices\n\nAn exception to this rule is selecting columns using a list, as in df[['a','b']], instead of df.loc[:, ['a','b']], to select columns 'a' and 'b' from df.\nHere are few examples of subsetting the GeoDataFrame of world countries.\nSubsetting rows by position, e.g., the first three rows:\n\nworld.iloc[0:3, :]\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.960\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nTanzania\nAfrica\n...\n64.163\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nNaN\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n\n\n3 rows × 11 columns\n\n\n\nwhich is equivalent to:\n\nworld.iloc[:3]\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.960\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nTanzania\nAfrica\n...\n64.163\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nNaN\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n\n\n3 rows × 11 columns\n\n\n\nas well as:\n\nworld.head(3)\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.960\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nTanzania\nAfrica\n...\n64.163\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nNaN\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n\n\n3 rows × 11 columns\n\n\n\nSubsetting columns by position, e.g., the first three columns:\n\nworld.iloc[:, 0:3]\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n\n\n\n\n0\nFJ\nFiji\nOceania\n\n\n1\nTZ\nTanzania\nAfrica\n\n\n2\nEH\nWestern Sahara\nAfrica\n\n\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nEurope\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n\n\n176\nSS\nSouth Sudan\nAfrica\n\n\n\n\n177 rows × 3 columns\n\n\n\nSubsetting rows and columns by position:\n\nworld.iloc[0:3, 0:3]\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n\n\n\n\n0\nFJ\nFiji\nOceania\n\n\n1\nTZ\nTanzania\nAfrica\n\n\n2\nEH\nWestern Sahara\nAfrica\n\n\n\n\n\n\n\nSubsetting columns by name:\n\nworld[['name_long', 'geometry']]\n\n\n\n\n\n\n\n\nname_long\ngeometry\n\n\n\n\n0\nFiji\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTanzania\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nWestern Sahara\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n...\n...\n...\n\n\n174\nKosovo\nMULTIPOLYGON (((20.59025 41.855...\n\n\n175\nTrinidad and Tobago\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n176\nSouth Sudan\nMULTIPOLYGON (((30.83385 3.5091...\n\n\n\n\n177 rows × 2 columns\n\n\n\n“Slice” of columns between given ones:\n\nworld.loc[:, 'name_long':'pop']\n\n\n\n\n\n\n\n\nname_long\ncontinent\nregion_un\n...\ntype\narea_km2\npop\n\n\n\n\n0\nFiji\nOceania\nOceania\n...\nSovereign country\n19289.970733\n885806.0\n\n\n1\nTanzania\nAfrica\nAfrica\n...\nSovereign country\n932745.792357\n52234869.0\n\n\n2\nWestern Sahara\nAfrica\nAfrica\n...\nIndeterminate\n96270.601041\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nKosovo\nEurope\nEurope\n...\nSovereign country\n11230.261672\n1821800.0\n\n\n175\nTrinidad and Tobago\nNorth America\nAmericas\n...\nSovereign country\n7737.809855\n1354493.0\n\n\n176\nSouth Sudan\nAfrica\nAfrica\n...\nSovereign country\n624909.099086\n11530971.0\n\n\n\n\n177 rows × 7 columns\n\n\n\nSubsetting by a list of boolean values (0 and 1 or True and False):\n\nx = [1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0] \nworld.iloc[:, x]\n\n\n\n\n\n\n\n\nname_long\nname_long\niso_a2\n...\nname_long\niso_a2\niso_a2\n\n\n\n\n0\nFiji\nFiji\nFJ\n...\nFiji\nFJ\nFJ\n\n\n1\nTanzania\nTanzania\nTZ\n...\nTanzania\nTZ\nTZ\n\n\n2\nWestern Sahara\nWestern Sahara\nEH\n...\nWestern Sahara\nEH\nEH\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nKosovo\nKosovo\nXK\n...\nKosovo\nXK\nXK\n\n\n175\nTrinidad and Tobago\nTrinidad and Tobago\nTT\n...\nTrinidad and Tobago\nTT\nTT\n\n\n176\nSouth Sudan\nSouth Sudan\nSS\n...\nSouth Sudan\nSS\nSS\n\n\n\n\n177 rows × 11 columns\n\n\n\nWe can remove specific rows by id using the .drop method, e.g., dropping rows 2, 3, and 5:\n\nworld.drop([2, 3, 5])\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.960000\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nTanzania\nAfrica\n...\n64.163000\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n4\nUS\nUnited States\nNorth America\n...\n78.841463\n51921.984639\nMULTIPOLYGON (((-171.73166 63.7...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nEurope\n...\n71.097561\n8698.291559\nMULTIPOLYGON (((20.59025 41.855...\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\n70.426000\n31181.821196\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n176\nSS\nSouth Sudan\nAfrica\n...\n55.817000\n1935.879400\nMULTIPOLYGON (((30.83385 3.5091...\n\n\n\n\n174 rows × 11 columns\n\n\n\nOr remove specific columns using the .drop method and axis=1 (i.e., columns):\n\nworld.drop(['name_long', 'continent'], axis=1)\n\n\n\n\n\n\n\n\niso_a2\nregion_un\nsubregion\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nOceania\nMelanesia\n...\n69.960000\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nAfrica\nEastern Africa\n...\n64.163000\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nEH\nAfrica\nNorthern Africa\n...\nNaN\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nEurope\nSouthern Europe\n...\n71.097561\n8698.291559\nMULTIPOLYGON (((20.59025 41.855...\n\n\n175\nTT\nAmericas\nCaribbean\n...\n70.426000\n31181.821196\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n176\nSS\nAfrica\nEastern Africa\n...\n55.817000\n1935.879400\nMULTIPOLYGON (((30.83385 3.5091...\n\n\n\n\n177 rows × 9 columns\n\n\n\nWe can rename columns using the .rename method:\n\nworld[['name_long', 'pop']].rename(columns={'pop': 'population'})\n\n\n\n\n\n\n\n\nname_long\npopulation\n\n\n\n\n0\nFiji\n885806.0\n\n\n1\nTanzania\n52234869.0\n\n\n2\nWestern Sahara\nNaN\n\n\n...\n...\n...\n\n\n174\nKosovo\n1821800.0\n\n\n175\nTrinidad and Tobago\n1354493.0\n\n\n176\nSouth Sudan\n11530971.0\n\n\n\n\n177 rows × 2 columns\n\n\n\nThe standard numpy comparison operators can be used in boolean subsetting, as illustrated in Table Table 2.1.\n\n\nTable 2.1: Comparison operators that return Booleans (True/False).\n\n\nSymbol\nName\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&gt;, &lt;\nGreater/Less than\n\n\n&gt;=, &lt;=\nGreater/Less than or equal\n\n\n&, |, ~\nLogical operators: And, Or, Not\n\n\n\n\nThe following example demonstrates logical vectors for subsetting by creating a new GeoDataFrame object called small_countries that contains only those countries whose surface area is smaller than 10,000 km2:\n\nidx_small = world['area_km2'] &lt; 10000  ## a logical 'Series'\nsmall_countries = world[idx_small]\nsmall_countries\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n45\nPR\nPuerto Rico\nNorth America\n...\n79.390122\n35066.046376\nMULTIPOLYGON (((-66.28243 18.51...\n\n\n79\nPS\nPalestine\nAsia\n...\n73.126000\n4319.528283\nMULTIPOLYGON (((35.39756 31.489...\n\n\n89\nVU\nVanuatu\nOceania\n...\n71.709000\n2892.341604\nMULTIPOLYGON (((166.79316 -15.6...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n160\nNaN\nNorthern Cyprus\nAsia\n...\nNaN\nNaN\nMULTIPOLYGON (((32.73178 35.140...\n\n\n161\nCY\nCyprus\nAsia\n...\n80.173000\n29786.365653\nMULTIPOLYGON (((32.73178 35.140...\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\n70.426000\n31181.821196\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n\n\n7 rows × 11 columns\n\n\n\nThe intermediary idx_small (short for index representing small countries) is a boolean Series that can be used to subset the seven smallest countries in the world by surface area. A more concise command, which omits the intermediary object, generates the same result:\n\nsmall_countries = world[world['area_km2'] &lt; 10000]\nsmall_countries\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n45\nPR\nPuerto Rico\nNorth America\n...\n79.390122\n35066.046376\nMULTIPOLYGON (((-66.28243 18.51...\n\n\n79\nPS\nPalestine\nAsia\n...\n73.126000\n4319.528283\nMULTIPOLYGON (((35.39756 31.489...\n\n\n89\nVU\nVanuatu\nOceania\n...\n71.709000\n2892.341604\nMULTIPOLYGON (((166.79316 -15.6...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n160\nNaN\nNorthern Cyprus\nAsia\n...\nNaN\nNaN\nMULTIPOLYGON (((32.73178 35.140...\n\n\n161\nCY\nCyprus\nAsia\n...\n80.173000\n29786.365653\nMULTIPOLYGON (((32.73178 35.140...\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\n70.426000\n31181.821196\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n\n\n7 rows × 11 columns\n\n\n\nThe various methods shown above can be chained for any combination with several subsetting steps, e.g.:\n\nworld[world['continent'] == 'Asia']  \\\n    .loc[:, ['name_long', 'continent']]  \\\n    .iloc[0:5, :]\n\n\n\n\n\n\n\n\nname_long\ncontinent\n\n\n\n\n5\nKazakhstan\nAsia\n\n\n6\nUzbekistan\nAsia\n\n\n8\nIndonesia\nAsia\n\n\n24\nTimor-Leste\nAsia\n\n\n76\nIsrael\nAsia\n\n\n\n\n\n\n\nWe can also combine indexes:\n\nidx_small = world['area_km2'] &lt; 10000\nidx_asia = world['continent'] == 'Asia'\nworld.loc[idx_small & idx_asia, ['name_long', 'continent', 'area_km2']]\n\n\n\n\n\n\n\n\nname_long\ncontinent\narea_km2\n\n\n\n\n79\nPalestine\nAsia\n5037.103826\n\n\n160\nNorthern Cyprus\nAsia\n3786.364506\n\n\n161\nCyprus\nAsia\n6207.006191\n\n\n\n\n\n\n\n\n\n2.3.2 Vector attribute aggregation\nAggregation involves summarizing data based on one or more grouping variables (typically values in a column;geographic aggregation is covered in the next chapter). A classic example of this attribute-based aggregation is calculating the number of people per continent based on country-level data (one row per country). The world dataset contains the necessary ingredients: the columns pop and continent, the population and the grouping variable, respectively. The aim is to find the sum() of country populations for each continent, resulting in a smaller data frame. (Since aggregation is a form of data reduction, it can be a useful early step when working with large datasets). This aggregation can be achieved using a combination of .groupby and .sum:\n\nworld_agg1 = world[['continent', 'pop']].groupby('continent').sum()\nworld_agg1\n\n\n\n\n\n\n\n\npop\n\n\ncontinent\n\n\n\n\n\nAfrica\n1.154947e+09\n\n\nAntarctica\n0.000000e+00\n\n\nAsia\n4.311408e+09\n\n\n...\n...\n\n\nOceania\n3.775783e+07\n\n\nSeven seas (open ocean)\n0.000000e+00\n\n\nSouth America\n4.120608e+08\n\n\n\n\n8 rows × 1 columns\n\n\n\nIf you dislike the scientific notation used by default to display the population sums, you can change the Pandas display format for float values like this:\n\npd.set_option('display.float_format', '{:.0f}'.format)\nworld_agg1\n\n\n\n\n\n\n\n\npop\n\n\ncontinent\n\n\n\n\n\nAfrica\n1154946633\n\n\nAntarctica\n0\n\n\nAsia\n4311408059\n\n\n...\n...\n\n\nOceania\n37757833\n\n\nSeven seas (open ocean)\n0\n\n\nSouth America\n412060811\n\n\n\n\n8 rows × 1 columns\n\n\n\nThe result is a (non-spatial) table with eight rows, one per continent, and two columns reporting the name and population of each continent.\nIf we want to include the geometry in the aggregation result, we can use the .dissolve method. That way, in addition to the summed population, we also get the associated geometry per continent, i.e., the union of all countries. Note that we use the by parameter to choose which column(s) are used for grouping, and the aggfunc parameter to choose the aggregation function for non-geometry columns:\n\nworld_agg2 = world[['continent', 'pop', 'geometry']] \\\n    .dissolve(by='continent', aggfunc='sum') \\\n    .reset_index()\nworld_agg2\n\n\n\n\n\n\n\n\ncontinent\ngeometry\npop\n\n\n\n\n0\nAfrica\nMULTIPOLYGON (((-11.43878 6.785...\n1154946633\n\n\n1\nAntarctica\nMULTIPOLYGON (((-61.13898 -79.9...\n0\n\n\n2\nAsia\nMULTIPOLYGON (((48.67923 14.003...\n4311408059\n\n\n...\n...\n...\n...\n\n\n5\nOceania\nMULTIPOLYGON (((147.91405 -43.2...\n37757833\n\n\n6\nSeven seas (open ocean)\nPOLYGON ((68.93500 -48.62500, 6...\n0\n\n\n7\nSouth America\nMULTIPOLYGON (((-68.63999 -55.5...\n412060811\n\n\n\n\n8 rows × 3 columns\n\n\n\nFigure 2.1 shows the result:\n\nworld_agg2.plot(column='pop', legend=True);\n\n\n\n\nFigure 2.1: Continents with summed population\n\n\n\n\nThe resulting world_agg2 object is a GeoDataFrame containing 8 features representing the continents of the world (and the open ocean).\nOther options for the aggfunc parameter in .dissolve include:\n\n'first'\n'last'\n'min'\n'max'\n'sum'\n'mean'\n'median'\n\nAdditionally, we can pass custom functions.\nAs a more complex example, here is how we can calculate the total population, area, and count of countries, per continent:\n\nworld_agg3 = world.dissolve(\n    by='continent', aggfunc={\n         \"name_long\": \"count\",\n         \"pop\": \"sum\",\n         'area_km2': \"sum\"\n     }).rename(columns={'name_long': 'n'})\nworld_agg3\n\n\n\n\n\n\n\n\ngeometry\nn\npop\narea_km2\n\n\ncontinent\n\n\n\n\n\n\n\n\nAfrica\nMULTIPOLYGON (((-11.43878 6.785...\n51\n1154946633\n29946198\n\n\nAntarctica\nMULTIPOLYGON (((-61.13898 -79.9...\n1\n0\n12335956\n\n\nAsia\nMULTIPOLYGON (((48.67923 14.003...\n47\n4311408059\n31252459\n\n\n...\n...\n...\n...\n...\n\n\nOceania\nMULTIPOLYGON (((147.91405 -43.2...\n7\n37757833\n8504489\n\n\nSeven seas (open ocean)\nPOLYGON ((68.93500 -48.62500, 6...\n1\n0\n11603\n\n\nSouth America\nMULTIPOLYGON (((-68.63999 -55.5...\n13\n412060811\n17762592\n\n\n\n\n8 rows × 4 columns\n\n\n\nFigure Figure 2.2 visualizes the resulting layer (world_agg3) of continents with the three aggregated attributes.\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 5))\nworld_agg3.plot(column='pop', edgecolor='black', legend=True, ax=axes[0][0])\nworld_agg3.plot(column='area_km2', edgecolor='black', legend=True, ax=axes[0][1])\nworld_agg3.plot(column='n', edgecolor='black', legend=True, ax=axes[1][0])\naxes[0][0].set_title('Summed population')\naxes[0][1].set_title('Summed area')\naxes[1][0].set_title('Count of countries')\nfig.delaxes(axes[1][1]);\n\n\n\n\nFigure 2.2: Continent properties, calculated using spatial aggregation using different functions\n\n\n\n\nLet’s proceed with the last result to demonstrate other table-related operations. Given the world_agg3 continent summary (Figure 2.2), we:\n\ndrop the geometry columns,\ncalculate population density of each continent,\narrange continents by the number countries they contain, and\nkeep only the 3 most populous continents.\n\n\nworld_agg4 = world_agg3.drop(columns=['geometry'])\nworld_agg4['density'] = world_agg4['pop'] / world_agg4['area_km2']\nworld_agg4 = world_agg4.sort_values(by='n', ascending=False)\nworld_agg4 = world_agg4.head(3)\nworld_agg4\n\n\n\n\n\n\n\n\nn\npop\narea_km2\ndensity\n\n\ncontinent\n\n\n\n\n\n\n\n\nAfrica\n51\n1154946633\n29946198\n39\n\n\nAsia\n47\n4311408059\n31252459\n138\n\n\nEurope\n39\n669036256\n23065219\n29\n\n\n\n\n\n\n\n\n\n2.3.3 Vector attribute joining\nCombining data from different sources is a common task in data preparation. Joins do this by combining tables based on a shared ‘key’ variable. pandas has a function named pd.merge for joining (Geo)DataFrames based on common column(s). The pd.merge function follows conventions used in the database language SQL (Grolemund and Wickham 2016). The pd.merge function works the same on DataFrame and GeoDataFrame objects. The result of pd.merge can be either a DataFrame or a GeoDataFrame object, depending on the inputs.\nA common type of attribute join on spatial data is to join DataFrames to GeoDataFrames. To achieve this, we use pd.merge with a GeoDataFrame as the first argument and add columns to it from a DataFrame specified as the second argument. In the following example, we combine data on coffee production with the world dataset. The coffee data is in a DataFrame called coffee_data imported from a CSV file of major coffee-producing nations:\n\ncoffee_data = pd.read_csv('data/coffee_data.csv')\ncoffee_data\n\n\n\n\n\n\n\n\nname_long\ncoffee_production_2016\ncoffee_production_2017\n\n\n\n\n0\nAngola\nNaN\nNaN\n\n\n1\nBolivia\n3\n4\n\n\n2\nBrazil\n3277\n2786\n\n\n...\n...\n...\n...\n\n\n44\nZambia\n3\nNaN\n\n\n45\nZimbabwe\n1\n1\n\n\n46\nOthers\n23\n26\n\n\n\n\n47 rows × 3 columns\n\n\n\nIts three columns are:\n\nname_long country name\ncoffee_production_2016 and coffee_production_2017 contain estimated values for coffee production in units of 60-kg bags per year.\n\nA left join, which preserves the first dataset, merges world with coffee_data, based on the common 'name_long' column:\n\nworld_coffee = pd.merge(world, coffee_data, on='name_long', how='left')\nworld_coffee\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\ngeometry\ncoffee_production_2016\ncoffee_production_2017\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\nMULTIPOLYGON (((-180.00000 -16....\nNaN\nNaN\n\n\n1\nTZ\nTanzania\nAfrica\n...\nMULTIPOLYGON (((33.90371 -0.950...\n81\n66\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nMULTIPOLYGON (((-8.66559 27.656...\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nEurope\n...\nMULTIPOLYGON (((20.59025 41.855...\nNaN\nNaN\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\nMULTIPOLYGON (((-61.68000 10.76...\nNaN\nNaN\n\n\n176\nSS\nSouth Sudan\nAfrica\n...\nMULTIPOLYGON (((30.83385 3.5091...\nNaN\nNaN\n\n\n\n\n177 rows × 13 columns\n\n\n\nThe result is a GeoDataFrame object identical to the original world object, but with two new variables (coffee_production_2016 and coffee_production_2017) on coffee production. This can be plotted as a map, as illustrated in Figure 2.3:\n\nbase = world_coffee.plot(color='white', edgecolor='lightgrey')\ncoffee_map = world_coffee.plot(ax=base, column='coffee_production_2017')\ncoffee_map.set_title('Coffee production');\n\n\n\n\nFigure 2.3: World coffee production, thousand 60-kg bags by country, in 2017 (source: International Coffee Organization).\n\n\n\n\nTo work, attribute-based joins need a ‘key variable’ in both datasets (on parameter of pd.merge). In the above example, both world_coffee and world DataFrames contained a column called name_long. (By default pd.merge uses all columns with matching names. However, it is recommended to explicitly specify the names of the columns to be used for matching, like we did in the last example.)\nIn case where column names are not the same, you can use left_on and right_on to specify the respective columns.\nNote that the result world_coffee has the same number of rows as the original dataset world. Although there are only 47 rows in coffee_data, all 177 country records are kept intact in world_coffee. Rows in the original dataset with no match are assigned np.nan values for the new coffee production variables. This is a characteristic of a left join (specified with how='left') and is what we typically want to do.\nWhat if we only want to keep countries that have a match in the key variable? In that case an inner join can be used:\n\npd.merge(world, coffee_data, on='name_long', how='inner')\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\ngeometry\ncoffee_production_2016\ncoffee_production_2017\n\n\n\n\n0\nTZ\nTanzania\nAfrica\n...\nMULTIPOLYGON (((33.90371 -0.950...\n81\n66\n\n\n1\nPG\nPapua New Guinea\nOceania\n...\nMULTIPOLYGON (((141.00021 -2.60...\n114\n74\n\n\n2\nID\nIndonesia\nAsia\n...\nMULTIPOLYGON (((104.36999 -1.08...\n742\n360\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42\nET\nEthiopia\nAfrica\n...\nMULTIPOLYGON (((47.78942 8.0030...\n215\n283\n\n\n43\nUG\nUganda\nAfrica\n...\nMULTIPOLYGON (((33.90371 -0.950...\n408\n443\n\n\n44\nRW\nRwanda\nAfrica\n...\nMULTIPOLYGON (((30.41910 -1.134...\n36\n42\n\n\n\n\n45 rows × 13 columns\n\n\n\nAn alternative way to join two (Geo)DataFrames is the aptly called join function:\n\nworld.join(coffee_data.set_index('name_long'), on='name_long', how='inner')\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\ngeometry\ncoffee_production_2016\ncoffee_production_2017\n\n\n\n\n1\nTZ\nTanzania\nAfrica\n...\nMULTIPOLYGON (((33.90371 -0.950...\n81\n66\n\n\n7\nPG\nPapua New Guinea\nOceania\n...\nMULTIPOLYGON (((141.00021 -2.60...\n114\n74\n\n\n8\nID\nIndonesia\nAsia\n...\nMULTIPOLYGON (((104.36999 -1.08...\n742\n360\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n165\nET\nEthiopia\nAfrica\n...\nMULTIPOLYGON (((47.78942 8.0030...\n215\n283\n\n\n168\nUG\nUganda\nAfrica\n...\nMULTIPOLYGON (((33.90371 -0.950...\n408\n443\n\n\n169\nRW\nRwanda\nAfrica\n...\nMULTIPOLYGON (((30.41910 -1.134...\n36\n42\n\n\n\n\n45 rows × 13 columns\n\n\n\nNote that in this case, we need to set the index of coffee_data to the name_long values to avoid error messages.\n\n\n2.3.4 Creating attributes and removing spatial information\nOften, we would like to create a new column based on already existing columns. For example, we want to calculate population density for each country. For this we need to divide a population column, here pop, by an area column, here area_km2. Note that we are working on a copy of world named world2 so that we do not modify the original layer:\n\nworld2 = world.copy()\nworld2['pop_dens'] = world2['pop'] / world2['area_km2']\nworld2\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\ngdpPercap\ngeometry\npop_dens\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n8222\nMULTIPOLYGON (((-180.00000 -16....\n46\n\n\n1\nTZ\nTanzania\nAfrica\n...\n2402\nMULTIPOLYGON (((33.90371 -0.950...\n56\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nEurope\n...\n8698\nMULTIPOLYGON (((20.59025 41.855...\n162\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\n31182\nMULTIPOLYGON (((-61.68000 10.76...\n175\n\n\n176\nSS\nSouth Sudan\nAfrica\n...\n1936\nMULTIPOLYGON (((30.83385 3.5091...\n18\n\n\n\n\n177 rows × 12 columns\n\n\n\nTo paste (i.e., concatenate) together existing columns, we can use the ordinary Python string operator +, as if we are working with individual strings rather than Series. For example, we want to combine the continent and region_un columns into a new column named con_reg, using ':' as a separator. Subsequesntly, we remove the original columns using .drop:\n\nworld2['con_reg'] = world['continent'] + ':' + world2['region_un']\nworld2 = world2.drop(['continent', 'region_un'], axis=1)\nworld2\n\n\n\n\n\n\n\n\niso_a2\nname_long\nsubregion\n...\ngeometry\npop_dens\ncon_reg\n\n\n\n\n0\nFJ\nFiji\nMelanesia\n...\nMULTIPOLYGON (((-180.00000 -16....\n46\nOceania:Oceania\n\n\n1\nTZ\nTanzania\nEastern Africa\n...\nMULTIPOLYGON (((33.90371 -0.950...\n56\nAfrica:Africa\n\n\n2\nEH\nWestern Sahara\nNorthern Africa\n...\nMULTIPOLYGON (((-8.66559 27.656...\nNaN\nAfrica:Africa\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nSouthern Europe\n...\nMULTIPOLYGON (((20.59025 41.855...\n162\nEurope:Europe\n\n\n175\nTT\nTrinidad and Tobago\nCaribbean\n...\nMULTIPOLYGON (((-61.68000 10.76...\n175\nNorth America:Americas\n\n\n176\nSS\nSouth Sudan\nEastern Africa\n...\nMULTIPOLYGON (((30.83385 3.5091...\n18\nAfrica:Africa\n\n\n\n\n177 rows × 11 columns\n\n\n\nThe resulting sf object has a new column called con_reg representing the continent and region of each country, e.g., 'South America:Americas' for Argentina and other South America countries. The opposite operation, splitting one column into multiple columns based on a separator string, is done using the .str.split method. As a result we go back to the previous state of two separate continent and region_un columns (only that their position is now last, since they are newly created):\n\nworld2[['continent', 'region_un']] = world2['con_reg'] \\\n    .str.split(':', expand=True)\nworld2\n\n\n\n\n\n\n\n\niso_a2\nname_long\nsubregion\n...\ncon_reg\ncontinent\nregion_un\n\n\n\n\n0\nFJ\nFiji\nMelanesia\n...\nOceania:Oceania\nOceania\nOceania\n\n\n1\nTZ\nTanzania\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n2\nEH\nWestern Sahara\nNorthern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nSouthern Europe\n...\nEurope:Europe\nEurope\nEurope\n\n\n175\nTT\nTrinidad and Tobago\nCaribbean\n...\nNorth America:Americas\nNorth America\nAmericas\n\n\n176\nSS\nSouth Sudan\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n\n\n177 rows × 13 columns\n\n\n\nRenaming one or more columns can be done using the .rename method combined with the columns argument, which should be a dictionary of the form old_name:new_name. The following command, for example, renames the lengthy name_long column to simply name:\n\nworld2.rename(columns={'name_long': 'name'})\n\n\n\n\n\n\n\n\niso_a2\nname\nsubregion\n...\ncon_reg\ncontinent\nregion_un\n\n\n\n\n0\nFJ\nFiji\nMelanesia\n...\nOceania:Oceania\nOceania\nOceania\n\n\n1\nTZ\nTanzania\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n2\nEH\nWestern Sahara\nNorthern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nSouthern Europe\n...\nEurope:Europe\nEurope\nEurope\n\n\n175\nTT\nTrinidad and Tobago\nCaribbean\n...\nNorth America:Americas\nNorth America\nAmericas\n\n\n176\nSS\nSouth Sudan\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n\n\n177 rows × 13 columns\n\n\n\nTo change all column names at once, we assign a list of the “new” column names into the .columns property. The list must be of the same length as the number of columns (i.e., world.shape[1]). This is illustrated below, which outputs the same world2 object, but with very short names:\n\nnew_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'geom', 'i', 'j', 'k', 'l']\nworld2.columns = new_names\nworld2\n\n\n\n\n\n\n\n\na\nb\nc\n...\nj\nk\nl\n\n\n\n\n0\nFJ\nFiji\nMelanesia\n...\nOceania:Oceania\nOceania\nOceania\n\n\n1\nTZ\nTanzania\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n2\nEH\nWestern Sahara\nNorthern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nSouthern Europe\n...\nEurope:Europe\nEurope\nEurope\n\n\n175\nTT\nTrinidad and Tobago\nCaribbean\n...\nNorth America:Americas\nNorth America\nAmericas\n\n\n176\nSS\nSouth Sudan\nEastern Africa\n...\nAfrica:Africa\nAfrica\nAfrica\n\n\n\n\n177 rows × 13 columns\n\n\n\nTo reorder columns, we can pass a modified columns list to the subsetting operator [. For example, the following expressions reorder world2 columns in reverse alphabetical order:\n\nnames = sorted(world2.columns, reverse=True)\nworld2 = world2[names]\nworld2\n\n\n\n\n\n\n\n\nl\nk\nj\n...\nc\nb\na\n\n\n\n\n0\nOceania\nOceania\nOceania:Oceania\n...\nMelanesia\nFiji\nFJ\n\n\n1\nAfrica\nAfrica\nAfrica:Africa\n...\nEastern Africa\nTanzania\nTZ\n\n\n2\nAfrica\nAfrica\nAfrica:Africa\n...\nNorthern Africa\nWestern Sahara\nEH\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nEurope\nEurope\nEurope:Europe\n...\nSouthern Europe\nKosovo\nXK\n\n\n175\nAmericas\nNorth America\nNorth America:Americas\n...\nCaribbean\nTrinidad and Tobago\nTT\n\n\n176\nAfrica\nAfrica\nAfrica:Africa\n...\nEastern Africa\nSouth Sudan\nSS\n\n\n\n\n177 rows × 13 columns\n\n\n\nEach of these attribute data operations, even though they are defined in the pandas package and applicable to any DataFrame, preserve the geometry column and the GeoDataFrame class. Sometimes, however, it makes sense to remove the geometry, for example to speed-up aggregation or to export just the attribute data for statistical analysis. To go from GeoDataFrame to DataFrame we need to:\n\nDrop the geometry column\nConvert from GeoDataFrame into a DataFrame\n\nFor example:\n\nworld2 = world2.drop('geom', axis=1)\nworld2 = pd.DataFrame(world2)\nworld2\n\n\n\n\n\n\n\n\nl\nk\nj\n...\nc\nb\na\n\n\n\n\n0\nOceania\nOceania\nOceania:Oceania\n...\nMelanesia\nFiji\nFJ\n\n\n1\nAfrica\nAfrica\nAfrica:Africa\n...\nEastern Africa\nTanzania\nTZ\n\n\n2\nAfrica\nAfrica\nAfrica:Africa\n...\nNorthern Africa\nWestern Sahara\nEH\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nEurope\nEurope\nEurope:Europe\n...\nSouthern Europe\nKosovo\nXK\n\n\n175\nAmericas\nNorth America\nNorth America:Americas\n...\nCaribbean\nTrinidad and Tobago\nTT\n\n\n176\nAfrica\nAfrica\nAfrica:Africa\n...\nEastern Africa\nSouth Sudan\nSS\n\n\n\n\n177 rows × 12 columns"
  },
  {
    "objectID": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "href": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "title": "2  Attribute data operations",
    "section": "2.4 Manipulating raster objects",
    "text": "2.4 Manipulating raster objects\n\n2.4.1 Raster subsetting\nWhen using rasterio, raster values are accessible through a numpy array, which can be imported with the .read method:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThen, we can access any subset of cell values using numpy methods, e.g.:\n\nelev[0, 0]  ## Value at row 1, column 1\n\n1\n\n\nCell values can be modified by overwriting existing values in conjunction with a subsetting operation, e.g. to set the upper left cell of elev to 0:\n\nelev[0, 0] = 0\nelev\n\narray([[ 0,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nMultiple cells can also be modified in this way:\n\nelev[0, 0:3] = 0\nelev\n\narray([[ 0,  0,  0,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\n\n2.4.2 Summarizing raster objects\nGlobal summaries of raster values can be calculated by applying numpy summary functions on the array with raster values, e.g. np.mean:\n\nnp.mean(elev)\n\n18.333333333333332\n\n\nNote that “No Data”-safe functions–such as np.nanmean—should be used in case the raster contains “No Data” values which need to be ignored. Before we can demontrate that, we must convert the array from int to float, as int arrays cannot contain np.nan (due to computer memory limitations):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1\n\narray([[ 0.,  0.,  0.,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nNow we can insert an np.nan value into the array. (Trying to do so in the original elev array raises an error, try it to see for yourself)\n\nelev1[0, 2] = np.nan\nelev1\n\narray([[ 0.,  0., nan,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nWith the np.nan value inplace, the summary value becomes unknown:\n\nnp.mean(elev1)\n\nnan\n\n\nTherefore, we need to ignore the “No Data” value(s):\n\nnp.nanmean(elev1)\n\n18.857142857142858\n\n\nRaster value statistics can be visualized in a variety of ways. One approach is to “flatten” the raster values into a one-dimensional array, then use a graphical function such as plt.hist or plt.boxplot (from matplotlib.pyplot). For example:\n\nx = elev.flatten()\nplt.hist(x);"
  },
  {
    "objectID": "03-attribute-operations.html#exercises",
    "href": "03-attribute-operations.html#exercises",
    "title": "2  Attribute data operations",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises"
  },
  {
    "objectID": "04-spatial-operations.html#prerequisites",
    "href": "04-spatial-operations.html#prerequisites",
    "title": "3  Spatial data operations",
    "section": "3.1 Prerequisites",
    "text": "3.1 Prerequisites\nLet’s import the required packages:\n\nimport shapely\nimport geopandas as gpd\nimport numpy as np\nimport os\nimport rasterio\nimport scipy.ndimage\nimport rasterio.plot\nimport rasterio.merge\nimport rasterio.features\n\nand load the sample data for this chapter:\n\nnz = gpd.read_file('data/nz.gpkg')\nnz_height = gpd.read_file('data/nz_height.gpkg')\nworld = gpd.read_file('data/world.gpkg')\ncycle_hire = gpd.read_file('data/cycle_hire.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_multi_rast = rasterio.open('data/landsat.tif')\nsrc_grain = rasterio.open('data/grain.tif')"
  },
  {
    "objectID": "04-spatial-operations.html#introduction",
    "href": "04-spatial-operations.html#introduction",
    "title": "3  Spatial data operations",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction"
  },
  {
    "objectID": "04-spatial-operations.html#sec-spatial-vec",
    "href": "04-spatial-operations.html#sec-spatial-vec",
    "title": "3  Spatial data operations",
    "section": "3.3 Spatial operations on vector data",
    "text": "3.3 Spatial operations on vector data\n\n3.3.1 Spatial subsetting\nSpatial subsetting is the process of taking a spatial object and returning a new object containing only features that relate in space to another object. Analogous to attribute subsetting (covered in Section 2.3.1), subsets of GeoDataFrames can be created with square bracket ([) operator using the syntax x[y], where x is an GeoDataFrame from which a subset of rows/features will be returned, and y is the ‘subsetting object’. y, in turn, can be created using one of the binary geometry relation methods, such as .intersects (see Section 3.3.2).\nTo demonstrate spatial subsetting, we will use the nz and nz_height layers, which contain geographic data on the 16 main regions and 101 highest points in New Zealand, respectively (Figure 3.1), in a projected coordinate system. The following lines of code create an object representing Canterbury (canterbury), then use spatial subsetting to return all high points in the region (canterbury_height):\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\ncanterbury\n\n\n\n\n\n\n\n\nName\nIsland\nLand_area\n...\nMedian_income\nSex_ratio\ngeometry\n\n\n\n\n10\nCanterbury\nSouth\n44504.499091\n...\n30100\n0.975327\nMULTIPOLYGON (((1686901.914 535...\n\n\n\n\n1 rows × 7 columns\n\n\n\n\n# Does each 'nz_height' point intersect with 'canterbury'?\nsel = nz_height.intersects(canterbury['geometry'].iloc[0])\nsel\n\n0      False\n1      False\n       ...  \n99     False\n100    False\nLength: 101, dtype: bool\n\n\n\ncanterbury_height = nz_height[sel]\ncanterbury_height\n\n\n\n\n\n\n\n\nt50_fid\nelevation\ngeometry\n\n\n\n\n4\n2362630\n2749\nPOINT (1378169.600 5158491.453)\n\n\n5\n2362814\n2822\nPOINT (1389460.041 5168749.086)\n\n\n...\n...\n...\n...\n\n\n93\n2380300\n2711\nPOINT (1654213.379 5349962.973)\n\n\n94\n2380308\n2885\nPOINT (1654898.622 5350462.779)\n\n\n\n\n70 rows × 3 columns\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ncanterbury.plot(ax=base, color='grey', edgecolor='none')\ncanterbury_height.plot(ax=base, color='None', edgecolor='red')\naxes[0].set_title('Original')\naxes[1].set_title('Subset (intersects)');\n\n\n\n\nFigure 3.1: Spatial subsetting of points by intersection with polygon\n\n\n\n\nLike in attribute subsetting (Section 2.3.1), we are using a boolean series (sel), of the same length as the number of rows in the filtered table (nz_height), created based on a condition applied on itself. The difference is that the condition is not a comparison of attribute values, but an evaluation of a spatial relation. Namely, we evaluate whether each geometry of nz_height intersects with canterbury geometry, using the .intersects method.\nVarious topological relations can be used for spatial subsetting which determine the type of spatial relationship that features in the target object must have with the subsetting object to be selected. These include touches, crosses or within, as we will see shortly in Section 3.3.2. The most commonly used method .intersects method which we used in the last example is a ‘catch all’ topological relation, that will return features in the target that touch, cross or are within the source ‘subsetting’ object. Alternatively, we can evaluate other methods, such as .disjoint to obtain all points that do not intersect with Canterbury:\n\n# Is each 'nz_height' point disjoint from 'canterbury'?\nsel = nz_height.disjoint(canterbury['geometry'].iloc[0])\ncanterbury_height2 = nz_height[sel]\n\nas shown in Figure 3.2:\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ncanterbury.plot(ax=base, color='grey', edgecolor='none')\ncanterbury_height2.plot(ax=base, color='None', edgecolor='red');\naxes[0].set_title('Original')\naxes[1].set_title('Subset (disjoint)');\n\n\n\n\nFigure 3.2: Spatial subsetting of points disjoint from a polygon\n\n\n\n\nIn case we need to subset according to several geometries at once, e.g., find out which points intersect with both Canterbury and Southland, we can dissolve the filtering subset before applying the .intersects (or any other) operator:\n\ncanterbury_southland = nz[(nz['Name'] == 'Canterbury') | (nz['Name'] == 'Southland')]\ncanterbury_southland = canterbury_southland.unary_union\ncanterbury_southland\n\n\n\n\n\nsel = nz_height.intersects(canterbury_southland)\ncanterbury_southland_height = nz_height[sel]\ncanterbury_southland_height\n\n\n\n\n\n\n\n\nt50_fid\nelevation\ngeometry\n\n\n\n\n0\n2353944\n2723\nPOINT (1204142.603 5049971.287)\n\n\n4\n2362630\n2749\nPOINT (1378169.600 5158491.453)\n\n\n...\n...\n...\n...\n\n\n93\n2380300\n2711\nPOINT (1654213.379 5349962.973)\n\n\n94\n2380308\n2885\nPOINT (1654898.622 5350462.779)\n\n\n\n\n71 rows × 3 columns\n\n\n\nor, alternatively, we can use the .isin function to filter the regions and use .overlay to calculate the pairwise intersection between the canterbury_southland GeoDataFrame (with two rows for Canterbury and Southland, respectively) and the nz_height GeoDataFrame:\n\ncanterbury_southland = nz[nz['Name'].isin(['Canterbury', 'Southland'])]\ncanterbury_southland\nnz_height.overlay(canterbury_southland)\n\n\n\n\n\n\n\n\nt50_fid\nelevation\nName\n...\nMedian_income\nSex_ratio\ngeometry\n\n\n\n\n0\n2353944\n2723\nSouthland\n...\n29500\n0.978507\nPOINT (1204142.603 5049971.287)\n\n\n1\n2362630\n2749\nCanterbury\n...\n30100\n0.975327\nPOINT (1378169.600 5158491.453)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69\n2380300\n2711\nCanterbury\n...\n30100\n0.975327\nPOINT (1654213.379 5349962.973)\n\n\n70\n2380308\n2885\nCanterbury\n...\n30100\n0.975327\nPOINT (1654898.622 5350462.779)\n\n\n\n\n71 rows × 9 columns\n\n\n\nThe resulting subset is shown in Figure 3.3:\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ncanterbury_southland.plot(ax=base, color='grey', edgecolor='none')\ncanterbury_southland_height.plot(ax=base, color='None', edgecolor='red')\naxes[0].set_title('Original')\naxes[1].set_title('Subset (intersects)');\n\n\n\n\nFigure 3.3: Spatial subsetting of points by intersection with more that one polygon\n\n\n\n\nThe next section further explores different types of spatial relation, also known as binary predicates (of which .intersects and .disjoint are two examples), that can be used to identify whether or not two features are spatially related or not.\n\n\n3.3.2 Topological relations\n\nTopological relations describe the spatial relationships between objects. “Binary topological relationships”, to give them their full name, are logical statements (in that the answer can only be True or False) about the spatial relationships between two objects defined by ordered sets of points (typically forming points, lines and polygons) in two or more dimensions (Egenhofer and Herring 1990). That may sound rather abstract and, indeed, the definition and classification of topological relations is based on mathematical foundations first published in book form in 1966 (Spanier 1995), with the field of algebraic topology continuing into the 21st century (Dieck 2008).\nDespite their mathematical origins, topological relations can be understood intuitively with reference to visualizations of commonly used functions that test for common types of spatial relationships.  Figure 4.2 shows a variety of geometry pairs and their associated relations. The third and fourth pairs in Figure 4.2 (from left to right and then down) demonstrate that, for some relations, order is important: while the relations equals, intersects, crosses, touches and overlaps are symmetrical, meaning that if function(x, y) is true, function(y, x) will also by true, relations in which the order of the geometries are important such as contains and within are not. Notice that each geometry pair has a “DE-9IM” string such as FF2F11212, described in the next section.\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue.  The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string.\n\n\nIn shapely, functions testing for different types of topological relations are known as “relationships”. To see how topological relations work in practice, let’s create a simple reproducible example, building on the relations illustrated in Figure 4.2 and consolidating knowledge of how vector geometries are represented from a previous chapter (Section 1.2.3 and Section 1.2.5):\n\npoints = gpd.GeoSeries([\n  shapely.Point(0.2,0.1), \n  shapely.Point(0.7,0.2), \n  shapely.Point(0.4,0.8)\n])\nline = gpd.GeoSeries([\n  shapely.LineString([(0.4,0.2), (1,0.5)])\n])\npoly = gpd.GeoSeries([\n  shapely.Polygon([(0,0), (0,1), (1,1), (1,0.5), (0,0)])\n])\n\nThe resulting GeoSeries named points, line, and poly are visualized as follows:\n\nax = poly.plot(color='grey', edgecolor='red')\nax = line.plot(ax=ax, color='black', linewidth=7)\npoints.plot(ax=ax, color='none', edgecolor='black')\nfor x, y, label in zip(points.x, points.y, [1,2,3]):\n    ax.annotate(label, xy=(x, y), xytext=(3, 3), weight='bold', textcoords=\"offset points\")\n\n\n\n\nFigure 3.4: Points, line and polygon objects arranged to illustrate topological relations\n\n\n\n\nA simple query is: which of the points in points intersect in some way with polygon poly? The question can be answered by inspection (points 1 and 3 are touching and within the polygon, respectively). This question can be answered with the .intersects method, which reports whether or not each geometry in a GeoSeries (points) intersects with a single shapely geometry (poly.iloc[0]). When both inputs are a GeoSeries object, a pairwise evaluation takes place between geometries aligned by index (align=True, the default) or by position (align=False):\n\npoints.intersects(poly.iloc[0])\n\n0     True\n1    False\n2     True\ndtype: bool\n\n\nThe result shown above is a boolean Series. Its contents should match your intuition: positive (True) results are returned for the first and third point, and a negative result (False) for the second are outside the polygon’s border. Each value in this Series represents a feature in the first input (points).\nThe .apply method can be used to obtain a matrix of pairwise results. In this case, the result is a DataFrame, where each row represents a points geometry and each column represents a poly geometry. We’ll add another polygon to demonstrate:\n\npoly = gpd.GeoSeries([\n  shapely.Polygon([(0,0), (0,1), (1,1), (1,0.5), (0,0)]),\n  shapely.Polygon([(0,0), (1,0.5), (1,0), (0,0)])\n])\npoints.apply(lambda x: poly.intersects(x))\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTrue\nTrue\n\n\n1\nFalse\nTrue\n\n\n2\nTrue\nFalse\n\n\n\n\n\n\n\nThe .intersects method returns True even in cases where the features just touch: intersects is a ‘catch-all’ topological operation which identifies many types of spatial relation, as illustrated in Figure 4.2. More restrictive questions include which points lie within the polygon, and which features are on or contain a shared boundary with it? These can be answered as follows:\n\npoints.within(poly.iloc[0])\n\n0    False\n1    False\n2     True\ndtype: bool\n\n\n\npoints.touches(poly.iloc[0])\n\n0     True\n1    False\n2    False\ndtype: bool\n\n\nNote that although the first point touches the boundary polygon, it is not within it; the third point is within the polygon but does not touch any part of its border. The opposite of .intersects is .disjoint, which returns only objects that do not spatially relate in any way to the selecting object:\n\npoints.disjoint(poly.iloc[0])\n\n0    False\n1     True\n2    False\ndtype: bool\n\n\nAnother useful type of relation is “within distance”, where we detect features that intersect with the target buffered by particular distance. Buffer distance determines how close target objects need to be before they are selected. This can be done by literally buffering (Section 1.2.5) the target geometry, and evaluating intersection (.intersects, see above). Another way is to calculate the distances and compare them to the distance threshold:\n\npoints.distance(poly.iloc[0]) &lt; 0.2\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\nNote that although point 2 is more than 0.2 units of distance from the nearest vertex of poly, it is still selected when the distance is set to 0.2. This is because distance is measured to the nearest edge, in this case the part of the the polygon that lies directly above point 2 in Figure 4.2. We can verify the actual distance between point 2 and the polygon is 0.13, as follows:\n\npoints.iloc[1].distance(poly.iloc[0])\n\n0.13416407864998736\n\n\n\n\n3.3.3 DE-9IM strings\nTo complete…\n\n\n3.3.4 Spatial joining\nJoining two non-spatial datasets relies on a shared ‘key’ variable, as described in Section 2.3.3. Spatial data joining applies the same concept, but instead relies on spatial relations, described in the previous section. As with attribute data, joining adds new columns to the target object (the argument x in joining functions), from a source object (y).\nThe process is illustrated by the following example: imagine you have ten points randomly distributed across the Earth’s surface and you ask, for the points that are on land, which countries are they in? Implementing this idea in a reproducible example will build your geographic data handling skills and show how spatial joins work. The starting point is to create points that are randomly scattered over the Earth’s surface:\n\nnp.random.seed(11)  ## set seed for reproducibility\nbb = world.total_bounds  ## the world's bounds\nx = np.random.uniform(low=bb[0], high=bb[2], size=10)\ny = np.random.uniform(low=bb[1], high=bb[3], size=10)\nrandom_points = gpd.points_from_xy(x, y, crs=4326)\nrandom_points = gpd.GeoSeries(random_points)\nrandom_points = gpd.GeoDataFrame({'geometry': random_points})\nrandom_points\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (-115.10291 36.78178)\n\n\n1\nPOINT (-172.98891 -71.02938)\n\n\n...\n...\n\n\n8\nPOINT (159.05039 -34.99599)\n\n\n9\nPOINT (126.28622 -62.49509)\n\n\n\n\n10 rows × 1 columns\n\n\n\nThe scenario illustrated in Figure 3.5 shows that the random_points object (top left) lacks attribute data, while the world (top right) has attributes, including country names shown for a sample of countries in the legend. Spatial joins are implemented with gpd.sjoin, as illustrated in the code chunk below. The output is the random_joined object which is illustrated in Figure 3.5 (bottom left). Before creating the joined dataset, we use spatial subsetting to create world_random, which contains only countries that contain random points, to verify the number of country names returned in the joined dataset should be four (see the top right panel of Figure 3.5).\n\n# Subset\nworld_random = world[world.intersects(random_points.unary_union)]\nworld_random\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n4\nUS\nUnited States\nNorth America\n...\n78.841463\n51921.984639\nMULTIPOLYGON (((-171.73166 63.7...\n\n\n18\nRU\nRussian Federation\nEurope\n...\n70.743659\n25284.586202\nMULTIPOLYGON (((-180.00000 64.9...\n\n\n52\nML\nMali\nAfrica\n...\n57.007000\n1865.160622\nMULTIPOLYGON (((-11.51394 12.44...\n\n\n159\nAQ\nAntarctica\nAntarctica\n...\nNaN\nNaN\nMULTIPOLYGON (((-180.00000 -89....\n\n\n\n\n4 rows × 11 columns\n\n\n\n\n# Spatial join\nrandom_joined = gpd.sjoin(random_points, world, how='left')\nrandom_joined\n\n\n\n\n\n\n\n\ngeometry\nindex_right\niso_a2\n...\npop\nlifeExp\ngdpPercap\n\n\n\n\n0\nPOINT (-115.10291 36.78178)\n4.0\nUS\n...\n318622525.0\n78.841463\n51921.984639\n\n\n1\nPOINT (-172.98891 -71.02938)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8\nPOINT (159.05039 -34.99599)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n\n\n9\nPOINT (126.28622 -62.49509)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 12 columns\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(8,4))\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[0][0])\nrandom_points.plot(ax=base, color='None', edgecolor='red')\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[0][1])\nworld_random.plot(ax=base, column='name_long')\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[1][0])\nrandom_joined.geometry.plot(ax=base, color='grey');\nrandom_joined.plot(ax=base, column='name_long', legend=True)\nfig.delaxes(axes[1][1]);\n\n\n\n\nFigure 3.5: Illustration of a spatial join. A new attribute variable is added to random points (top left) from source world object (top right) resulting in the data represented in the final panel.\n\n\n\n\n\n\n3.3.5 Non-overlapping joins\nSometimes two geographic datasets do not touch but still have a strong geographic relationship. The datasets cycle_hire and cycle_hire_osm, provide a good example. Plotting them shows that they are often closely related but they do not touch, as shown in Figure 3.6:\n\nbase = cycle_hire.plot(edgecolor='blue', color='none')\ncycle_hire_osm.plot(ax=base, edgecolor='red', color='none');\n\n\n\n\nFigure 3.6: The spatial distribution of cycle hire points in London based on official data (blue) and OpenStreetMap data (red).\n\n\n\n\nWe can check if any of the points are the same by creating a pairwise boolean matrix of .intersects relations, then evaluating whether any of the values in it is True. Note that the .to_numpy method is applied to go from a DataFrame to a numpy array, for which .any gives a global rather than a row-wise summary, which is what we want in this case:\n\nm = cycle_hire['geometry'].apply(\n  lambda x: cycle_hire_osm['geometry'].intersects(x)\n)\nm.to_numpy().any()\n\nFalse\n\n\nImagine that we need to join the capacity variable in cycle_hire_osm onto the official ‘target’ data contained in cycle_hire. This is when a non-overlapping join is needed. Spatial join (gpd.sjoin) along with buffered geometries can be used to do that. This is demonstrated below, using a threshold distance of 20 \\(m\\). Note that we transform the data to a projected CRS (27700) to use real buffer distances, in meters.\n\ncrs = 27700\ncycle_hire_buffers = cycle_hire.copy().to_crs(crs)\ncycle_hire_buffers['geometry'] = cycle_hire_buffers.buffer(20)\nz = gpd.sjoin(\n  cycle_hire_buffers, \n  cycle_hire_osm.to_crs(crs)\n)\nz\n\n\n\n\n\n\n\n\nid\nname_left\narea\n...\ncapacity\ncyclestreets_id\ndescription\n\n\n\n\n0\n1\nRiver Street\nClerkenwell\n...\n9.0\nNaN\nNaN\n\n\n1\n2\nPhillimore Gardens\nKensington\n...\n27.0\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n729\n765\nRanelagh Gardens\nFulham\n...\n29.0\nNaN\nNaN\n\n\n737\n773\nTallis Street\nTemple\n...\n14.0\nNaN\nNaN\n\n\n\n\n458 rows × 12 columns\n\n\n\nNote that the number of rows in the joined result is greater than the target. This is because some cycle hire stations in cycle_hire_buffers have multiple matches in cycle_hire_osm. To aggregate the values for the overlapping points and return the mean, we can use the aggregation methods learned in Section 2.3.2, resulting in an object with the same number of rows as the target. We also go back from buffers to points using .centroid:\n\nz = z[['id', 'capacity', 'geometry']] \\\n    .dissolve(by='id', aggfunc='mean') \\\n    .reset_index()\nz['geometry'] = z.centroid\nz\n\n\n\n\n\n\n\n\nid\ngeometry\ncapacity\n\n\n\n\n0\n1\nPOINT (531203.517 182832.066)\n9.0\n\n\n1\n2\nPOINT (525208.067 179391.922)\n27.0\n\n\n...\n...\n...\n...\n\n\n436\n765\nPOINT (524652.998 175817.001)\n29.0\n\n\n437\n773\nPOINT (531435.032 180916.010)\n14.0\n\n\n\n\n438 rows × 3 columns\n\n\n\nThe capacity of nearby stations can be verified by comparing a plot of the capacity of the source cycle_hire_osm data with the results in this new object z (Figure 3.7):\n\n# Input\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\ncycle_hire_osm.plot(column='capacity', legend=True, ax=ax);\n# Join result\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nz.plot(column='capacity', legend=True, ax=ax);\n\n\n\n\n\n\n\n(a) Input\n\n\n\n\n\n\n\n(b) Join result\n\n\n\n\nFigure 3.7: Non-overlapping join\n\n\n\n\n\n3.3.6 Spatial aggregation\nAs with attribute data aggregation, spatial data aggregation condenses data: aggregated outputs have fewer rows than non-aggregated inputs. Statistical aggregating functions, such as mean, average, or sum, summarise multiple values of a variable, and return a single value per grouping variable. Section 2.3.2 demonstrated how the .groupby method, combined with summary functions such as .sum, condense data based on attribute variables. This section shows how grouping by spatial objects can be acheived using spatial joins combined with non-spatial aggregation.\nReturning to the example of New Zealand, imagine you want to find out the average height of high points in each region. It is the geometry of the source (nz) that defines how values in the target object (nz_height) are grouped. This can be done in three steps:\n\nFiguring out which nz region each nz_height point falls in—using gpd.sjoin\nSummarizing the average elevation per region—using .groupby and .mean\nJoining the result back to nz—using pd.merge\n\nFirst, we ‘attach’ the region classification of each point, using spatial join. Note that we are using the minimal set of columns required: the geometries (for the spatial join to work), the point elevation (to later calculate an average), and the region name (to use as key when joining the results back to nz).\n\nnz_height2 = gpd.sjoin(\n  nz_height[['elevation', 'geometry']], \n  nz[['Name', 'geometry']], \n  how='left'\n)\nnz_height2\n\n\n\n\n\n\n\n\nelevation\ngeometry\nindex_right\nName\n\n\n\n\n0\n2723\nPOINT (1204142.603 5049971.287)\n12\nSouthland\n\n\n1\n2820\nPOINT (1234725.325 5048309.302)\n11\nOtago\n\n\n...\n...\n...\n...\n...\n\n\n99\n2720\nPOINT (1822262.592 5650428.656)\n2\nWaikato\n\n\n100\n2732\nPOINT (1822492.184 5650492.304)\n2\nWaikato\n\n\n\n\n101 rows × 4 columns\n\n\n\nSecond, we calculate the average elevation:\n\nnz_height3 = nz_height2.groupby('Name')[['elevation']].mean()\nnz_height3\n\n\n\n\n\n\n\n\nelevation\n\n\nName\n\n\n\n\n\nCanterbury\n2994.600000\n\n\nManawatu-Wanganui\n2777.000000\n\n\n...\n...\n\n\nWaikato\n2734.333333\n\n\nWest Coast\n2889.454545\n\n\n\n\n7 rows × 1 columns\n\n\n\nThe third and final step is joining the averages with the nz layer:\n\nnz_height4 = pd.merge(nz[['Name', 'geometry']], nz_height3, on='Name', how='left')\nnz_height4\n\n\n\n\n\n\n\n\nName\ngeometry\nelevation\n\n\n\n\n0\nNorthland\nMULTIPOLYGON (((1745493.196 600...\nNaN\n\n\n1\nAuckland\nMULTIPOLYGON (((1803822.103 590...\nNaN\n\n\n...\n...\n...\n...\n\n\n14\nNelson\nMULTIPOLYGON (((1624866.279 541...\nNaN\n\n\n15\nMarlborough\nMULTIPOLYGON (((1686901.914 535...\n2720.0\n\n\n\n\n16 rows × 3 columns\n\n\n\nWe now have create the nz_height4 layer, which gives the average nz_height elevation value per polygon. The result is shown in Figure 3.8. Note that the missing_kwds part determines the style of geometries where the symbology attribute (elevation) is missing, because the were no nz_height points overlapping with them. The default is to omit them, which is usually not what we want. With {'color':'none','edgecolor':'black'}, those polygons are shown with black outline and no fill.\n\nnz_height4.plot(\n  column='elevation', \n  legend=True,\n  cmap='Blues', edgecolor='black',\n  missing_kwds={'color': 'none', 'edgecolor': 'black'}\n);\n\n\n\n\nFigure 3.8: Average height of the top 101 high points across the regions of New Zealand\n\n\n\n\n\n\n3.3.7 Joining incongruent layers\nSpatial congruence is an important concept related to spatial aggregation. An aggregating object (which we will refer to as y) is congruent with the target object (x) if the two objects have shared borders. Often this is the case for administrative boundary data, whereby larger units—such as Middle Layer Super Output Areas (MSOAs) in the UK or districts in many other European countries—are composed of many smaller units.\nIncongruent aggregating objects, by contrast, do not share common borders with the target (Qiu, Zhang, and Zhou 2012). This is problematic for spatial aggregation (and other spatial operations) illustrated in Figure 3.9: aggregating the centroid of each sub-zone will not return accurate results. Areal interpolation overcomes this issue by transferring values from one set of areal units to another, using a range of algorithms including simple area weighted approaches and more sophisticated approaches such as ‘pycnophylactic’ methods (Tobler 1979).\nTo demonstrate, we will create a “synthetic” layer comprising a regular grid of rectangles of size \\(100\\times100\\) \\(km\\), covering the extent of the nz layer:\n\nxmin, ymin, xmax, ymax = nz.total_bounds\nres = 100000\ncols = list(range(int(np.floor(xmin)), int(np.ceil(xmax+res)), res))\nrows = list(range(int(np.floor(ymin)), int(np.ceil(ymax+res)), res))\nrows.reverse()\npolygons = []\nfor x in cols:\n    for y in rows:\n        polygons.append( shapely.Polygon([(x,y), (x+res, y), (x+res, y-res), (x, y-res)]) )\ngrid = gpd.GeoDataFrame({'geometry': polygons}, crs=nz.crs)\nsel = grid.intersects(shapely.box(*nz.total_bounds))\ngrid = grid[sel]\ngrid['id'] = grid.index\ngrid\n\n\n\n\n\n\n\n\ngeometry\nid\n\n\n\n\n0\nPOLYGON ((1090143.000 6248536.0...\n0\n\n\n1\nPOLYGON ((1090143.000 6148536.0...\n1\n\n\n...\n...\n...\n\n\n157\nPOLYGON ((1990143.000 4948536.0...\n157\n\n\n158\nPOLYGON ((1990143.000 4848536.0...\n158\n\n\n\n\n150 rows × 2 columns\n\n\n\nas shown in Figure 3.9.\n\nbase = grid.plot(color='none', edgecolor='grey')\nnz.plot(ax=base, column='Population', edgecolor='black', legend=True, cmap='viridis_r');\nbase.set_title('Population');\n\n\n\n\nFigure 3.9: The nz layer and a regular grid of rectangles\n\n\n\n\nOur goal, now, is to “transfer” the Population attribute to the rectangular grid polygons, which is an example of a join between incongruent layers. To do that, we basically need to calculate–for each grid cell—the weighted sum of the population in nz polygons coinciding with that cell. The weights in the weighted sum calculation are the ratios between the area of the coinciding “part” out of the entire nz polygon. That is, we (inevitably) assume that the population in each nz polygon is equally distributed across space, therefore a partial nz polygon contains the respective partial population size.\nWe start with calculating the entire area of each nz polygon, as follows, using the .area method (see Section 1.2.7):\n\nnz['area'] = nz.area\nnz\n\n\n\n\n\n\n\n\nName\nIsland\nLand_area\n...\nSex_ratio\ngeometry\narea\n\n\n\n\n0\nNorthland\nNorth\n12500.561149\n...\n0.942453\nMULTIPOLYGON (((1745493.196 600...\n1.289058e+10\n\n\n1\nAuckland\nNorth\n4941.572557\n...\n0.944286\nMULTIPOLYGON (((1803822.103 590...\n4.911565e+09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14\nNelson\nSouth\n422.195242\n...\n0.925967\nMULTIPOLYGON (((1624866.279 541...\n4.080754e+08\n\n\n15\nMarlborough\nSouth\n10457.745485\n...\n0.957792\nMULTIPOLYGON (((1686901.914 535...\n1.046485e+10\n\n\n\n\n16 rows × 8 columns\n\n\n\nNext, we use the .overlay method to calculate the pairwise intersections between nz and grid, hereby named nz_grid. We also calculate the area of the intersections, hereby named area_sub. If an nz polygon was completely within a single grid polygon, then area_sub is going to be equal to area; otherwise, it is going to be smaller:\n\nnz_grid = nz.overlay(grid)\nnz_grid = nz_grid[['id', 'area', 'Population', 'geometry']]\nnz_grid['area_sub'] = nz_grid.area\nnz_grid\n\n\n\n\n\n\n\n\nid\narea\nPopulation\ngeometry\narea_sub\n\n\n\n\n0\n64\n1.289058e+10\n175500.0\nPOLYGON ((1586362.965 6168009.0...\n3.231015e+08\n\n\n1\n80\n1.289058e+10\n175500.0\nPOLYGON ((1590143.000 6162776.6...\n4.612641e+08\n\n\n...\n...\n...\n...\n...\n...\n\n\n108\n87\n4.080754e+08\n51400.0\nPOLYGON ((1649908.695 5455398.2...\n1.716260e+07\n\n\n109\n87\n1.046485e+10\n46200.0\nMULTIPOLYGON (((1678688.086 545...\n4.526248e+08\n\n\n\n\n110 rows × 5 columns\n\n\n\nThe resulting layer nz_grid is shown in Figure 3.10.\n\nbase = grid.plot(color='none', edgecolor='grey')\nnz_grid.plot(ax=base, column='Population', edgecolor='black', legend=True, cmap='viridis_r');\n\n\n\n\nFigure 3.10: The nz layer and a regular grid of rectangles: after an overlay operation\n\n\n\n\nNote that each of the “intersections” still holds the Population attribute of its “origin” feature of nz, as depicted in Figure 3.10. The real population size of each nz_grid feature, however, is smaller (or equal), depending on the geographic area proportion that it occupies out of the original nz feature. To make the “correction”, we first calculate the ratio (area_prop) and then multiply it by the population. The new (lowercase) attribute population now has the correct estimate of population sizes in nz_grid:\n\nnz_grid['area_prop'] = nz_grid['area_sub'] / nz_grid['area']\nnz_grid['population'] = nz_grid['Population'] * nz_grid['area_prop']\nnz_grid\n\n\n\n\n\n\n\n\nid\narea\nPopulation\n...\narea_sub\narea_prop\npopulation\n\n\n\n\n0\n64\n1.289058e+10\n175500.0\n...\n3.231015e+08\n0.025065\n4398.897141\n\n\n1\n80\n1.289058e+10\n175500.0\n...\n4.612641e+08\n0.035783\n6279.925114\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n108\n87\n4.080754e+08\n51400.0\n...\n1.716260e+07\n0.042057\n2161.752203\n\n\n109\n87\n1.046485e+10\n46200.0\n...\n4.526248e+08\n0.043252\n1998.239223\n\n\n\n\n110 rows × 7 columns\n\n\n\nWhat is left to be done is to sum (see Section 2.3.2) the population in all parts forming the same grid cell:\n\nnz_grid = nz_grid.groupby('id')['population'].sum().reset_index()\nnz_grid\n\n\n\n\n\n\n\n\nid\npopulation\n\n\n\n\n0\n11\n67.533590\n\n\n1\n12\n15339.996965\n\n\n...\n...\n...\n\n\n55\n149\n31284.910446\n\n\n56\n150\n129.326331\n\n\n\n\n57 rows × 2 columns\n\n\n\nand join (see Section 2.3.3) them back to the grid layer:\n\ngrid = pd.merge(grid, nz_grid[['id', 'population']], on='id', how='left')\ngrid\n\n\n\n\n\n\n\n\ngeometry\nid\npopulation\n\n\n\n\n0\nPOLYGON ((1090143.000 6248536.0...\n0\nNaN\n\n\n1\nPOLYGON ((1090143.000 6148536.0...\n1\nNaN\n\n\n...\n...\n...\n...\n\n\n148\nPOLYGON ((1990143.000 4948536.0...\n157\nNaN\n\n\n149\nPOLYGON ((1990143.000 4848536.0...\n158\nNaN\n\n\n\n\n150 rows × 3 columns\n\n\n\nThe final result grid, with the incongruently-joined population attribute from nz, is shown in Figure 3.11.\n\nbase = grid.plot(column='population', edgecolor='black', legend=True, cmap='viridis_r');\nnz.plot(ax=base, color='none', edgecolor='grey', legend=True);\n\n\n\n\nFigure 3.11: The nz layer and a regular grid of rectangles: final result\n\n\n\n\nWe can demonstrate that, expectedly, the summed population in nz and grid is identical, even though the geometry is different (since we created grid to completely cover nz):\n\nnz['Population'].sum()\n\n4787200.0\n\n\n\ngrid['population'].sum()\n\n4787199.999999998\n\n\nThe procedure in this section is known as an area-weighted interpolation of a spatially extensive (e.g., population) variable. An area-weighted interpolation of a spatially intensive variable (e.g., population density) is almost identical, except that we would have to calculate the weighted .mean rather than .sum, to preserve the average rather than the sum.\n\n\n3.3.8 Distance relations\nWhile topological relations are binary — a feature either intersects with another or does not — distance relations are continuous. The distance between two objects is calculated with the distance method. The method is applied on a GeoSeries (or a GeoDataFrame), with the argument being an individual shapely geometry. The result is a Series of pairwise distances.\nThis is illustrated in the code chunk below, which finds the distance between the three highest point in New Zealand:\n\nnz_highest = nz_height \\\n  .sort_values(by='elevation', ascending=False) \\\n  .iloc[:3, :]\nnz_highest\n\n\n\n\n\n\n\n\nt50_fid\nelevation\ngeometry\n\n\n\n\n64\n2372236\n3724\nPOINT (1369317.630 5169132.284)\n\n\n63\n2372235\n3717\nPOINT (1369512.866 5168235.616)\n\n\n67\n2372252\n3688\nPOINT (1369381.942 5168761.875)\n\n\n\n\n\n\n\nand the geographic centroid of the Canterbury region, created in Section 4.2.1:\n\ncanterbury_centroid = canterbury.centroid.iloc[0]\n\nHere are the distances:\n\nnz_highest.distance(canterbury_centroid)\n\n64    115539.995747\n63    115390.248038\n67    115493.594066\ndtype: float64\n\n\nTo obtain a distance matrix, i.e., a pairwise set of distances between all combinations of features in objects x and y, we need to use the .apply method. This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co:\n\nsel = nz['Name'].str.contains('Canter|Otag')\nco = nz[sel]\nco\n\n\n\n\n\n\n\n\nName\nIsland\nLand_area\n...\nSex_ratio\ngeometry\narea\n\n\n\n\n10\nCanterbury\nSouth\n44504.499091\n...\n0.975327\nMULTIPOLYGON (((1686901.914 535...\n4.532656e+10\n\n\n11\nOtago\nSouth\n31186.309188\n...\n0.951169\nMULTIPOLYGON (((1335204.789 512...\n3.190356e+10\n\n\n\n\n2 rows × 8 columns\n\n\n\nThe distance matrix d is obtained as follows (technically speaking, this is a DataFrame). In plain language, we take the geometry from each each row in nz_height.iloc[:3, :], and apply the .distance method on co with that row as the argument:\n\nd = nz_height.iloc[:3, :].apply(lambda x: co.distance(x['geometry']), axis=1)\nd\n\n\n\n\n\n\n\n\n10\n11\n\n\n\n\n0\n123537.158269\n15497.717252\n\n\n1\n94282.773074\n0.000000\n\n\n2\n93018.560814\n0.000000\n\n\n\n\n\n\n\nNote that the distance between the second and third features in nz_height and the second feature in co is zero. This demonstrates the fact that distances between points and polygons refer to the distance to any part of the polygon: The second and third points in nz_height are in Otago, which can be verified by plotting them:\n\nbase = co.iloc[[1]].plot(color='none')\nnz_height.iloc[1:3, :].plot(ax=base);"
  },
  {
    "objectID": "04-spatial-operations.html#sec-spatial-ras",
    "href": "04-spatial-operations.html#sec-spatial-ras",
    "title": "3  Spatial data operations",
    "section": "3.4 Spatial operations on raster data",
    "text": "3.4 Spatial operations on raster data\n\n3.4.1 Spatial subsetting\nThe previous chapter (Section Section 2.4) demonstrated how to retrieve values associated with specific cell IDs or row and column combinations. Raster objects can also be extracted by location (coordinates) and other spatial objects. To use coordinates for subsetting, we can use the .sample method of a rasterio file connection object, combined with a list of coordinate tuples. The methods is demonstrated below to find the value of the cell that covers a point located at coordinates of 0.1, 0.1 in elev. The returned object is a generator:\n\nsrc_elev.sample([(0.1, 0.1)])\n\n&lt;generator object sample_gen at 0x7fa64af1b320&gt;\n\n\nIn case we want all values at once we can apply list. The result is 16:\n\nlist(src_elev.sample([(0.1, 0.1)]))\n\n[array([16], dtype=uint8)]\n\n\nAnother common use case of spatial subsetting is using a boolean mask, based on another raster with the same extent and resolution, or the original one, as illustrated in Figure 3.12. To do that, we “erase” the values in the array of one raster, according to another corresponding “mask” raster. For example, let us read the elev.tif raster array:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nand create a correspinding random mask:\n\nnp.random.seed(1)\nmask = np.random.choice([True, False], elev.shape)\nmask\n\narray([[False, False,  True,  True, False, False],\n       [False, False, False,  True,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True, False,  True,  True],\n       [False,  True,  True,  True, False,  True],\n       [ True,  True, False, False, False, False]])\n\n\nIn the code chunk above, we have created a mask object called mask with values randomly assigned to True and False. Next, we want to keep those values of elev which are False in mask (i.e., they are not masked). In other words, we want to mask elev with mask. The result is stored in a copy named elev1. To be able to store np.nan in the raster, we also need to convert it to float (see Section 2.4.2):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[mask] = np.nan\nelev1\n\narray([[ 1.,  2., nan, nan,  5.,  6.],\n       [ 7.,  8.,  9., nan, nan, 12.],\n       [nan, 14., 15., nan, nan, 18.],\n       [nan, nan, nan, 22., nan, nan],\n       [25., nan, nan, nan, 29., nan],\n       [nan, nan, 33., 34., 35., 36.]])\n\n\nThe result is shown in Figure 3.12.\n\nfig, ax = plt.subplots(ncols=3, figsize=(8,4))\nrasterio.plot.show(elev, ax=ax[0])\nrasterio.plot.show(mask, ax=ax[1])\nrasterio.plot.show(elev1, ax=ax[2])\nax[0].set_title('Original')\nax[1].set_title('Mask')\nax[2].set_title('Result');\n\n\n\n\nFigure 3.12: Original raster (left). Raster mask (middle). Output of masking a raster (right).\n\n\n\n\nThe above approach can be also used to replace some values (e.g., expected to be wrong) with np.nan:\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[elev1 &lt; 20] = np.nan\nelev1\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nThese operations are in fact Boolean local operations, since we compare cell-wise two rasters. The next subsection explores these and related operations in more detail.\n\n\n3.4.2 Map algebra\nThe term ‘map algebra’ was coined in the late 1970s to describe a “set of conventions, capabilities, and techniques” for the analysis of geographic raster and (although less prominently) vector data (Tomlin 1994). In this context, we define map algebra more narrowly, as operations that modify or summarise raster cell values, with reference to surrounding cells, zones, or statistical functions that apply to every cell.\nMap algebra operations tend to be fast, because raster datasets only implicitly store coordinates, hence the old adage “raster is faster but vector is corrector”. The location of cells in raster datasets can be calculated by using its matrix position and the resolution and origin of the dataset (stored in the header). For the processing, however, the geographic position of a cell is barely relevant as long as we make sure that the cell position is still the same after the processing. Additionally, if two or more raster datasets share the same extent, projection and resolution, one could treat them as matrices for the processing.\nThis is the way that map algebra works with the terra package. First, the headers of the raster datasets are queried and (in cases where map algebra operations work on more than one dataset) checked to ensure the datasets are compatible. Second, map algebra retains the so-called one-to-one locational correspondence, meaning that cells cannot move. This differs from matrix algebra, in which values change position, for example when multiplying or dividing matrices.\nMap algebra (or cartographic modeling with raster data) divides raster operations into four subclasses (Tomlin 1990), with each working on one or several grids simultaneously:\n\nLocal or per-cell operations\nFocal or neighborhood operations. Most often the output cell value is the result of a 3 x 3 input cell block\nZonal operations are similar to focal operations, but the surrounding pixel grid on which new values are computed can have irregular sizes and shapes\nGlobal or per-raster operations; that means the output cell derives its value potentially from one or several entire rasters\n\nThis typology classifies map algebra operations by the number of cells used for each pixel processing step and the type of the output. For the sake of completeness, we should mention that raster operations can also be classified by discipline such as terrain, hydrological analysis, or image classification. The following sections explain how each type of map algebra operations can be used, with reference to worked examples.\n\n\n3.4.3 Local operations\nLocal operations comprise all cell-by-cell operations in one or several layers. Raster algebra is a classical use case of local operations - this includes adding or subtracting values from a raster, squaring and multipling rasters. Raster algebra also allows logical operations such as finding all raster cells that are greater than a specific value (5 in our example below). Local operations are applied using the numpy array operations syntax, as demonstrated below:\nFirst, we need to read raster values:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nNow, any element-wise array operation can be applied. For example:\n\nelev + elev\n\narray([[ 2,  4,  6,  8, 10, 12],\n       [14, 16, 18, 20, 22, 24],\n       [26, 28, 30, 32, 34, 36],\n       [38, 40, 42, 44, 46, 48],\n       [50, 52, 54, 56, 58, 60],\n       [62, 64, 66, 68, 70, 72]], dtype=uint8)\n\n\nFigure 3.13 demonstrates a few more examples.\n\nfig, axes = plt.subplots(ncols=4, figsize=(8,4))\nrasterio.plot.show(elev + elev, ax=axes[0], cmap='Oranges')\nrasterio.plot.show(elev1 ** 2, ax=axes[1], cmap='Oranges')  # using elev here produces int8 overflows\nrasterio.plot.show(np.log(elev), ax=axes[2], cmap='Oranges')\nrasterio.plot.show(elev &gt; 5, ax=axes[3], cmap='Oranges')\naxes[0].set_title('elev+elev')\naxes[1].set_title('elev ** 2')\naxes[2].set_title('np.log(elev)')\naxes[3].set_title('elev &gt; 5');\n\n\n\n\nFigure 3.13: Examples of different local operations of the elev raster object: adding two rasters, squaring, applying logarithmic transformation, and performing a logical operation.\n\n\n\n\nAnother good example of local operations is the classification of intervals of numeric values into groups such as grouping a digital elevation model into low (class 1), middle (class 2) and high elevations (class 3). Here, we assign the raster values in the ranges 0–12, 12–24 and 24–36 are reclassified to take values 1, 2 and 3, respectively.\n\nrecl = elev.copy()\nrecl[(elev &gt; 0)  & (elev &lt;= 12)] = 1\nrecl[(elev &gt; 12) & (elev &lt;= 24)] = 2\nrecl[(elev &gt; 24) & (elev &lt;= 36)] = 3\n\nThe reclassified result is shown in Figure 3.14.\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nrasterio.plot.show(elev, ax=axes[0], cmap='Oranges')\nrasterio.plot.show(recl, ax=axes[1], cmap='Oranges')\naxes[0].set_title('Original')\naxes[1].set_title('Reclassified');\n\n\n\n\nFigure 3.14: Reclassifying a continuous raster into three categories.\n\n\n\n\nThe calculation of the normalized difference vegetation index (NDVI) is a well-known local (pixel-by-pixel) raster operation. It returns a raster with values between -1 and 1; positive values indicate the presence of living plants (mostly &gt; 0.2). NDVI is calculated from red and near-infrared (NIR) bands of remotely sensed imagery, typically from satellite systems such as Landsat or Sentinel. Vegetation absorbs light heavily in the visible light spectrum, and especially in the red channel, while reflecting NIR light, explaining the NVDI formula (Equation 3.1):\n\\[NDVI=\\frac{NIR-Red} {NIR+Red} \\tag{3.1}\\]\nLet’s calculate NDVI for the multispectral satellite file of the Zion National Park.\n\nmulti_rast = src_multi_rast.read()\nnir = multi_rast[3,:,:]\nred = multi_rast[2,:,:]\nndvi = (nir-red)/(nir+red)\n\nConvert values &gt;1 to “No Data”:\n\nndvi[ndvi&gt;1] = np.nan\n\nWhen plotting an RGB image using the show function, the function assumes that:\n\nValues are in the range [0,1] for floats, or [0,255] for integers (otherwise clipped)\nThe order of bands is RGB\n\nTo “prepare” the multi-band raster for show, we therefore reverse the order of bands (which is originally BGR+NIR), and divided by the maximum to set the maximum value at 1:\n\nmulti_rast_rgb = multi_rast[(2,1,0), :, :] / multi_rast.max()\n\nThe result is shown in Figure 3.15.\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nrasterio.plot.show(multi_rast_rgb, ax=axes[0], cmap='RdYlGn')\nrasterio.plot.show(ndvi, ax=axes[1], cmap='Greens')\naxes[0].set_title('RGB image')\naxes[1].set_title('NDVI');\n\n\n\n\nFigure 3.15: RGB image (left) and NDVI values (right) calculated for the example satellite file of the Zion National Park.\n\n\n\n\n\n\n3.4.4 Focal operations\nWhile local functions operate on one cell, though possibly from multiple layers, focal operations take into account a central (focal) cell and its neighbors. The neighborhood (also named kernel, filter or moving window) under consideration is typically of size 3-by-3 cells (that is the central cell and its eight surrounding neighbors), but can take on any other (not necessarily rectangular) shape as defined by the user. A focal operation applies an aggregation function to all cells within the specified neighborhood, uses the corresponding output as the new value for the the central cell, and moves on to the next central cell (Figure …). Other names for this operation are spatial filtering and convolution (Burrough, McDonnell, and Lloyd 2015).\nIn Python, the scipy.ndimage package has a comprehensive collection of functions to perform filtering of numpy arrays, such as:\n\nminimum_filter\nmaximum_filter\nuniform_filter (i.e., mean filter)\nmedian_filter etc.\n\nIn this group of functions, we define the shape of the moving window with either one of:\n\nsize—a single number or tuple, implying a filter of those dimensions\nfootprint—a boolean array, representing both the window shape and the identity of elements being included\n\nIn addition to specific built-in filters,\n\nconvolve applies the sum function after multiplying by a custom weights array\ngeneric_filter makes it possible to pass any custom function, where the user can specify any type of custom window-based calculatio.\n\nFor example, here we apply the minimum filter with window size of 3 on elev:\n\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\nelev_min = scipy.ndimage.minimum_filter(elev, size=3)\nelev_min\n\narray([[ 1,  1,  2,  3,  4,  5],\n       [ 1,  1,  2,  3,  4,  5],\n       [ 7,  7,  8,  9, 10, 11],\n       [13, 13, 14, 15, 16, 17],\n       [19, 19, 20, 21, 22, 23],\n       [25, 25, 26, 27, 28, 29]], dtype=uint8)\n\n\nSpecial care should be given to the edge pixels. How should they be calculated? scipy.ndimage gives several options through the mode parameter:\n\nreflect (the default)\nconstant\nnearest\nmirror\nwrap\n\nSometimes artificially extending raster edges is considered unsuitable. In other words, we may wish the resulting raster to contain pixel values with “complete” windows only, for example to have a uniform sample size or because values in all directions matter (such as in topographic calculations). There is no specific option not to extend edges in scipy.ndimage. However, to get the same effect, the edges of the filtered array can be assigned with nan, in a number of rows and columns according to filter size. For example, when using a filter of size=3, the first “layer” of pixels may be assigned with nan, reflecting the fact that these pixels have incomplete 3*3 neighborhoods:\n\nelev_min = elev_min.astype('float')\nelev_min[:, [0, -1]] = np.nan\nelev_min[[0, -1], :] = np.nan\nelev_min\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan,  1.,  2.,  3.,  4., nan],\n       [nan,  7.,  8.,  9., 10., nan],\n       [nan, 13., 14., 15., 16., nan],\n       [nan, 19., 20., 21., 22., nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWe can quickly check if the output meets our expectations. In our example, the minimum value has to be always the upper left corner of the moving window (remember we have created the input raster by row-wise incrementing the cell values by one starting at the upper left corner).\nFocal functions or filters play a dominant role in image processing. Low-pass or smoothing filters use the mean function to remove extremes. In the case of categorical data, we can replace the mean with the mode, which is the most common value. By contrast, high-pass filters accentuate features. The line detection Laplace and Sobel filters might serve as an example here.\nTerrain processing, the calculation of topographic characteristics such as slope, aspect and flow directions, relies on focal functions. The TerrainAttribute function from package richdem can be used to calculate common metrics, specified through the attrib argument, namely:\n\nslope_riserun Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_percentage Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_degrees Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_radians Horn (1981) doi: 10.1109/PROC.1981.11918\naspect Horn (1981) doi: 10.1109/PROC.1981.11918\ncurvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nplanform_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nprofile_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\n\n\n\n3.4.5 Zonal operations\nJust like focal operations, zonal operations apply an aggregation function to multiple raster cells. However, a second raster, usually with categorical values, defines the zonal filters (or ‘zones’) in the case of zonal operations, as opposed to a predefined neighborhood window in the case of focal operation presented in the previous section. Consequently, raster cells defining the zonal filter do not necessarily have to be neighbors. Our grain size raster is a good example, as illustrated in the right panel of Figure 3.2: different grain sizes are spread irregularly throughout the raster. Finally, the result of a zonal operation is a summary table grouped by zone which is why this operation is also known as zonal statistics in the GIS world. This is in contrast to focal operations which return a raster object.\nTo demonstrate, let us get back to the grain and elev rasters (Figure 3.2). To calculate zonal statistics, we use the arrays with raster values. The elev array was already imported earlier:\n\ngrain = src_grain.read(1)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nOur interntion is to calculate the average (or any other summary function, for that matter) of elevation in each zone defined by grain values. First, we can obtain the unique values defining the zones using np.unique:\n\nnp.unique(grain)\n\narray([0, 1, 2], dtype=uint8)\n\n\nNow, we can use dictionary conprehension to “split” the elev array into separate one-dimensional arrays with values per grain group, with keys being the unique grain values:\n\nz = {i: elev[grain == i] for i in np.unique(grain)}\nz\n\n{0: array([ 2,  7,  9, 10, 13, 16, 17, 19, 20, 35], dtype=uint8),\n 1: array([ 1,  3, 12, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32], dtype=uint8),\n 2: array([ 4,  5,  6,  8, 11, 14, 15, 18, 28, 31, 33, 34, 36], dtype=uint8)}\n\n\nAt this stage, we can expand the dictionary comprehension expression to calculate the mean elevation associated with each grain size class. Instead of placing the elevation values (elev[grain==i]) into the dictionary values, we place their mean (elev[grain==i].mean()):\n\nz = {i: elev[grain == i].mean() for i in np.unique(grain)}\nz\n\n{0: 14.8, 1: 21.153846153846153, 2: 18.692307692307693}\n\n\nThis returns the statistics for each category, here the mean elevation for each grain size class. For example, the mean elevation in pixels characterized by grain size 0 is 14.8, and so on.\n\n\n3.4.6 Global operations and distances\nGlobal operations are a special case of zonal operations with the entire raster dataset representing a single zone. The most common global operations are descriptive statistics for the entire raster dataset such as the minimum or maximum—we already discussed those in Section 2.4.2.\nAside from that, global operations are also useful for the computation of distance and weight rasters. In the first case, one can calculate the distance from each cell to specific target cells or vector geometries. For example, one might want to compute the distance to the nearest coast (see example in Section 5.7). We might also want to consider topography, that means, we are not only interested in the pure distance but would like also to avoid the crossing of mountain ranges when going to the coast. To do so, we can weight the distance with elevation so that each additional altitudinal meter “prolongs” the Euclidean distance (this is beyond the scope of the book). Visibility and viewshed computations also belong to the family of global operations (this is also beyond the scope of the book).\n\n\n3.4.7 Map algebra counterparts in vector processing\nMany map algebra operations have a counterpart in vector processing (Liu and Mason 2009 to add citation…). Computing a distance raster (global operation) while only considering a maximum distance (logical focal operation) is the equivalent to a vector buffer operation (Section 4.3.3). Reclassifying raster data (either local or zonal function depending on the input) is equivalent to dissolving vector data (Section Section 4.3.7). Overlaying two rasters (local operation), where one contains “No Data” values representing a mask, is similar to vector clipping (Section Section 4.3.5). Quite similar to spatial clipping is intersecting two layers (Section Section 3.4.1). The difference is that these two layers (vector or raster) simply share an overlapping area. However, be careful with the wording. Sometimes the same words have slightly different meanings for raster and vector data models. While aggregating polygon geometries means dissolving boundaries, for raster data geometries it means increasing cell sizes and thereby reducing spatial resolution. Zonal operations dissolve the cells of one raster in accordance with the zones (categories) of another raster dataset using an aggregating function.\n\n\n3.4.8 Merging rasters\nSuppose we would like to compute the NDVI (see Section 3.4.3), and additionally want to compute terrain attributes from elevation data for observations within a study area. Such computations rely on remotely sensed information. The corresponding imagery is often divided into scenes covering a specific spatial extent (i.e., “tiles”), and frequently, a study area covers more than one scene. Then, we would need to merge (also known as “mosaic”) the scenes covered by our study area. In the easiest case, we can just merge these scenes, that is put them side by side. However, the procedure is the same regardless of the number of scenes. In case when all scenes are “aligned” (i.e., share the same origin and resolution), this can be thought of as simply gluing them into one big raster; otherwise, all scenes are resampled (see Section 4.4.4) to the grid defined by the first scene.\nFor example, let us merge digital elevation data from two SRTM elevation tiles, for Austria (aut.tif) and Switzerland (ch.tif). Merging can be done using function rasterio.merge.merge, which accepts a list of raster file connections, and returns the new ndarray and a “transform”:\n\nsrc_1 = rasterio.open('data/aut.tif')\nsrc_2 = rasterio.open('data/ch.tif')\nout_image, out_transform = rasterio.merge.merge([src_1, src_2])\n\nBoth inputs and the result are shown in Figure 3.16:\n\nfig, axes = plt.subplots(2, 2, figsize=(8,4))\nrasterio.plot.show(src_1, ax=axes[0][0])\nrasterio.plot.show(src_2, ax=axes[0][1])\nrasterio.plot.show(out_image, transform=out_transform, ax=axes[1][0])\naxes[1][1].axis('off')\naxes[0][0].set_title('aut.tif')\naxes[0][1].set_title('ch.tif')\naxes[1][0].set_title('Mosaic (aut.tif+ch.tif)');\n\n\n\n\nFigure 3.16: Raster merging\n\n\n\n\nBy default (method='first'), areas of overlap retain the value of the first raster. Other possible methods are:\n\n'last'—Value of the last raster\n'min'—Minimum value\n'max'—Maximum value\n\nWhen dealing with non-overlapping tiles, such as aut.tif and ch.tif (above), the method argument has no practical effect. However, it becomes relevant when we want to combine spectral imagery from scenes that were taken on different dates. The above four options for method do not cover the commonly required scenario when we would like to compute the mean value—for example to calculate a seasonal average NDVI image from a set of partially overlapping satellite images (such as Landsat). An alternative worflow to rasterio.merge.merge, for calculating a mosaic as well as “averaging” any overlaps, could be to go through two steps:\n\nResampling all scenes into a common “global” grid (Section 4.4.4), thereby producing a series of “matching” rasters (with the area surrounding each scene set as “No Data”)\nAveraging the rasters through raster algebra (Section 3.4.3), as in np.mean(m,axis=0) or np.nanmean(m,axis=0), where m is the multi-band array, which would return a single-band array of averages"
  },
  {
    "objectID": "04-spatial-operations.html#exercises",
    "href": "04-spatial-operations.html#exercises",
    "title": "3  Spatial data operations",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nWrite a function which accepts and array and an int specifying the number of rows/columns to erase along an array edges. The function needs to return the modified array with np.nan values along its edges."
  },
  {
    "objectID": "05-geometry-operations.html#prerequisites",
    "href": "05-geometry-operations.html#prerequisites",
    "title": "4  Geometry operations",
    "section": "4.1 Prerequisites",
    "text": "4.1 Prerequisites\nLet’s import the required packages:\n\nimport numpy as np\nimport shapely\nimport geopandas as gpd\nimport topojson as tp\nimport rasterio\nimport rasterio.warp\nimport rasterio.plot\nimport rasterio.mask\n\nand load the sample data for this chapter:\n\nseine = gpd.read_file('data/seine.gpkg')\nus_states = gpd.read_file('data/us_states.gpkg')\nnz = gpd.read_file('data/nz.gpkg')\nsrc = rasterio.open('data/dem.tif')\nsrc_elev = rasterio.open('data/elev.tif')"
  },
  {
    "objectID": "05-geometry-operations.html#introduction",
    "href": "05-geometry-operations.html#introduction",
    "title": "4  Geometry operations",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nSo far the book has explained the structure of geographic datasets (Chapter 2), and how to manipulate them based on their non-geographic attributes (Chapter 3) and spatial relations (Chapter 4). This chapter focusses on manipulating the geographic elements of geographic objects, for example by simplifying and converting vector geometries, cropping raster datasets, and converting vector objects into rasters and from rasters into vectors. After reading it—and attempting the exercises at the end—you should understand and have control over the geometry column in vector layers and the extent and geographic location of pixels represented in rasters in relation to other geographic objects.\nSection 4.3 covers transforming vector geometries with ‘unary’ and ‘binary’ operations. Unary operations work on a single geometry in isolation, including simplification (of lines and polygons), the creation of buffers and centroids, and shifting/scaling/rotating single geometries using ‘affine transformations’ (Section 4.3.1 to Section 4.3.4). Binary transformations modify one geometry based on the shape of another, including clipping and geometry unions, covered in Section 4.3.5 and Section 4.3.7, respectively. Type transformations (from a polygon to a line, for example) are demonstrated in Section Section 4.3.8.\nSection 4.4 covers geometric transformations on raster objects. This involves changing the size and number of the underlying pixels, and assigning them new values. It teaches how to change the resolution (also called raster aggregation and disaggregation), the extent and the origin of a raster. These operations are especially useful if one would like to align raster datasets from diverse sources. Aligned raster objects share a one-to-one correspondence between pixels, allowing them to be processed using map algebra operations, described in Section 4.3.2. The final Section 6 connects vector and raster objects. It shows how raster values can be ‘masked’ and ‘extracted’ by vector geometries. Importantly it shows how to ‘polygonize’ rasters and ‘rasterize’ vector datasets, making the two data models more interchangeable."
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-vec",
    "href": "05-geometry-operations.html#sec-geo-vec",
    "title": "4  Geometry operations",
    "section": "4.3 Geometric operations on vector data",
    "text": "4.3 Geometric operations on vector data\nThis section is about operations that in some way change the geometry of vector layers. It is more advanced than the spatial data operations presented in the previous chapter (in Section 3.3), because here we drill down into the geometry: the functions discussed in this section work on the geometric (GeoSeries) part, either as standalone object or as part of a GeoDataFrame.\n\n4.3.1 Simplification\nSimplification is a process for generalization of vector objects (lines and polygons) usually for use in smaller scale maps. Another reason for simplifying objects is to reduce the amount of memory, disk space and network bandwidth they consume: it may be wise to simplify complex geometries before publishing them as interactive maps. The geopandas package provides the .simplify method, which uses the GEOS implementation of the Douglas-Peucker algorithm to reduce the vertex count. .simplify uses the tolerance to control the level of generalization in map units (see Douglas and Peucker 1973 for details).\nFor example, a simplified geometry of a \"LineString\" geometry, representing the river Seine and tributaries, using tolerance of 2000 meters, can created using the following command:\n\nseine_simp = seine.simplify(2000) # meters\n\nFigure Figure 4.1 illustrates the input and the result of the simplification:\n\nfig, axes = plt.subplots(ncols=2)\nseine.plot(ax=axes[0])\nseine_simp.plot(ax=axes[1])\naxes[0].set_title('Original')\naxes[1].set_title('Simplified (d=2000 m)');\n\n\n\n\nFigure 4.1: Comparison of the original and simplified geometry of the seine object.\n\n\n\n\nThe resulting seine_simp object is a copy of the original seine but with fewer vertices. This is apparent, with the result being visually simpler (Figure 4.1, right) and consuming less memory than the original object, as verified below:\n\nimport sys\nprint(f'Original: {sys.getsizeof(seine)} bytes')\n\nOriginal: 374 bytes\n\n\n\nprint(f'Simplified: {sys.getsizeof(seine_simp)} bytes')\n\nSimplified: 188 bytes\n\n\nSimplification is also applicable for polygons. This is illustrated using us_states, representing the contiguous United States. As we show in Chapter 6, GEOS assumes that the data is in a projected CRS and this could lead to unexpected results when using a geographic CRS. Therefore, the first step is to project the data into some adequate projected CRS, such as US National Atlas Equal Area (epsg = 2163) (on the left in Figure Figure 4.2):\n\nus_states2163 = us_states.to_crs(2163)\n\nThe .simplify method from geopandas works the same way with a \"Polygon\"/\"MultiPolygon\" layer such as us_states2163:\n\nus_states_simp1 = us_states2163.simplify(100000)\n\nA limitation with .simplify is that it simplifies objects on a per-geometry basis. This means the “topology” is lost, resulting in overlapping and “holey” areal units illustrated in Figure Figure 4.2 (middle panel). The toposimplify function from topojson provides an alternative that overcomes this issue. By default it uses the Douglas-Peucker algorithm like the .simplify method. Another algorithm known as Visvalingam-Whyatt, which overcomes some limitations of the Douglas-Peucker algorithm (Visvalingam and Whyatt 1993), is also available in toposimplify. The main advanatage of toposimplify, however, is that it is topologically “aware”. That is, it simplifies the combined borders of the polygons (rather than each polygon on its own), thus ensuring that the overlap is maintained. The following code chunk uses this function to simplify us_states2163:\n\ntopo = tp.Topology(us_states2163, prequantize=False)\nus_states_simp2 = topo.toposimplify(100000).to_gdf()\n\n/usr/local/lib/python3.11/site-packages/topojson/core/dedup.py:107: RuntimeWarning: invalid value encountered in cast\n  data[\"bookkeeping_shared_arcs\"] = array_bk_sarcs.astype(np.int64).tolist()\n\n\nFigure Figure 4.2 demonstrates the two simplification methods applied to us_states2163.\n\nfig, axes = plt.subplots(ncols=3, figsize=(8,4))\nus_states2163.plot(ax=axes[0], color='lightgrey', edgecolor='black')\nus_states_simp1.plot(ax=axes[1], color='lightgrey', edgecolor='black')\nus_states_simp2.plot(ax=axes[2], color='lightgrey', edgecolor='black')\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Simplified (w/ GeoPandas)\")\naxes[2].set_title(\"Simplified (w/ TopoJSON)\");\n\n\n\n\nFigure 4.2: Polygon simplification in action, comparing the original geometry of the contiguous United States with simplified versions, generated with functions from the GeoPandas (middle), and TopoJSON (right), packages.\n\n\n\n\n\n\n4.3.2 Centroids\nCentroid operations identify the center of geographic objects. Like statistical measures of central tendency (including mean and median definitions of ‘average’), there are many ways to define the geographic center of an object. All of them create single point representations of more complex vector objects.\nThe most commonly used centroid operation is the geographic centroid. This type of centroid operation (often referred to as ‘the centroid’) represents the center of mass in a spatial object (think of balancing a plate on your finger). Geographic centroids have many uses, for example to create a simple point representation of complex geometries, or to estimate distances between polygons. Centroids of the geometries in a GeoSeries or a GeoDataFrame are accessible through the .centroid property, as demonstrated in the code below, which generates the geographic centroids of regions in New Zealand and tributaries to the River Seine, illustrated with black points in Figure 4.3.\n\nnz_centroid = nz.centroid\nseine_centroid = seine.centroid\n\nSometimes the geographic centroid falls outside the boundaries of their parent objects (think of a doughnut). In such cases point on surface operations can be used to guarantee the point will be in the parent object (e.g., for labeling irregular multipolygon objects such as island states), as illustrated by the red points in Figure 4.3. Notice that these red points always lie on their parent objects. They were created with the representative_point method, as follows:\n\nnz_pos = nz.representative_point()\nseine_pos = seine.representative_point()\n\nThe centroids and points in surface are illustrated in Figure 4.3:\n\nfig, axes = plt.subplots(ncols=2)\nnz.plot(ax=axes[0], color='white', edgecolor='lightgrey')\nnz_centroid.plot(ax=axes[0], color='None', edgecolor='black')\nnz_pos.plot(ax=axes[0], color='None', edgecolor='red')\nseine.plot(ax=axes[1], color='grey')\nseine_pos.plot(ax=axes[1], color='None', edgecolor='red')\nseine_centroid.plot(ax=axes[1], color='None', edgecolor='black');\nfig.suptitle(\"Represenative points in red and centroids in black\", y=0.85)\nfig.tight_layout()\n\n\n\n\nFigure 4.3: Centroids (black) and points on surface red of New Zealand and Seine datasets.\n\n\n\n\n\n\n4.3.3 Buffers\nBuffers are polygons representing the area within a given distance of a geometric feature: regardless of whether the input is a point, line or polygon, the output is a polygon. Unlike simplification (which is often used for visualization and reducing file size) buffering tends to be used for geographic data analysis. How many points are within a given distance of this line? Which demographic groups are within travel distance of this new shop? These kinds of questions can be answered and visualized by creating buffers around the geographic entities of interest.\nFigure 4.4 illustrates buffers of different sizes (5 and 50 km) surrounding the river Seine and tributaries. These buffers were created with commands below, which show that the .buffer method, applied to a GeoSeries (or GeoDataFrame) requires one important argument: the buffer distance, provided in the units of the CRS (in this case meters):\n\nseine_buff_5km = seine.buffer(5000)\nseine_buff_50km = seine.buffer(50000)\n\nThe 5 and 50 km buffers are visualized in Figure 4.4:\n\nfig, axes = plt.subplots(ncols=2)\nseine_buff_5km.plot(ax=axes[0], color='none', edgecolor=['red', 'green', 'blue'])\nseine_buff_50km.plot(ax=axes[1], color='none', edgecolor=['red', 'green', 'blue'])\naxes[0].set_title('5 km buffer')\naxes[1].set_title('50 km buffer');\n\n\n\n\nFigure 4.4: Buffers around the Seine dataset of 5 km (left) and 50 km (right). Note the colors, which reflect the fact that one buffer is created per geometry feature.\n\n\n\n\nNote that both .centroid and .buffer return a GeoSeries object, even when the input is a GeoDataFrame:\n\nseine_buff_5km\n\n0    POLYGON ((657550.332 6852587.97...\n1    POLYGON ((517151.801 6930724.10...\n2    POLYGON ((701519.740 6813075.49...\ndtype: geometry\n\n\nIn the common scenario when the original attributes of the input features need to be retained, you can replace the existing geometry with the new GeoSeries as in:\n\nseine_buff_5km = seine.copy()\nseine_buff_5km['geometry'] = seine.buffer(5000)\nseine_buff_5km\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nMarne\nPOLYGON ((657550.332 6852587.97...\n\n\n1\nSeine\nPOLYGON ((517151.801 6930724.10...\n\n\n2\nYonne\nPOLYGON ((701519.740 6813075.49...\n\n\n\n\n\n\n\n\n\n4.3.4 Affine transformations\nAffine transformation is any transformation that preserves lines and parallelism. However, angles or length are not necessarily preserved. Affine transformations include, among others, shifting (translation), scaling and rotation. Additionally, it is possible to use any combination of these. Affine transformations are an essential part of geocomputation. For example, shifting is needed for labels placement, scaling is used in non-contiguous area cartograms, and many affine transformations are applied when reprojecting or improving the geometry that was created based on a distorted or wrongly projected map.\nThe geopandas package implements affine transformation, for objects of classes GeoSeries and GeoDataFrame. In both cases, the method is applied on the GeoSeries part, returning a new GeoSeries of transformed geometries.\nAffine transformations of GeoSeries can be done using the .affine_transform method, which is a wrapper around the shapely.affinity.affine_transform function. According to the documentation, a 2D affine transformation requires a six-parameter list [a,b,d,e,xoff,yoff] which represents the following equations for transforming the coordinates:\n\\[\nx' = a x + b y + x_\\mathrm{off}\n\\]\n\\[\ny' = d x + e y + y_\\mathrm{off}\n\\]\nThere are also simplified GeoSeries methods for specific scenarios:\n\nGeoSeries.translate(xoff=0.0, yoff=0.0, zoff=0.0)\nGeoSeries.scale(xfact=1.0, yfact=1.0, zfact=1.0, origin='center')\nGeoSeries.rotate(angle, origin='center', use_radians=False)\nGeoSeries.skew(angle, origin='center', use_radians=False)\n\nFor example, shifting only requires the \\(x_{off}\\) and \\(y_{off}\\), using .translate. The code below shifts the y-coordinates by 100,000 meters to the north, but leaves the x-coordinates untouched:\n\nnz_shift = nz.translate(0, 100000)\n\nScaling enlarges or shrinks objects by a factor. It can be applied either globally or locally. Global scaling increases or decreases all coordinates values in relation to the origin coordinates, while keeping all geometries topological relations intact.\nGeopandas implements local scaling using the .scale method. Local scaling treats geometries independently and requires points around which geometries are going to be scaled, e.g., centroids. In the example below, each geometry is shrunk by a factor of two around the centroids (middle panel in Figure 4.5). To achieve that, we pass the 0.5 and 0.5 scaling factors (for x and y, respectively), and the 'centroid' option for the point of origin. (Other than 'centroid', it is possible to use 'center' for the bounding box center, or specific point coordinates.)\n\nnz_scale = nz.scale(0.5, 0.5, origin='centroid')\n\nRotating the geometries can be done using the .rotate method. When rotating, we need to specify the rotation angle (positive values imply clockwise rotation) and the origin points (using the same options as in scale). For example, the following expression rotates nz by 30 degrees counter-clockwise, around the geometry centroids:\n\nnz_rotate = nz.rotate(-30, origin='centroid')\n\nFigure 4.5 shows the original layer nz, and the shifting, scaling and rotation results.\n\nfig, axes = plt.subplots(ncols=3, figsize=(8,4))\nnz.plot(ax=axes[0], color='lightgrey', edgecolor='darkgrey')\nnz_shift.plot(ax=axes[0], color='red', edgecolor='darkgrey')\nnz.plot(ax=axes[1], color='lightgrey', edgecolor='darkgrey')\nnz_scale.plot(ax=axes[1], color='red', edgecolor='darkgrey')\nnz.plot(ax=axes[2], color='lightgrey', edgecolor='darkgrey')\nnz_rotate.plot(ax=axes[2], color='red', edgecolor='darkgrey')\naxes[0].set_title('Shift')\naxes[1].set_title('Scale')\naxes[2].set_title('Rotate');\n\n\n\n\nFigure 4.5: Illustrations of affine transformations: shift, scale and rotate.\n\n\n\n\n\n\n4.3.5 Pairwise geometry-generating operations\nSpatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one (Figure 4.6).\n\nx = shapely.Point((0, 0)).buffer(1)\ny = shapely.Point((1, 0)).buffer(1)\nshapely.GeometryCollection([x, y])\n\n\n\n\nFigure 4.6: Overlapping circles.\n\n\n\n\nImagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the .intersection method from shapely, illustrated using objects named x and y which represent the left- and right-hand circles (Figure 4.7).\n\nx.intersection(y)\n\n\n\n\nFigure 4.7: Intersection between two circles\n\n\n\n\nThe next lines of code demonstrate how this works for the .difference (Figure 4.8), .union (Figure 4.9), and .symmetric_difference (Figure 4.10) operators:\n\nx.difference(y)\n\n\n\n\nFigure 4.8: Difference between two circles (1st “minus” 2nd)\n\n\n\n\n\nx.union(y)\n\n\n\n\nFigure 4.9: Union of two circles\n\n\n\n\n\nx.symmetric_difference(y)\n\n\n\n\nFigure 4.10: Symmetric difference between two circles\n\n\n\n\n\n\n4.3.6 Subsetting and clipping\nClipping objects can change their geometry but it can also subset objects, returning only features that intersect (or partly intersect) with a clipping/subsetting object. To illustrate this point, we will subset points that cover the bounding box of the circles x and y in Figure 4.6. Some points will be inside just one circle, some will be inside both and some will be inside neither. The following code sections generates a simple random distribution of points within the extent of circles x and y, resulting in output illustrated in Figure 4.11. We do this in two steps. First, we figure out the bounds where random points are to be generated:\n\nbounds = x.union(y).bounds\nbounds\n\n(-1.0, -1.0, 2.0, 1.0)\n\n\nSecond, we use np.random.uniform to calculate n random x and y coordinates within the given bounds:\n\nnp.random.seed(1)\nn = 10  ## Number of points to generate\ncoords_x = np.random.uniform(bounds[0], bounds[2], n)\ncoords_y = np.random.uniform(bounds[1], bounds[3], n)\ncoords = list(zip(coords_x, coords_y))\ncoords\n\n[(0.2510660141077219, -0.1616109711934104),\n (1.1609734803264744, 0.370439000793519),\n (-0.9996568755479653, -0.5910955005369651),\n (-0.0930022821044807, 0.7562348727818908),\n (-0.5597323275486609, -0.9452248136041477),\n (-0.7229842156936066, 0.34093502035680445),\n (-0.4412193658669873, -0.16539039526574606),\n (0.03668218112914312, 0.11737965689150331),\n (0.1903024226920098, -0.7192261228095325),\n (0.6164502020100708, -0.6037970218302424)]\n\n\nThird, we transform the list of coordinates into a list of shapely points:\n\npnt = [shapely.Point(i) for i in coords]\npnt\n\n[&lt;POINT (0.251 -0.162)&gt;,\n &lt;POINT (1.161 0.37)&gt;,\n &lt;POINT (-1 -0.591)&gt;,\n &lt;POINT (-0.093 0.756)&gt;,\n &lt;POINT (-0.56 -0.945)&gt;,\n &lt;POINT (-0.723 0.341)&gt;,\n &lt;POINT (-0.441 -0.165)&gt;,\n &lt;POINT (0.037 0.117)&gt;,\n &lt;POINT (0.19 -0.719)&gt;,\n &lt;POINT (0.616 -0.604)&gt;]\n\n\nand then to a GeoSeries:\n\npnt = gpd.GeoSeries(pnt)\npnt\n\n0     POINT (0.25107 -0.16161)\n1      POINT (1.16097 0.37044)\n2    POINT (-0.99966 -0.59110)\n               ...            \n7      POINT (0.03668 0.11738)\n8     POINT (0.19030 -0.71923)\n9     POINT (0.61645 -0.60380)\nLength: 10, dtype: geometry\n\n\nThe result is shown in Figure 4.11:\n\nbase = pnt.plot(color='none', edgecolor='black')\ngpd.GeoSeries([x]).plot(ax=base, color='none', edgecolor='darkgrey');\ngpd.GeoSeries([y]).plot(ax=base, color='none', edgecolor='darkgrey');\n\n\n\n\nFigure 4.11: Randomly distributed points within the bounding box enclosing circles x and y. The point that intersects with both objects x and y are highlighted.\n\n\n\n\nNow, we get back to our question: how to subset the points to only return the point that intersects with both x and y? The code chunks below demonstrate three ways to achieve the same result. We can calculate a boolean Series, evaluating whether each point of pnt intersects with the intersection of x and y:\n\nsel = pnt.intersects(x.intersection(y))\nsel\n\n0     True\n1    False\n2    False\n     ...  \n7     True\n8    False\n9     True\nLength: 10, dtype: bool\n\n\nthen use it to subset pnt to get the result pnt1:\n\npnt1 = pnt[sel]\npnt1\n\n0    POINT (0.25107 -0.16161)\n7     POINT (0.03668 0.11738)\n9    POINT (0.61645 -0.60380)\ndtype: geometry\n\n\nWe can also find the intersection between the input points represented by pnt, using the intersection of x and y as the subsetting/clipping object. Since the second argument is an individual shapely geometry (x.intersection(y)), we get “pairwise” intersections of each pnt with it:\n\npnt2 = pnt.intersection(x.intersection(y))\npnt2\n\n0    POINT (0.25107 -0.16161)\n1                 POINT EMPTY\n2                 POINT EMPTY\n               ...           \n7     POINT (0.03668 0.11738)\n8                 POINT EMPTY\n9    POINT (0.61645 -0.60380)\nLength: 10, dtype: geometry\n\n\nThe result is shown in Figure 4.12:\n\nbase = pnt.plot(color='none', edgecolor='black')\ngpd.GeoSeries([x]).plot(ax=base, color='none', edgecolor='darkgrey');\ngpd.GeoSeries([y]).plot(ax=base, color='none', edgecolor='darkgrey');\npnt2.plot(ax=base, color='red');\n\n\n\n\nFigure 4.12: Randomly distributed points within the bounding box enclosing circles x and y. The point that intersects with both objects x and y are highlighted.\n\n\n\n\nEmpty geometries can be filtered out to retain the required subset, and to get pnt2 which is identical to pnt1:\n\npnt2 = pnt2[~pnt2.is_empty]  ## Subset non-empty geometries\npnt2\n\n0    POINT (0.25107 -0.16161)\n7     POINT (0.03668 0.11738)\n9    POINT (0.61645 -0.60380)\ndtype: geometry\n\n\nThis second approach will return features that partly intersect with x.intersection(y) but with modified geometries for spatially extensive features that cross the border of the subsetting object. The results are identical, but the implementation differs substantially.\nAlthough the example above is rather contrived and provided for educational rather than applied purposes, and we encourage the reader to reproduce the results to deepen your understanding for handling geographic vector objects in R, it raises an important question: which implementation to use? Generally, more concise implementations should be favored, meaning the first approach above. We will return to the question of choosing between different implementations of the same technique or algorithm in Chapter 11.\n\n\n4.3.7 Geometry unions\nAs we saw in Section 2.3.2, spatial aggregation can silently dissolve the geometries of touching polygons in the same group. This is demonstrated in the code chunk below in which 49 us_states are aggregated into 4 regions using the .dissolve method:\n\nregions = us_states.dissolve(by='REGION', aggfunc='sum').reset_index()\nregions\n\n/usr/local/lib/python3.11/site-packages/geopandas/geodataframe.py:1676: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  aggregated_data = data.groupby(**groupby_kwargs).agg(aggfunc)\n\n\n\n\n\n\n\n\n\nREGION\ngeometry\nAREA\ntotal_pop_10\ntotal_pop_15\n\n\n\n\n0\nMidwest\nMULTIPOLYGON (((-89.10077 36.94...\n1.984047e+06\n66514091.0\n67546398.0\n\n\n1\nNorteast\nMULTIPOLYGON (((-75.61724 39.83...\n4.357609e+05\n54909218.0\n55989520.0\n\n\n2\nSouth\nMULTIPOLYGON (((-81.38550 30.27...\n2.314087e+06\n112072990.0\n118575377.0\n\n\n3\nWest\nMULTIPOLYGON (((-118.36998 32.8...\n3.073145e+06\n68444193.0\n72264052.0\n\n\n\n\n\n\n\nThe result is shown in Figure 4.13:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 2.5))\nus_states.plot(ax=axes[0], edgecolor='black', column='total_pop_15', legend=True, cmap='PuRd')\nregions.plot(ax=axes[1], edgecolor='black', column='total_pop_15', legend=True, cmap='PuRd')\naxes[0].set_title('State')\naxes[1].set_title('Region');\n\n\n\n\nFigure 4.13: Spatial aggregation on contiguous polygons, illustrated by aggregating the population of US states into regions, with population represented by color. Note the operation automatically dissolves boundaries between states.\n\n\n\n\nWhat is going on in terms of the geometries? Behind the scenes, .dissolve combines the geometries and dissolve the boundaries between them using the .unary_union method per group. This is demonstrated in the code chunk below which creates a united western US using the standalone unary_union operation:\n\nus_west = us_states[us_states['REGION'] == 'West']\nus_west_union = us_west['geometry'].unary_union\n\nNote that the result is a shapely geometry, as the individual attributes are “lost” as part of dissolving. The result is shown in Figure 4.14.\n\nus_west_union\n\n\n\n\nFigure 4.14: Western US\n\n\n\n\nTo dissolve two (or more) groups of a GeoDataFrame into one geometry, we can either use a combined condition:\n\nsel = (us_states['REGION'] == 'West') | (us_states['NAME'] == 'Texas')\ntexas_union = us_states[sel]\ntexas_union = texas_union['geometry'].unary_union\n\nor concatenate the two separate subsets:\n\nus_west = us_states[us_states['REGION'] == 'West']\ntexas = us_states[us_states['NAME'] == 'Texas']\ntexas_union = pd.concat([us_west, texas]).unary_union\n\nand then dissove using .unary_union. The result is identical in both cases, shown in Figure 4.15.\n\ntexas_union\n\n\n\n\nFigure 4.15: Western US and Texas\n\n\n\n\n\n\n4.3.8 Type transformations\nTransformation of geometries, from one type to another, also known as “geometry casting”, is often required to facilitate spatial analysis. The shapely package can be used for geometry casting. The exact expression(s) depend on the specific transformation we are interested in. In general, you need to figure out the required input of the respective construstor function according to the “destination” geometry (e.g., shapely.LineString, etc.), then reshape the input of the “source” geometry into the right form to be passed to that function.\nLet’s create a \"MultiPoint\" to illustrate how geometry casting works on shapely geometry objects:\n\nmultipoint = shapely.MultiPoint([(1,1), (3,3), (5,1)])\nmultipoint\n\n\n\n\nA \"LineString\" can be created using shapely.LineString from a list of points. Consequently, a \"MultiPoint\" can be converted to a \"LineString\" by extracting the individual points into a list, then passing them to shapely.LineString:\n\nlinestring = shapely.LineString(list(multipoint.geoms))\nlinestring\n\n\n\n\nA \"Polygon\" can also be created using function shapely.Polygon, which acceps a sequence of point coordinates. In principle, the last coordinate must be equal to the first, in order to form a closed shape. However, shapely.Polygon is able to complete the last coordinate automatically. Therefore:\n\npolygon = shapely.Polygon([[p.x, p.y] for p in multipoint.geoms])\npolygon\n\n\n\n\nThe source \"MultiPoint\" geometry, and the derived \"LineString\" and \"Polygon\" geometries are shown in Figure 4.16. Note that we convert the shapely geometries to GeoSeries for easier multi-panel plotting:\n\nfig, axes = plt.subplots(ncols=3, figsize=(8,4))\ngpd.GeoSeries(multipoint).plot(ax=axes[0])\ngpd.GeoSeries(linestring).plot(ax=axes[1])\ngpd.GeoSeries(polygon).plot(ax=axes[2])\naxes[0].set_title(\"MultiPoint\")\naxes[1].set_title(\"LineString\")\naxes[2].set_title(\"Polygon\");\n\n\n\n\nFigure 4.16: Examples of linestring and polygon casted from a multipoint geometry.\n\n\n\n\nConversion from \"MultiPoint\" to \"LineString\" is a common operation that creates a line object from ordered point observations, such as GPS measurements or geotagged media. This allows spatial operations such as the length of the path traveled. Conversion from \"MultiPoint\" or \"LineString\" to \"Polygon\" is often used to calculate an area, for example from the set of GPS measurements taken around a lake or from the corners of a building lot.\nOur \"LineString\" geometry can be converted bact to a \"MultiPoint\" geometry by passing its coordinates directly to shapely.MultiPoint:\n\n# 'LineString' -&gt; 'MultiPoint'\nshapely.MultiPoint(linestring.coords)\n\n\n\n\nThe \"Polygon\" (exterior) coordinates can be passed to shapely.MultiPoint as well:\n\n# 'Polygon' -&gt; 'MultiPoint'\nshapely.MultiPoint(polygon.exterior.coords)\n\n\n\n\nSummary / table of possible transformations… (?)\nLet’s apply another commonly used type transformation to demonstrate. As input, we will create a \"MultiLineString\" geometry composed of three lines:\n\nl1 = shapely.LineString([(1, 5), (4, 3)])\nl2 = shapely.LineString([(4, 4), (4, 1)])\nl3 = shapely.LineString([(2, 2), (4, 2)])\nml = shapely.MultiLineString([l1, l2, l3])\nml\n\n\n\n\nLet’s place it into a GeoSeries:\n\ngeom = gpd.GeoSeries([ml])\ngeom\n\n0    MULTILINESTRING ((1.00000 5.000...\ndtype: geometry\n\n\nand finally into a GeoDataFrame with an attribute called \"id\":\n\ndat = gpd.GeoDataFrame(geometry = geom, data = pd.DataFrame({'id': [1]}))\ndat\n\n\n\n\n\n\n\n\nid\ngeometry\n\n\n\n\n0\n1\nMULTILINESTRING ((1.00000 5.000...\n\n\n\n\n\n\n\nYou can imagine it as a road or river network. The above layer dat has only one row that defines all the lines. This restricts the number of operations that can be done, for example it prevents adding names to each line segment or calculating lengths of single lines. Using shapely methods which we are already familiar with (see above), the individual single-part geometries (i.e., the “parts”) can be accessed through the .geoms property:\n\nlist(ml.geoms)\n\n[&lt;LINESTRING (1 5, 4 3)&gt;, &lt;LINESTRING (4 4, 4 1)&gt;, &lt;LINESTRING (2 2, 4 2)&gt;]\n\n\nHowever, spoecifically for the “multi-part to single part” type transformation scenarios, there is also a method called .explode, which can convert an entire multi-part GeoDataFrame to a single-part one. The advantage is that the original attributes (such as id) are retained, so that we can keep track of the original multi-part geometry properties that each part came from. The index_parts=True argument also lets us keep track of the original multipart geometry indices, and part indices, named level_0 and level_1, respectively:\n\ndat1 = dat.explode(index_parts=True).reset_index()\ndat1\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nid\ngeometry\n\n\n\n\n0\n0\n0\n1\nLINESTRING (1.00000 5.00000, 4....\n\n\n1\n0\n1\n1\nLINESTRING (4.00000 4.00000, 4....\n\n\n2\n0\n2\n1\nLINESTRING (2.00000 2.00000, 4....\n\n\n\n\n\n\n\nFor example, here we see that all \"LineString\" geometries came from the same multi-part geometry (level_0=0), which had three parts (level_1=0,1,2).\nFigure 4.17 demonstrates the effect of .explode in converting a layer with multi-part geometries into a layer with single part geometries.\n\nfig, axes = plt.subplots(ncols=2)\ndat.plot(ax=axes[0], column='id')\ndat1.plot(ax=axes[1], column='level_1')\naxes[0].set_title('MultiLineString\\n1 feature')\naxes[1].set_title('LineString\\n3 features');\n\n\n\n\nFigure 4.17: Transformation from a \"MultiLineString\" layer with one feature (left), to a \"LineString\" layer with three features (right) using .explode\n\n\n\n\nBy the way, the opposite transformation, i.e., “single-part to multi-part”, is acheived using the .dissolve method (Section 4.3.7)."
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-ras",
    "href": "05-geometry-operations.html#sec-geo-ras",
    "title": "4  Geometry operations",
    "section": "4.4 Geometric operations on raster data",
    "text": "4.4 Geometric operations on raster data\n\n4.4.1 Geometric intersections\nIn Section 3.4.1 we have shown how to extract values from a raster overlaid by other spatial objects. In case the area of interest is defined by a raster B, to retrieve a spatial output, that is, a (smaller) subset of raster A, we can:\n\nExtract the bounding box polygon of B (hereby, clip)\nMask and crop A (hereby, elev.tif) using B (Section 5.3)\n\nFor example, suppose that we want to get a subset of the elev.tif raster using another, smaller, raster. For demonstration, let’s create (see Section 1.3.3) that smaller raster, hereby named clip. We first create a \\(3 \\times 3\\) array of raster values:\n\nclip = np.array([1] * 9).reshape(3, 3)\nclip\n\narray([[1, 1, 1],\n       [1, 1, 1],\n       [1, 1, 1]])\n\n\nThen, we define the transformation matrix, in such a way that clip intersects with elev.tif (Figure 4.18):\n\nnew_transform = rasterio.transform.from_origin(\n    west=0.9, \n    north=0.45, \n    xsize=0.3, \n    ysize=0.3\n)\nnew_transform\n\nAffine(0.3, 0.0, 0.9,\n       0.0, -0.3, 0.45)\n\n\nNow, for subsetting, we will derive a shapely geometry representing the clip raster extent, using rasterio.transform.array_bounds:\n\nbbox = rasterio.transform.array_bounds(\n    clip.shape[1], # columns\n    clip.shape[0], # rows\n    new_transform\n)\nbbox\n\n(0.9, -0.4499999999999999, 1.7999999999999998, 0.45)\n\n\nThe four numeric values can be transformed into a rectangular shapely geometry using shapely.box:\n\nbbox = shapely.box(*bbox)\nbbox\n\n\n\n\nFigure 4.18 shows the alignment of bbox and elev.tif:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(src_elev, ax=ax)\ngpd.GeoSeries([bbox]).plot(color='none', ax=ax);\n\n\n\n\nFigure 4.18: The elev.tif raster, and the extent of another (smaller) raster clip which we use to subset it\n\n\n\n\nFrom here on, subsetting can be done using masking and cropping, just like with any other vector layer, regardless whether it is rectangular or not. We elaborate on masking and cropping in Section 5.3 (check that section for details about rasterio.mask.mask), but for completeness let’s go through that last step:\n\nout_image, out_transform = rasterio.mask.mask(\n    src_elev, \n    [bbox], \n    crop=True,\n    all_touched=True,\n    nodata=0\n)\n\nThe resulting subset array out_image contains all pixels intersecting with clip pixels (not necessarily with the centroids!). However, due to the all_touched=True argument, those pixels which intersect with clip, but their centroid does not, retain their original values (e.g., 17, 23) rather than turned into “No Data” (e.g., 0):\n\nout_image\n\narray([[[17, 18],\n        [23, 24]]], dtype=uint8)\n\n\nTherefore, in our case, subset out_image dimensions are \\(2 \\times 2\\) (Figure 4.19):\n\nfig, ax = plt.subplots()\nrasterio.plot.show(out_image, transform=out_transform, ax=ax)\ngpd.GeoSeries([bbox]).plot(color='none', ax=ax);\n\n\n\n\nFigure 4.19: The resulting subset of the elev.tif raster\n\n\n\n\n\n\n4.4.2 Extent and origin\nWhen merging or performing map algebra on rasters, their resolution, projection, origin and/or extent have to match. Otherwise, how should we add the values of one raster with a resolution of 0.2 decimal degrees to a second raster with a resolution of 1 decimal degree? The same problem arises when we would like to merge satellite imagery from different sensors with different projections and resolutions. We can deal with such mismatches by aligning the rasters. Typically, raster alignment is done through resampling—that way, it is guaranteed that the rasters match exactly (Section 4.4.4). However, sometimes it can be useful to modify raster placement and extent “manually”, by adding or removing rows and columns, or by modifying the origin, that is, shifting the raster. For example, it may be useful to add extra rows and columns to a raster prior to focal operations, so that it is easier to operate on the edges.\nLet’s demostrate the first operation, raster padding. First, we will read the array with the elev.tif values:\n\nr = src_elev.read(1)\nr\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nTo pad an ndarray, we can use the np.pad function. The function accepts an array, and a tuple of the form ((rows_top,rows_bottom),(columns_left, columns_right)). Also, we can specify the value that’s being used for padding with constant_values (e.g., 18). For example, here we pad r with one extra row and two extra columns, on both sides:\n\nrows = 1\ncols = 2\ns = np.pad(r, ((rows,rows),(cols,cols)), constant_values=18)\ns\n\narray([[18, 18, 18, 18, 18, 18, 18, 18, 18, 18],\n       [18, 18,  1,  2,  3,  4,  5,  6, 18, 18],\n       [18, 18,  7,  8,  9, 10, 11, 12, 18, 18],\n       [18, 18, 13, 14, 15, 16, 17, 18, 18, 18],\n       [18, 18, 19, 20, 21, 22, 23, 24, 18, 18],\n       [18, 18, 25, 26, 27, 28, 29, 30, 18, 18],\n       [18, 18, 31, 32, 33, 34, 35, 36, 18, 18],\n       [18, 18, 18, 18, 18, 18, 18, 18, 18, 18]], dtype=uint8)\n\n\nHowever, for s to be used in spatial operations, we also have to update its transformation matrix. Whenever we add extra columns on the left, or extra rows on top, the raster origin changes. To reflect that fact, we take to “original” origin and add the required multiple of pixel widths or heights (i.e., raster resolution).\nRecall from Section 1.3.3, here is the transformation matrix of elev.tif:\n\nsrc_elev.transform \n\nAffine(0.5, 0.0, -1.5,\n       0.0, -0.5, 1.5)\n\n\nFrom the transformation matrix, we can extract the origin:\n\nxmin, ymax = src_elev.transform[2], src_elev.transform[5]\nxmin, ymax\n\n(-1.5, 1.5)\n\n\nAnd the resolution:\n\ndx, dy = src_elev.transform[0], src_elev.transform[4]\ndx, dy\n\n(0.5, -0.5)\n\n\nNow we can actually update the origin:\n\nxmin_new = xmin - dx * cols\nymax_new = ymax - dy * rows\nxmin_new, ymax_new\n\n(-2.5, 2.0)\n\n\nLet’s create the updated transformation matrix (Section 1.3.3). Keep in mind that the meaning of the last two arguments is xsize, ysize, so we need to pass the absolute value of dy (since it is negative).\n\nnew_transform = rasterio.transform.from_origin(xmin_new, ymax_new, dx, abs(dy))\nnew_transform\n\nAffine(0.5, 0.0, -2.5,\n       0.0, -0.5, 2.0)\n\n\nFigure 4.20 shows the padded raster, with the outline of the original one, demonstrating that the origin was shifted correctly:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(s, transform=new_transform, cmap='Greys', ax=ax)\nelev_bbox = gpd.GeoSeries(shapely.box(*src_elev.bounds))\nelev_bbox.plot(color='none', edgecolor='red', ax=ax);\n\n\n\n\nFigure 4.20: The padded elev.tif raster, and the outline of the extent of the original (in red)\n\n\n\n\nWe can shift a raster origin not just when padding, but in any other use case just by changing its transformation matrix. The effect is that the raster id going to be shifted. This is rarely required in real-life scenarios, but it is useful for understanding the concept of raster origin. For example, let’s shift the origin of elev.tif by (-0.25,0.25):\n\nxmin_new = xmin - 0.25  # shift xmin to the left\nymax_new = ymax + 0.25  # shift ymax upwards\nxmin_new, ymax_new\n\n(-1.75, 1.75)\n\n\nTo shift the origin in other directions change the two operators (-, +) accordingly.\nAgain, let’s create the updated transformation matrix:\n\nnew_transform = rasterio.transform.from_origin(xmin_new, ymax_new, dx, abs(dy))\nnew_transform\n\nAffine(0.5, 0.0, -1.75,\n       0.0, -0.5, 1.75)\n\n\nFigure 4.21 shows the shifted raster and the outline of the original:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(r, transform=new_transform, cmap='Greys', ax=ax)\nelev_bbox.plot(color='none', edgecolor='red', ax=ax);\n\n\n\n\nFigure 4.21: The elev.tif raster shifted by (0.25,0.25) and the extent of the original (in red)\n\n\n\n\n\n\n4.4.3 Aggregation and disaggregation\nRaster datasets vary based on their resolution, from high resolution datasets that enable individual trees to be seen, to low resolution datasets covering the whole Earth. Raster datasets can be transformed to either decrease (aggregate) or increase (disaggregate) their resolution for a number of reasons. Aggregation can reduce computational resource requirements of raster storage and subsequent steps, disaggregation can be used to match other datasets or to add detail. As an example, we here change the spatial resolution of dem.tif by a factor of 5 (Figure Figure 4.22).\nRaster aggregation is, in fact, a special case of raster resampling (see Section 4.4.4), where the target raster grid is aligned with the original raster, only with coarser pixels. Raster resampling, is the general case where the new grid is not necessarily an aggregation of the original one, but any other case as well (such as a rotated and/or shifted one, etc.).\nTo aggregate a raster using rasterio, we go through two steps:\n\nReading the raster values (using .read) into an out_shape that is different from the original .shape\nUpdating the transform according to the out_shape\n\nLet’s demonstrate, using the dem.tif file. Note the shape of the raster, it has 117 rows and 117 columns:\n\nsrc.read(1).shape\n\n(117, 117)\n\n\nAlso note the transform, which tells us that the raster resolution is 30.85 \\(m\\):\n\nsrc.transform\n\nAffine(30.849999999999604, 0.0, 794599.1076146346,\n       0.0, -30.84999999999363, 8935384.324602526)\n\n\nNow, instead of reading the raster values the usual way, as in src.read(1), we can specify out_shape to read the values into a different shape. Here, we calculate a new shape which is downscaled by a factor of 5, i.e., the number of rows and columns is multiplied by 0.2. We must truncate any “partial” rows and columns, e.g., using int. Each new pixel is now obtained, or “resampled”, from \\(\\sim 5 \\times 5 = \\sim 25\\) “old” raster values. We can choose the resampling method through the resampling parameter. Here we use rasterio.enums.Resampling.average, i.e., the new “large” pixel value is the average of all coinciding small pixels, which makes sense for our elevation data in dem.tif:\n\nfactor = 0.2\nr = src.read(1,\n    out_shape=(\n        int(src.height * factor),\n        int(src.width * factor)\n        ),\n    resampling=rasterio.enums.Resampling.average\n)\n\nThe resulting array r has a smaller .shape, as shown below:\n\nr.shape\n\n(23, 23)\n\n\nOther useful options include:\n\nrasterio.enums.Resampling.nearest—Nearest neighbor resampling\nrasterio.enums.Resampling.bilinear—Bilinear resampling\nrasterio.enums.Resampling.cubic—Cubic resampling\nrasterio.enums.Resampling.lanczos—Lanczos windowed resampling\nrasterio.enums.Resampling.mode—Mode resampling (most common value)\nrasterio.enums.Resampling.min—Minimum resampling\nrasterio.enums.Resampling.max—Maximum resampling\nrasterio.enums.Resampling.med—Median resampling\nrasterio.enums.Resampling.sum—Median resampling\n\nSee below (Section 4.4.4) for an explanation of these methods.\nThe second step is to update the transform, taking into account the change in raster shape, as follows:\n\nnew_transform = src.transform * src.transform.scale(\n    (src.width / r.shape[1]),\n    (src.height / r.shape[0])\n)\nnew_transform\n\nAffine(156.93260869565017, 0.0, 794599.1076146346,\n       0.0, -156.9326086956198, 8935384.324602526)\n\n\nThe original raster and the aggregated one, are shown in Figure 4.22:\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nrasterio.plot.show(src, ax=axes[0])\nrasterio.plot.show(r, transform=new_transform, ax=axes[1])\naxes[0].set_title('Original')\naxes[1].set_title('Aggregated (average)');\n\n\n\n\nFigure 4.22: Original raster (left), and aggregated raster (right).\n\n\n\n\nIn case we need to export the new raster, we need to update the metadata:\n\ndst_kwargs = src.meta.copy()\ndst_kwargs.update({\n    'transform': new_transform,\n    'width': r.shape[1],\n    'height': r.shape[0],\n})\ndst_kwargs\n\n{'driver': 'GTiff',\n 'dtype': 'float32',\n 'nodata': nan,\n 'width': 23,\n 'height': 23,\n 'count': 1,\n 'crs': CRS.from_epsg(32717),\n 'transform': Affine(156.93260869565017, 0.0, 794599.1076146346,\n        0.0, -156.9326086956198, 8935384.324602526)}\n\n\nThen create a new file in writing mode, and write the values in r into that file (see Section 7.8.2):\n\ndst = rasterio.open('output/dem_agg5.tif', 'w', **dst_kwargs)\ndst.write(r, 1)\ndst.close()\n\nThe opposite operastion, disaggregation, is when we increase the resolution of raster objects. Either of the supported resampling methods (see above) can be used. However, since we are not actually summarizing information but transferring the value of a large pixel into multiple small pixels, it makes sense to use either:\n\nNearest neighbor resampling (rasterio.enums.Resampling.nearest), when want to keep the original values as-is, since when modifying them would be incorrect (such as in categorical rasters)\nSmooting techniques, such as Bilinear resampling (rasterio.enums.Resampling.bilinear), when we would like the smaller pixels to reflect gradual change between the original values, e.g., when the disaggregated raster is used for visulalization purposes\n\nTo disaggregate a raster, we go through the same workflow as for aggregation, only using a different factor, such as factor=5 instead of factor=0.2, i.e., increasing the number of raster pixels instead of decreasing. In this example, we use bilinear interpolation to get a smoothed high-resolution raster:\n\nfactor = 5\nr2 = src.read(1,\n    out_shape=(\n        int(src.height * factor),\n        int(src.width * factor)\n        ),\n    resampling=rasterio.enums.Resampling.bilinear\n)\n\nHere is the size of the disaggregated raster:\n\nr2.shape\n\n(585, 585)\n\n\nAnd here is the same expression as shown for aggregation, to calculate the new transform:\n\nnew_transform2 = src.transform * src.transform.scale(\n    (src.width / r2.shape[1]),\n    (src.height / r2.shape[0])\n)\nnew_transform2\n\nAffine(6.169999999999921, 0.0, 794599.1076146346,\n       0.0, -6.169999999998726, 8935384.324602526)\n\n\nA zoom-in on the top-left corner of the original raster and the disaggregated one, are shown in Figure 4.23:\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nrasterio.plot.show(src.read(1)[:5, :5], transform=src.transform, ax=axes[0])\nrasterio.plot.show(r2[:25, :25], transform=new_transform2, ax=axes[1])\naxes[0].set_title('Original')\naxes[1].set_title('Disaggregated (bilinear)');\n\n\n\n\nFigure 4.23: Original raster (left), and disaggregated raster (right). The same top-left corner extent of both rasters is shown, to zoom-in and demonstrate the effect of bilinear interpolation.\n\n\n\n\nCode to export the disaggregated raster would be identical to the one used above for the aggregated raster, so omit it to save space.\n\n\n4.4.4 Resampling\nThe above methods of aggregation and disaggregation are only suitable when we want to change the resolution of our raster by the aggregation/disaggregation factor. However, what to do when we have two or more rasters with different resolutions and origins? This is the role of resampling—a process of computing values for new pixel locations. In short, this process takes the values of our original raster and recalculates new values for a target raster with custom resolution and origin (Figure 4.24).\nThere are several methods for estimating values for a raster with different resolutions/origins (Figure 4.24). The main resampling methods include:\n\nNearest neighbor: assigns the value of the nearest cell of the original raster to the cell of the target one. This is a fast simple technique that is usually suitable for resampling categorical rasters.\nBilinear interpolation: assigns a weighted average of the four nearest cells from the original raster to the cell of the target one. This is the fastest method that is appropriate for continuous rasters.\nCubic interpolation: uses values of the 16 nearest cells of the original raster to determine the output cell value, applying third-order polynomial functions. Used for continuous rasters and results in a smoother surface compared to bilinear interpolation, but is computationally more demanding.\nCubic spline interpolation: also uses values of the 16 nearest cells of the original raster to determine the output cell value, but applies cubic splines (piecewise third-order polynomial functions). Used for continuous rasters.\nLanczos windowed sinc resampling: uses values of the 36 nearest cells of the original raster to determine the output cell value. Used for continuous rasters.\nAdditionally, we can use straightforward summary methods, taking into account all pixels that coincide with the target pixel, such as average (Figure 4.22), minimum, maximum (Figure 4.24), median, mode, and sum.\n\nThe above explanation highlights that only nearest neighbor resampling is suitable for categorical rasters, while all remaining methods can be used (with different outcomes) for continuous rasters.\nWith rasterio, resampling can be done using function rasterio.warp.reproject. Note, again, that raster reprojection is not fundamentally different from resampling—the difference is just whether the target grid is in the same CRS as the origin (resampling) or in a different CRS (reprojection). In other words, reprojection is resampling into a grid that is in a different CRS. We will demonstrate reprojection using rasterio.warp.reproject later on (Section 6.9).\nThe information required for rasterio.warp.reproject, whether we are resampling or reprojecting, is:\n\nThe source and target CRS. These may be identical, when resampling, or different, when reprojecting.\nThe source and target transform\n\nAlso, rasterio.warp.reproject works with file connections, so it requires a connection to an output file in write ('w') mode. This makes the function efficient for large rasters.\nThe target and destination CRS are straightforward to specify, depending on our choice. The source transform is also available, e.g., from the source file connection. The only complicated part is to figure out the destination transform. When resampling, the transform is typically derived from a template raster, such as an existing raster file that we would like our origin raster to match, or a numeric specification of our target grid (see below). Otherwise, when the exact grid is not of importance, we can simply aggregate or disaggregate our raster as shown above (Section 4.4.3). (Note that when reprojecting, the target transform is not straightforward to figure out, therefore we use the rasterio.warp.calculate_default_transform function to calculate it, as will be shown in Section 6.9.)\nLet’s demonstrate resampling into a destination grid which is specified through numeric contraints, such as the extent and resolution:\n\nxmin = 794650\nxmax = 798250\nymin = 8931750 \nymax = 8935350\nres = 300\n\nThe transform is a function of the origin and resolution, and can be created using the rasterio.transform.from_origin function as follows:\n\ndst_transform = rasterio.transform.from_origin(west=xmin, north=ymax, xsize=res, ysize=res)\ndst_transform\n\nAffine(300.0, 0.0, 794650.0,\n       0.0, -300.0, 8935350.0)\n\n\nAgain, note that in case we needed to resample into a grid specified by an existing “template” raster, we could skip this step and simply use read the transform from that file, as in rasterio.open('template.tif').transform.\nNow we move on to creating the destination file connection. For that, we also have to know the raster dimensions. These can be derived from the extent and the resolution, as follows:\n\nwidth = int((xmax - xmin) / res)\nheight = int((ymax - ymin) / res)\nwidth, height\n\n(12, 12)\n\n\nNow we create the destination file connection. We are using the same metadata as the source file, except for the dimensions and the transform, which are going to be different and reflecting the resampling process:\n\ndst_kwargs = src.meta.copy()\ndst_kwargs.update({\n    'transform': dst_transform,\n    'width': width,\n    'height': height\n})\ndst = rasterio.open('output/dem_resample_nearest.tif', 'w', **dst_kwargs)\n\nHere is how we reproject, using function rasterio.warp.reproject. The resampling method being used here is nearest neighbor resampling:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src, 1),\n    destination=rasterio.band(dst, 1),\n    src_transform=src.transform,\n    src_crs=src.crs,\n    dst_transform=dst_transform,\n    dst_crs=src.crs,\n    resampling=rasterio.enums.Resampling.nearest\n)\n\n(Band(ds=&lt;open DatasetWriter name='output/dem_resample_nearest.tif' mode='w'&gt;, bidx=1, dtype='float32', shape=(12, 12)),\n Affine(300.0, 0.0, 794650.0,\n        0.0, -300.0, 8935350.0))\n\n\nIn the end, we close the file:\n\ndst.close()\n\nHere is another code section to demontrate another resampling method, the maximum resampling, i.e., every new pixel gets the maximum value of all the original pixels it coincides with. Note that the transform is identical (Figure 4.24), so we do not need to calculate it again:\n\ndst = rasterio.open('output/dem_resample_maximum.tif', 'w', **dst_kwargs)\nrasterio.warp.reproject(\n    source=rasterio.band(src, 1),\n    destination=rasterio.band(dst, 1),\n    src_transform=src.transform,\n    src_crs=src.crs,\n    dst_transform=dst_transform,\n    dst_crs=src.crs,\n    resampling=rasterio.enums.Resampling.max\n)\ndst.close()\n\nThe original raster dem.tif, and the two resampling results dem_resample_nearest.tif and dem_resample_maximum.tif, are shown in Figure 4.24:\n\nfig, axes = plt.subplots(ncols=3, figsize=(8,4))\nrasterio.plot.show(src, ax=axes[0])\nrasterio.plot.show(rasterio.open('output/dem_resample_nearest.tif'), ax=axes[1])\nrasterio.plot.show(rasterio.open('output/dem_resample_maximum.tif'), ax=axes[2])\naxes[0].set_title('Original')\naxes[1].set_title('Nearest neighbor')\naxes[2].set_title('Maximum');\n\n\n\n\nFigure 4.24: Visual comparison of the original raster and two different resampling methods: nearest neighbor and maximum resampling."
  },
  {
    "objectID": "05-geometry-operations.html#exercises",
    "href": "05-geometry-operations.html#exercises",
    "title": "4  Geometry operations",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises"
  },
  {
    "objectID": "06-raster-vector.html#prerequisites",
    "href": "06-raster-vector.html#prerequisites",
    "title": "5  Raster-vector interactions",
    "section": "5.1 Prerequisites",
    "text": "5.1 Prerequisites\nLet’s import the required packages:\n\nimport numpy as np\nimport shapely\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.mask\nimport rasterstats\nimport rasterio.plot\nimport rasterio.features\nimport math\nimport os\n\nand load the sample data:\n\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nsrc_grain = rasterio.open('data/grain.tif')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_dem = rasterio.open('data/dem.tif')\nzion = gpd.read_file('data/zion.gpkg')\nzion_points = gpd.read_file('data/zion_points.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')\nus_states = gpd.read_file('data/us_states.gpkg')\nnz = gpd.read_file('data/nz.gpkg')\nsrc_nz_elev = rasterio.open('data/nz_elev.tif')"
  },
  {
    "objectID": "06-raster-vector.html#introduction",
    "href": "06-raster-vector.html#introduction",
    "title": "5  Raster-vector interactions",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nThis Chapter focuses on interactions between raster and vector geographic data models, introduced in Chapter Chapter 1. It includes four main techniques: raster cropping and masking using vector objects (Section Section 5.3); extracting raster values using different types of vector data (Section Section 5.4); and raster-vector conversion (Section 5.5 and Section 5.6). These concepts are demonstrated using data used in previous chapters to understand their potential real-world applications."
  },
  {
    "objectID": "06-raster-vector.html#sec-raster-cropping",
    "href": "06-raster-vector.html#sec-raster-cropping",
    "title": "5  Raster-vector interactions",
    "section": "5.3 Raster cropping",
    "text": "5.3 Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps, and may be a necessary preprocessing step before creating attractive maps involving raster data.\nWe will use two objects to illustrate raster cropping:\n\nThe srtm.tif raster representing elevation (meters above sea level) in south-western Utah\nThe zion.gpkg vector layer representing the Zion National Park\n\nBoth target and cropping objects must have the same projection. The following reprojects the vector layer zion into the CRS of the raster src_srtm:\n\nzion = zion.to_crs(src_srtm.crs)\n\nTo mask the image, i.e., convert all pixels which do not intersect with the zion polygon to “No Data”, we use the rasterio.mask.mask function as follows:\n\nout_image_mask, out_transform_mask = rasterio.mask.mask(\n    src_srtm, \n    zion['geometry'], \n    crop=False, \n    nodata=9999\n)\n\nNote that we need to specify a “No Data” value in agreement with the raster data type. Since srtm.tif is of type uint16, we choose 9999 (a positive integer that is guaranteed not to occur in the raster).\nThe result is the out_image array with the masked values:\n\nout_image_mask\n\narray([[[9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        ...,\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999]]], dtype=uint16)\n\n\nand the new out_transform:\n\nout_transform_mask\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nNote that masking (without cropping!) does not modify the raster spatial configuration. Therefore, the new transform is identical to the original:\n\nsrc_srtm.transform\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nUnfortunately, the out_image and out_transform object do not contain any information indicating that 9999 represents “No Data”. To associate the information with the raster, we must write it to file along with the corresponding metadata. For example, to write the cropped raster to file, we need to modify the “No Data” setting in the metadata:\n\nout_meta = src_srtm.meta\nout_meta.update(nodata=9999)\nout_meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nThen we can write the cropped raster to file:\n\nnew_dataset = rasterio.open('output/srtm_masked.tif', 'w', **out_meta)\nnew_dataset.write(out_image_mask)\nnew_dataset.close()\n\nNow we can re-import the raster:\n\nsrc_srtm_mask = rasterio.open('output/srtm_masked.tif')\n\nThe .meta property contains the nodata entry. Now, any relevant operation (such as plotting) will take “No Data” into account:\n\nsrc_srtm_mask.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nCropping means reducing the raster extent to the extent of the vector layer:\n\nTo crop and mask, we can use the same in rasterio.mask.mask expression shown above for masking, just setting crop=True instead of crop=False.\nTo just crop, without masking, we can derive the extent polygon and then crop using it.\n\nFor example, here is how we can obtain the extent polygon of zion, as a shapely geometry object:\n\nbb = zion.unary_union.envelope\nbb\n\n\n\n\nThe extent can now be used for masking. Here, we are also using the all_touched=True option so that pixels partially overlapping with the extent are included:\n\nout_image_crop, out_transform_crop = rasterio.mask.mask(\n    src_srtm, \n    [bb], \n    crop=True, \n    all_touched=True, \n    nodata=9999\n)\n\nFigure 5.1 shows the original raster, and the cropped and masked results:\n\n# Original\nfig, ax = plt.subplots(figsize=(3.5, 3.5))\nrasterio.plot.show(src_srtm, ax=ax)\nzion.plot(ax=ax, color='none', edgecolor='black');\n\n# Masked\nfig, ax = plt.subplots(figsize=(3.5, 3.5))\nrasterio.plot.show(src_srtm_mask, ax=ax)\nzion.plot(ax=ax, color='none', edgecolor='black');\n\n# Cropped\nfig, ax = plt.subplots(figsize=(3.5, 3.5))\nrasterio.plot.show(out_image_crop, transform=out_transform_crop, ax=ax)\nzion.plot(ax=ax, color='none', edgecolor='black');\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n(b) Masked\n\n\n\n\n\n\n\n(c) Cropped\n\n\n\n\nFigure 5.1: Raster masking and cropping"
  },
  {
    "objectID": "06-raster-vector.html#sec-raster-extraction",
    "href": "06-raster-vector.html#sec-raster-extraction",
    "title": "5  Raster-vector interactions",
    "section": "5.4 Raster extraction",
    "text": "5.4 Raster extraction\nRaster extraction is the process of identifying and returning the values associated with a ‘target’ raster at specific locations, based on a (typically vector) geographic ‘selector’ object. The reverse of raster extraction — assigning raster cell values based on vector objects — is rasterization, described in Section 5.5.\nIn the following examples, we use a third-party package called rasterstats, which is specifically aimed at extracting raster values:\n\nto points, via the rasterstats.point_query function, or\nto polygons, via the rasterstats.zonal_stats function.\n\n\n5.4.1 Extraction to points\nThe basic example is of extracting the value of a raster cell at specific points. For this purpose, we will use zion_points, which contain a sample of 30 locations within the Zion National Park, which in figure Figure 5.2:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(src_srtm, ax=ax)\nzion_points.plot(ax=ax, color='black');\n\n\n\n\nFigure 5.2: 30 point locations within the Zion National Park, with elevation in the background\n\n\n\n\nThe following expression extracts elevation values from srtm:\n\nresult = rasterstats.point_query(\n    zion_points, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform,\n    interpolate='nearest'\n)\n\nThe resulting object is a list of raster values, corresponding to zion_points:\n\nresult[:5]\n\n[1802, 2433, 1886, 1370, 1452]\n\n\nTo get a DataFrame with points geometries (and attributes, if any) and the related srtm values for each point, we can assign the extraction result into a new column:\n\nzion_points['elev'] = result\nzion_points\n\n\n\n\n\n\n\n\ngeometry\nelev\n\n\n\n\n0\nPOINT (-112.91587 37.20013)\n1802\n\n\n1\nPOINT (-113.09369 37.39263)\n2433\n\n\n2\nPOINT (-113.02462 37.33466)\n1886\n\n\n...\n...\n...\n\n\n27\nPOINT (-113.03655 37.23446)\n1372\n\n\n28\nPOINT (-113.13933 37.39004)\n1905\n\n\n29\nPOINT (-113.09677 37.24237)\n1574\n\n\n\n\n30 rows × 2 columns\n\n\n\n\n\n5.4.2 Extraction to lines\nRaster extraction is also applicable with line selectors. The typical line extraction algorithm is to extract one value for each raster cell touched by a line. However, this particular approach is not recommended to obtain values along the transects, as it is hard to get the correct distance between each pair of extracted raster values.\nFor line extraction, a better approach is to split the line into many points (at equal distances along the line) and then extract the values for these points. To demonstrate this, the code below creates zion_transect, a straight line going from northwest to southeast of the Zion National Park (see Section 1.2 for a recap on the vector data model):\n\ncoords = [[-113.2, 37.45], [-112.9, 37.2]]\nzion_transect = shapely.LineString(coords)\nzion_transect\n\n\n\n\nHere is a printout demonstrating that this is a \"LineString\" geometry representing a straight line between two points:\n\nprint(zion_transect)\n\nLINESTRING (-113.2 37.45, -112.9 37.2)\n\n\nThe line is illustrated in the context of the raster in Figure 5.3.\nThe utility of extracting heights from a linear selector is illustrated by imagining that you are planning a hike. The method demonstrated below provides an ‘elevation profile’ of the route (the line does not need to be straight), useful for estimating how long it will take due to long climbs.\nFirst, we need to create a layer zion_transect_pnt consisting of points along our line (zion_transect), at specified intervals (distance_delta). To do that, we need to transform the line into a projected CRS (so that we work with true distances, in \\(m\\)), such as UTM. This requires going through a GeoSeries, as shapely geometries have no CRS definition nor concept of reprojection (see Section 1.2.6):\n\nzion_transect_utm = gpd.GeoSeries(zion_transect, crs=4326)\nzion_transect_utm = zion_transect_utm.to_crs(32612)\nzion_transect_utm = zion_transect_utm.iloc[0]\n\nThe printout of the new geometry shows this is still a straight line between two points, only with coordinates in a different CRS:\n\nprint(zion_transect_utm)\n\nLINESTRING (305399.67208180577 4147066.650206682, 331380.8917453843 4118750.0947884847)\n\n\nThen, we calculate the distances, along the line, where points are going to be generated, using np.arange. This is a numeric sequence starting at 0, going up to line .length, in steps of 250 (\\(m\\)):\n\ndistances = np.arange(0, zion_transect_utm.length, 250)\ndistances[:7]  ## First 7 distance cutoff points\n\narray([   0.,  250.,  500.,  750., 1000., 1250., 1500.])\n\n\nThe distances cutoffs are used to sample (“interpolate”) points along the line. The shapely method line.interpolate(d) is used to generate the points. The points are then reproject back to the CRS of the raster:\n\nzion_transect_pnt = [zion_transect_utm.interpolate(distance) for distance in distances]\nzion_transect_pnt = gpd.GeoSeries(zion_transect_pnt, crs=32612)\nzion_transect_pnt = zion_transect_pnt.to_crs(4326)\nzion_transect_pnt\n\n0      POINT (-113.20000 37.45000)\n1      POINT (-113.19804 37.44838)\n2      POINT (-113.19608 37.44675)\n                  ...             \n151    POINT (-112.90529 37.20443)\n152    POINT (-112.90334 37.20280)\n153    POINT (-112.90140 37.20117)\nLength: 154, dtype: geometry\n\n\nSecond, we extract elevation values for each point in our transects and combine this information with zion_transect_pnt (after “promoting” it to a GeoDataFrame, to accomodate extra attributes), using the point extraction method shown earlier (Section 5.4.1). We also attach the respective distance cutoff points distances:\n\nresult = rasterstats.point_query(\n    zion_transect_pnt, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform,\n    interpolate='nearest'\n)\nzion_transect_pnt = gpd.GeoDataFrame(geometry=zion_transect_pnt)\nzion_transect_pnt['dist'] = distances\nzion_transect_pnt['elev'] = result\nzion_transect_pnt\n\n\n\n\n\n\n\n\ngeometry\ndist\nelev\n\n\n\n\n0\nPOINT (-113.20000 37.45000)\n0.0\n2001\n\n\n1\nPOINT (-113.19804 37.44838)\n250.0\n2037\n\n\n2\nPOINT (-113.19608 37.44675)\n500.0\n1949\n\n\n...\n...\n...\n...\n\n\n151\nPOINT (-112.90529 37.20443)\n37750.0\n1837\n\n\n152\nPOINT (-112.90334 37.20280)\n38000.0\n1841\n\n\n153\nPOINT (-112.90140 37.20117)\n38250.0\n1819\n\n\n\n\n154 rows × 3 columns\n\n\n\nThe information in zion_transect_pnt, namely the \"dist\" and \"elev\" attributes, can now be used to create elevation profiles, as illustrated in Figure 5.3:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,4))\nrasterio.plot.show(src_srtm, ax=axes[0])\ngpd.GeoSeries(zion_transect).plot(ax=axes[0], color='black')\nzion.plot(ax=axes[0], color='none', edgecolor='darkgrey')\nzion_transect_pnt.set_index('dist')['elev'].plot(ax=axes[1])\naxes[1].set_xlabel('Distance (m)')\naxes[1].set_ylabel('Elevation (m)')\naxes[0].set_title('Line extraction')\naxes[1].set_title('Elevation along the line');\n\n\n\n\nFigure 5.3: Location of a line used for raster extraction (left) and the elevation along this line (right).\n\n\n\n\n\n\n5.4.3 Extraction to polygons\nThe final type of geographic vector object for raster extraction is polygons. Like lines, polygons tend to return many raster values per polygon. Typically, we generate summary statistics for raster values per polygon, for example to characterize a single region or to compare many regions. The generation of raster summary statistics, by polygons, is demonstrated in the code below, which creates a list of summary statistics (in this case a list of length 1, since there is just one polygon), again using rasterstats:\n\nrasterstats.zonal_stats(\n    zion, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform, \n    stats = ['mean', 'min', 'max']\n)\n\n/usr/local/lib/python3.11/site-packages/rasterstats/main.py:156: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n  if 'Point' in geom.type:\n\n\n[{'min': 1122.0, 'max': 2661.0, 'mean': 1818.211830154405}]\n\n\nThe results provide useful summaries, for example that the maximum height in the park is around 2,661 meters above see level (other summary statistics, such as standard deviation, can also be calculated in this way). Because there is only one polygon in the example a data frame with a single row is returned; however, the method works when multiple selector polygons are used.\nNote the stats argument, where we determine what type of statistics are calculated per polygon. Possible values other than 'mean', 'min', 'max' are:\n\n'count'—The number of valid (i.e., excluding “No Data”) pixels\n'nodata'—The number of pixels with ’No Data”\n'majority'—The most frequently occurring value\n'median'—The median value\n\nSee the documentation for the complete list. Additionally, the zonal_stats function accepts user-defined functions for calculating any custom statistics.\nTo count occurrences of categorical raster values within polygons, we can use masking (see Section…) combined with np.unique, as follows:\n\nout_image, out_transform = rasterio.mask.mask(\n    src_nlcd, \n    zion['geometry'].to_crs(src_nlcd.crs), \n    crop=False, \n    nodata=9999\n)\ncounts = np.unique(out_image, return_counts=True)\n\nAccording to the result, for example, pixel value 2 (“Developed” class) appears in 4205 pixels within the Zion polygon:\n\ncounts\n\n(array([ 2,  3,  4,  5,  6,  7,  8, 15], dtype=uint8),\n array([  4205,  98285, 298299, 203701,    235,     62,    679, 852741]))\n\n\nRaster to polygon extraction is illustrated in Figure 5.4.\n\nfig, axes = plt.subplots(ncols=2, figsize=(6,4))\nrasterio.plot.show(src_srtm, ax=axes[0])\nzion.plot(ax=axes[0], color='none', edgecolor='black')\nrasterio.plot.show(src_nlcd, ax=axes[1], cmap='Set3')\nzion.to_crs(src_nlcd.crs).plot(ax=axes[1], color='none', edgecolor='black')\naxes[0].set_title('Continuous data extraction')\naxes[1].set_title('Categorical data extraction');\n\n\n\n\nFigure 5.4: Area used for continuous (left) and categorical (right) raster extraction."
  },
  {
    "objectID": "06-raster-vector.html#sec-rasterization",
    "href": "06-raster-vector.html#sec-rasterization",
    "title": "5  Raster-vector interactions",
    "section": "5.5 Rasterization",
    "text": "5.5 Rasterization\n\n5.5.1 Rasterizing points\nRasterization is the conversion of vector objects into their representation in raster objects. Usually, the output raster is used for quantitative analysis (e.g., analysis of terrain) or modeling. As we saw in Chapter 1, the raster data model has some characteristics that make it conducive to certain methods. Furthermore, the process of rasterization can help simplify datasets because the resulting values all have the same spatial resolution: rasterization can be seen as a special type of geographic data aggregation.\nThe rasterio package contains the rasterio.features.rasterize function for doing this work. To make it happen, we need to have the “template” grid definition, i.e., the “template” raster defining the extent, resolution and CRS of the output, in the form of out_shape (the dimensions) and transform (the transform). In case we have an existing template raster, we simply need to query its out_shape and transform. In case we create a custom template, e.g., covering the vector layer extent with specified resolution, there is some extra work to calculate the out_shape and transform (see next example).\nFurthermore, the rasterio.features.rasterize function requires the input shapes in the form for a generator of (geom, value) tuples, where:\n\ngeom is the given geometry (shapely)\nvalue is the value to be “burned” into pixels coinciding with the geometry (int or float)\n\nAgain, this will be made clear in the next example.\nThe geographic resolution of the “template” raster has a major impact on the results: if it is too low (cell size is too large), the result may miss the full geographic variability of the vector data; if it is too high, computational times may be excessive. There are no simple rules to follow when deciding an appropriate geographic resolution, which is heavily dependent on the intended use of the results. Often the target resolution is imposed on the user, for example when the output of rasterization needs to be aligned to the existing raster.\nTo demonstrate rasterization in action, we will use a template raster that has the same extent and CRS as the input vector data cycle_hire_osm_projected (a dataset on cycle hire points in London is illustrated in Figure 5.5) and spatial resolution of 1000 meters. First, we obtain the vector layer:\n\ncycle_hire_osm_projected = cycle_hire_osm.to_crs(27700)\ncycle_hire_osm_projected\n\n\n\n\n\n\n\n\nosm_id\nname\ncapacity\ncyclestreets_id\ndescription\ngeometry\n\n\n\n\n0\n108539\nWindsor Terrace\n14.0\nNaN\nNaN\nPOINT (532353.838 182857.655)\n\n\n1\n598093293\nPancras Road, King's Cross\nNaN\nNaN\nNaN\nPOINT (529848.350 183337.175)\n\n\n2\n772536185\nClerkenwell, Ampton Street\n11.0\nNaN\nNaN\nPOINT (530635.620 182608.992)\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n537\n5121513755\nNaN\n5.0\nNaN\nNaN\nPOINT (532531.241 178805.391)\n\n\n538\n5188912370\nNaN\nNaN\nNaN\nNaN\nPOINT (538723.338 180836.279)\n\n\n539\n5188912371\nNaN\nNaN\nNaN\nNaN\nPOINT (538413.214 180774.335)\n\n\n\n\n540 rows × 6 columns\n\n\n\nNext, we need to calculate the out_shape and transform of out template raster. To calculate the transform, we combine the top-left corner of the cycle_hire_osm_projected bounding box with the required resolution (e.g., 1000 \\(m\\)):\n\nbounds = cycle_hire_osm_projected.total_bounds\nbounds\n\narray([523038.61452275, 174934.65727249, 538723.33812747, 184971.40854298])\n\n\n\nres = 1000\ntransform = rasterio.transform.from_origin(\n    west=bounds[0], \n    north=bounds[3], \n    xsize=res, \n    ysize=res\n)\ntransform\n\nAffine(1000.0, 0.0, 523038.61452275474,\n       0.0, -1000.0, 184971.40854297992)\n\n\nTo calculate the out_shape, we divide the x-axis and y-axis extent by the resolution:\n\nrows = math.ceil((bounds[3] - bounds[1]) / res)\ncols = math.ceil((bounds[2] - bounds[0]) / res)\nshape = (rows, cols)\nshape\n\n(11, 16)\n\n\nNow, we can rasterize. Rasterization is a very flexible operation: the results depend not only on the nature of the template raster, but also on the type of input vector (e.g., points, polygons), the pixel “activation” method, and the function applied when there is more than one match.\nTo illustrate this flexibility we will try three different approaches to rasterization. First, we create a raster representing the presence or absence of cycle hire points (known as presence/absence rasters). In this case, we transfer the value of 1 to all pixels where at least one point falls in. To transform the point GeoDataFrame into a generator of shapely geometries and the (fixed) values, we use the following expression:\n\n((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list())\n\n&lt;generator object &lt;genexpr&gt; at 0x7f61b156a5a0&gt;\n\n\nTherefore, the rasterizing expression is:\n\nch_raster1 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform\n)\n\nThe result is a numpy array with the burned values of 1, and 0 in unaffected “pixels”:\n\nch_raster1\n\narray([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n\n\nTo count the number of stations, we can use the fixed value of 1 combined with the merge_alg=rasterio.enums.MergeAlg.add, which means that multiple values burned into the same pixel are summed, rather than replaced keeping last (the default):\n\nch_raster2 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\n\nHere is the resulting array of counts:\n\nch_raster2\n\narray([[ 0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  1,  3,  3],\n       [ 0,  0,  0,  1,  3,  3,  5,  5,  8,  9,  1,  3,  2,  6,  7,  0],\n       [ 0,  0,  0,  8,  5,  4, 11, 10, 12,  9, 11,  4,  8,  5,  4,  0],\n       [ 0,  1,  4, 10, 10, 11, 18, 16, 13, 12,  8,  6,  5,  2,  3,  0],\n       [ 3,  3,  9,  3,  5, 14, 10, 15,  9,  9,  5,  8,  0,  0, 12,  2],\n       [ 4,  5,  9, 11,  6,  7,  7,  3, 10,  9,  4,  0,  0,  0,  0,  0],\n       [ 4,  0,  7,  8,  8,  4, 11, 10,  7,  3,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  3,  0,  0,  1,  4,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  1,  0,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n      dtype=uint8)\n\n\nThe new output, ch_raster2, shows the number of cycle hire points in each grid cell. The cycle hire locations have different numbers of bicycles described by the capacity variable, raising the question, what’s the capacity in each grid cell? To calculate that, we must sum the field (\"capacity\") rather than the fixed values of 1. This requires using an expanded generator of geometries and values, where we (1) extract both geometries and attribute values, and (2) filter out “No Data” values, as follows:\n\nch_raster3 = rasterio.features.rasterize(\n    ((g, v) for g, v in cycle_hire_osm_projected[['geometry', 'capacity']] \\\n        .dropna(subset='capacity')\n        .to_numpy() \\\n        .tolist()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\n\nHere is the code to illustrate the input point layer cycle_hire_osm_projected and the three variants of rasterizing it ch_raster1, ch_raster2, and ch_raster3 (Figure 5.5):\n\nfig, axes = plt.subplots(2, 2, figsize=(8.5, 6.5))\ncycle_hire_osm_projected.plot(ax=axes[0][0], column='capacity')\nrasterio.plot.show(ch_raster1, transform=transform, ax=axes[0][1])\nrasterio.plot.show(ch_raster2, transform=transform, ax=axes[1][0])\nrasterio.plot.show(ch_raster3, transform=transform, ax=axes[1][1])\naxes[0][0].set_title('Points')\naxes[0][1].set_title('Presence/Absence')\naxes[1][0].set_title('Count')\naxes[1][1].set_title('Aggregated capacity');\n\n\n\n\nFigure 5.5: Examples of point rasterization\n\n\n\n\n\n\n5.5.2 Rasterizing lines and polygons\nAnother dataset based on California’s polygons and borders (created below) illustrates rasterization of lines. There are three preliminary steps. First, we subset the California polygon:\n\ncalifornia = us_states[us_states['NAME'] == \"California\"]\ncalifornia\n\n\n\n\n\n\n\n\nGEOID\nNAME\nREGION\n...\ntotal_pop_10\ntotal_pop_15\ngeometry\n\n\n\n\n26\n06\nCalifornia\nWest\n...\n36637290.0\n38421464.0\nMULTIPOLYGON (((-118.60337 33.4...\n\n\n\n\n1 rows × 7 columns\n\n\n\nSecond, we “cast” the polygon into a \"MultiLineString\" geometry, using the .boundary property that GeoSeries have:\n\ncalifornia_borders = california['geometry'].boundary\ncalifornia_borders\n\n26    MULTILINESTRING ((-118.60337 33...\ndtype: geometry\n\n\nThird, we create a template raster with a resolution of a 0.5 degree:\n\nbounds = california_borders.total_bounds\nres = 0.5\ntransform = rasterio.transform.from_origin(\n    west=bounds[0], \n    north=bounds[3], \n    xsize=res, \n    ysize=res\n)\nrows = math.ceil((bounds[3] - bounds[1]) / res)\ncols = math.ceil((bounds[2] - bounds[0]) / res)\nshape = (rows, cols)\nshape\n\n(19, 21)\n\n\nFinally, we rasterize california_borders based on the calculated template shape and transform. When considering line or polygon rasterization, one useful additional argument is all_touched. By default it is False, but when changed to True—all cells that are touched by a line or polygon border get a value. Line rasterization with all_touched=True is demonstrated in the code below (Figure 5.6, left). We are also using fill=np.nan to set “background” values as “No Data”:\n\ncalifornia_raster1 = rasterio.features.rasterize(\n    ((g, 1) for g in california_borders.to_list()),\n    out_shape=shape,\n    transform=transform,\n    all_touched=True,\n    fill=np.nan\n)\n\nCompare it to a polygon rasterization, with all_touched=False by default, which selects only raster cells whose centroids are inside the selector polygon, as illustrated in Figure 5.6 (right):\n\ncalifornia_raster2 = rasterio.features.rasterize(\n    ((g, 1) for g in california['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform,\n    fill=np.nan\n)\n\nThe results are shown in Figure 5.6. To illustrate which raster pixels are actually selected as part of rasterization, we also show them as points, which also requires the “Raster to points” code section, as explained in Section 5.6:\n\n# Raster to points\nids = california_raster1.copy()\nids = np.arange(0, california_raster1.size) \\\n    .reshape(california_raster1.shape) \\\n    .astype(np.int32)\nshapes = rasterio.features.shapes(ids, transform=transform)\npol = list(shapes)\npnt = [shapely.geometry.shape(i[0]).centroid for i in pol]\npnt = gpd.GeoSeries(pnt, crs=california.crs)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(8.5, 6.5))\nrasterio.plot.show(california_raster1, transform=transform, ax=axes[0], cmap='Set3')\ngpd.GeoSeries(california_borders, crs=california.crs).plot(ax=axes[0], edgecolor='darkgrey', linewidth=1)\npnt.plot(ax=axes[0], color='black', markersize=1)\nrasterio.plot.show(california_raster2, transform=transform, ax=axes[1], cmap='Set3')\ncalifornia.plot(ax=axes[1], color='none', edgecolor='darkgrey', linewidth=1)\npnt.plot(ax=axes[1], color='black', markersize=1)\naxes[0].set_title('Line rasterization')\naxes[1].set_title('Polygon rasterization');\n\n\n\n\nFigure 5.6: Examples of line and polygon rasterization"
  },
  {
    "objectID": "06-raster-vector.html#sec-spatial-vectorization",
    "href": "06-raster-vector.html#sec-spatial-vectorization",
    "title": "5  Raster-vector interactions",
    "section": "5.6 Spatial vectorization",
    "text": "5.6 Spatial vectorization\nSpatial vectorization is the counterpart of rasterization (Section 5.5), but in the opposite direction. It involves converting spatially continuous raster data into spatially discrete vector data such as points, lines or polygons.\nThere are three standard methods to convert a raster to a vector layer:\n\nRaster to polygons\nRaster to points\nRaster to contours\n\n\n5.6.1 Raster to polygons\nThe most straightforward form of vectorization is the first one, converting raster cells to polygons, where each pixel is represented by a rectangular polygon. The second method, raster to points, has the additional step of calculating polygon centroids. The third method, raster to contours, is somewhat unrelated. Let us demonstrate the three in the given order.\nThe rasterio.features.shapes function can be used to access to the raster pixel as polygon geometries, as well as raster values. The returned object is a generator, which yields geometry,value pairs. The additional transform argument is used to yield true spatial coordinates of the polygons, which is usually what we want.\nFor example, the following expression returns a generator named shapes, referring to the pixel polygons:\n\nshapes = rasterio.features.shapes(\n    src_grain.read(), \n    transform=src_grain.transform\n)\nshapes\n\n&lt;generator object shapes at 0x7f61b366f9a0&gt;\n\n\nWe can generate all shapes at once, into a list named pol, as follows:\n\npol = list(shapes)\n\nEach element in pol is a tuple of length 2, containing:\n\nThe GeoJSON-like dict representing the polygon geometry\nThe value of the pixel(s) which comprise the polygon\n\nFor example:\n\npol[0]\n\n({'type': 'Polygon',\n  'coordinates': [[(-1.5, 1.5),\n    (-1.5, 1.0),\n    (-1.0, 1.0),\n    (-1.0, 1.5),\n    (-1.5, 1.5)]]},\n 1.0)\n\n\nNote that each raster cell is converted into a polygon consisting of five coordinates, all of which are stored in memory (explaining why rasters are often fast compared with vectors!).\nTo transform the list into a GeoDataFrame, we need few more steps of data reshaping:\n\n# Create 'GeoSeries' with the polygons\ngeom = [shapely.geometry.shape(i[0]) for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_grain.crs)\n# Create 'Series' with the values\nvalues = [i[1] for i in pol]\nvalues = pd.Series(values)\n# Combine the 'Series' and 'GeoSeries' into a 'DataFrame'\nresult = gpd.GeoDataFrame({'value': values, 'geometry': geom})\nresult\n\n\n\n\n\n\n\n\nvalue\ngeometry\n\n\n\n\n0\n1.0\nPOLYGON ((-1.50000 1.50000, -1....\n\n\n1\n0.0\nPOLYGON ((-1.00000 1.50000, -1....\n\n\n2\n1.0\nPOLYGON ((-0.50000 1.50000, -0....\n\n\n...\n...\n...\n\n\n11\n2.0\nPOLYGON ((0.00000 -0.50000, 0.5...\n\n\n12\n0.0\nPOLYGON ((0.50000 -1.00000, 0.5...\n\n\n13\n2.0\nPOLYGON ((1.00000 -1.00000, 1.0...\n\n\n\n\n14 rows × 2 columns\n\n\n\nThe resulting polygon layer is shown in Figure 5.7. As shown using the edgecolor='black' option, neighboring pixels sharing the same raster value are dissolved into larger polygons. The rasterio.features.shapes function does not offer a way to avoid this type of dissolving. One way to work around that is to convert an array with consecutive IDs, instead of the real values, to polygons, then extract the real values from the raster (similarly to the “raster to points” example, see below).\n\nresult.plot(column='value', edgecolor='black', legend=True);\n\n\n\n\nFigure 5.7: grain.tif converted to a polygon layer\n\n\n\n\n\n\n5.6.2 Raster to points\nTo transform raster to points, we can use rasterio.features.shapes, as in conversion to polygons, only with the addition of the .centroid method to go from polygons to their centroids. However, to avoid dissolving nearby pixels, we will actually convert a raster with consecutive IDs, then extract the “true” values by point (it is not strictly necessary in this example, since the values of elev.tif are all unique):\n\n# Prepare IDs array\nr = src_elev.read(1)\nids = r.copy()\nids = np.arange(0, r.size).reshape(r.shape).astype(np.int32)\nids\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]], dtype=int32)\n\n\n\n# IDs raster to points\nshapes = rasterio.features.shapes(ids, transform=src_elev.transform)\npol = list(shapes)\ngeom = [shapely.geometry.shape(i[0]).centroid for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_elev.crs)\nresult = gpd.GeoDataFrame(geometry=geom)\n\n\n# Extract values to points\nresult['value'] = rasterstats.point_query(\n    result, \n    r, \n    nodata = src_elev.nodata, \n    affine = src_elev.transform,\n    interpolate='nearest'\n)\n\n/usr/local/lib/python3.11/site-packages/rasterstats/io.py:313: UserWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\"Setting nodata to -999; specify nodata explicitly\")\n\n\nThe result is shown in Figure 5.8.\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nresult.plot(column='value', legend=True, ax=axes[0])\nrasterio.plot.show(src_elev, transform=src_elev.transform, ax=axes[0])\nresult.plot(column='value', legend=True, ax=axes[1])\nrasterio.plot.show(src_elev, cmap='Greys', ax=axes[1]);\n\n\n\n\nFigure 5.8: Raster and point representation of the elev.tif.\n\n\n\n\n\n\n5.6.3 Raster to contours\nAnother common type of spatial vectorization is the creation of contour lines representing lines of continuous height or temperatures (isotherms) for example. We will use a real-world digital elevation model (DEM) because the artificial raster elev produces parallel lines (task for the reader: verify this and explain why this happens). Plotting contour lines is straightforward, using the contour=True option of rasterio.plot.show (Figure 5.9):\n\nfig, ax = plt.subplots(1)\nrasterio.plot.show(src_dem, ax=ax)\nrasterio.plot.show(src_dem, ax=ax, contour=True, levels=np.arange(0,1200,50), colors='black');\n\n\n\n\nFigure 5.9: Displaying raster contours\n\n\n\n\nUnfortunately, rasterio does not provide any way of extracting the contour lines in the form of a vector layer, for uses other than plotting. There are two possible workarounds:\n\nUsing gdal_contour on the command line (see below), or through its Python interface osgeo\nWriting a custom function to export contour coordinates generated by, e.g., matplotlib or skimage\n\nWe hereby demonstrate the first and easiest approach, using gdal_contour. Although we deviate from exclusively using the Python language, the benefit of gdal_contour is the proven algorithm, customized to spatial data, and with many relevant options. gdal_contour (along with other GDAL programs) should already be installed on your system since this is a dependency of rasterio. For example, generating 50 \\(m\\) contours of the dem.tif file can be done as follows:\n\nos.system('gdal_contour -a elev data/dem.tif output/dem_contour.gpkg -i 50.0')\n\nNote that we ran the gdal_contour command through os.system, in order to remain in the Python environment. You can also run the standalone command in the command line interface you are using, such as the Anaconda Prompt:\ngdal_contour -a elev data/dem.tif output/dem_contour.gpkg -i 50.0\nLike all GDAL programs, gdal_contour works with files. Here:\n\nThe input is the data/dem.tif file\nThe result is exported to the output/dem_contour.gpkg file\n\nTo illustrate the result, let’s read the result back into the Python environment. Note that the layer contains an arrtibute named elev (as specified using -a elev) with the contour elevation values:\n\ncontours = gpd.read_file('output/dem_contour.gpkg')\ncontours\n\n\n\n\n\n\n\n\nID\nelev\ngeometry\n\n\n\n\n0\n0\n750.0\nLINESTRING (795382.355 8935384....\n\n\n1\n1\n800.0\nLINESTRING (795237.703 8935384....\n\n\n2\n2\n650.0\nLINESTRING (798098.379 8935384....\n\n\n...\n...\n...\n...\n\n\n29\n29\n450.0\nLINESTRING (795324.083 8931774....\n\n\n30\n30\n450.0\nLINESTRING (795488.616 8931774....\n\n\n31\n31\n450.0\nLINESTRING (795717.420 8931774....\n\n\n\n\n32 rows × 3 columns\n\n\n\nHere is a plot of the contour layer in dem_contour.gpkg (Figure 5.10):\n\nfig, ax = plt.subplots()\nrasterio.plot.show(src_dem, ax=ax)\ncontours.plot(ax=ax, edgecolor='black');\n\n\n\n\nFigure 5.10: Raster contours calculated with the gdal_contour program"
  },
  {
    "objectID": "06-raster-vector.html#sec-distance-to-nearest-geometry",
    "href": "06-raster-vector.html#sec-distance-to-nearest-geometry",
    "title": "5  Raster-vector interactions",
    "section": "5.7 Distance to nearest geometry",
    "text": "5.7 Distance to nearest geometry\nCalculating a raster of distances to the nearest geometry is an example of a “global” raster operation (Section 3.4.6). To demonstrate it, suppose that we need to calculate a raster representing the distance to the nearest coast in New Zealand. This example also wraps many of the concepts introduced in this chapter and in previous chapter, such as raster aggregation, raster conversion to points, and rasterizing points.\nFor the coastline, we will dissolve the New Zealand administrative division polygon layer and “extract” the boundary (which is a \"MultiLineString\" geometry):\n\ncoastline = gpd.GeoSeries(nz.unary_union, crs=nz.crs) \\\n    .to_crs(src_nz_elev.crs) \\\n    .boundary\ncoastline\n\n0    MULTILINESTRING ((1229997.901 4...\ndtype: geometry\n\n\nFor a “template” raster, we will aggregate the New Zealand DEM, in the nz_elev.tif file, to 5 times coarser resolution. The code section below follows the aggeregation example in Section 4.4.3, then replaces the original (aggregated) values with unique IDs, which is a preliminary step when converting to points, as explained in Section 5.6.2. Finally, we also replace “erase” (i.e., replace with np.nan) IDs which were np.nan in the aggregated elevation raster, i.e., beyond the land area of New Zealand:\n\nfactor = 0.2\n# Reading aggregated array\nr = src_nz_elev.read(1,\n    out_shape=(\n        int(src_nz_elev.height * factor),\n        int(src_nz_elev.width * factor)\n        ),\n    resampling=rasterio.enums.Resampling.average\n)\n# Updating the transform\nnew_transform = src_nz_elev.transform * src_nz_elev.transform.scale(\n    (src_nz_elev.width / r.shape[1]),\n    (src_nz_elev.height / r.shape[0])\n)\n# Generating unique IDs per cell\nids = r.copy()\nids = np.arange(0, r.size).reshape(r.shape).astype(np.float32)\n# \"Erasing\" irrelevant IDs\nids[np.isnan(r)] = np.nan\nids\n\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\n\n\nThe result is an array named ids with the IDs, and the corresponding new_transform, as plotted in Figure 5.11:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(ids, transform=new_transform, ax=ax)\ngpd.GeoSeries(coastline).plot(ax=ax, edgecolor='black');\n\n\n\n\nFigure 5.11: Template with cell IDs to calculate distance to nearest geometry\n\n\n\n\nTo calculate distances, we must convert each pixel to a vector (point) geometry. We use the technique demonstrated in Section 5.6.2:\n\nshapes = rasterio.features.shapes(ids, transform=new_transform)\npol = list(shapes)\npnt = [shapely.geometry.shape(i[0]).centroid for i in pol]\n\nThe result pnt is a list of shapely geometries, representing raster cell centroids (excluding np.nan pixels):\n\nprint(pnt[0])\n\nPOINT (1572956.546197626 6189460.927303582)\n\n\nNext we calculate the correspinding list of distances, using the .distance method from shapely:\n\ndistances = [(i, i.distance(coastline)) for i in pnt]\ndistances[0]\n\n(&lt;POINT (1572956.546 6189460.927)&gt;,\n 0    826.752396\n dtype: float64)\n\n\nFinally, we rasterize (see Section 5.5.1) the distances into our raster template:\n\nimage = rasterio.features.rasterize(\n    distances,\n    out_shape=ids.shape,\n    dtype=np.float_,\n    transform=new_transform,\n    fill=np.nan\n)\nimage\n\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])\n\n\nThe resulting raster of distances is shown in Figure 5.12:\n\nfig, ax = plt.subplots()\nrasterio.plot.show(image, transform=new_transform, ax=ax)\ngpd.GeoSeries(coastline).plot(ax=ax, edgecolor='black');\n\n\n\n\nFigure 5.12: Distance to nearest coastline in New Zealand"
  },
  {
    "objectID": "06-raster-vector.html#exercises",
    "href": "06-raster-vector.html#exercises",
    "title": "5  Raster-vector interactions",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises"
  },
  {
    "objectID": "07-reproj.html#prerequisites",
    "href": "07-reproj.html#prerequisites",
    "title": "6  Reprojecting geographic data",
    "section": "6.1 Prerequisites",
    "text": "6.1 Prerequisites\nLet’s import the required packages:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport shapely\nimport geopandas as gpd\nimport rasterio\nimport rasterio.warp\nfrom rasterio.plot import show\nimport pyproj\nimport shutil\n\nand load the sample data:\n\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nzion = gpd.read_file('data/zion.gpkg')\nworld = gpd.read_file('data/world.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')"
  },
  {
    "objectID": "07-reproj.html#introduction",
    "href": "07-reproj.html#introduction",
    "title": "6  Reprojecting geographic data",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nSection 6.3 introduced coordinate reference systems (CRSs), with a focus on the two major types: geographic (‘lon/lat’, with units in degrees longitude and latitude) and projected (typically with units of meters from a datum) coordinate systems. This chapter builds on that knowledge and goes further. It demonstrates how to set and transform geographic data from one CRS to another and, furthermore, highlights specific issues that can arise due to ignoring CRSs that you should be aware of, especially if your data is stored with lon/lat coordinates.\nIn many projects there is no need to worry about, let alone convert between, different CRSs. It is important to know if your data is in a projected or geographic coordinate system, and the consequences of this for geometry operations. However, if you know the CRS of your data and the consequences for geometry operations (covered in the next section), CRSs should just work behind the scenes: people often suddenly need to learn about CRSs when things go wrong. Having a clearly defined project CRS that all project data is in, plus understanding how and why to use different CRSs, can ensure that things don’t go wrong. Furthermore, learning about coordinate systems will deepen your knowledge of geographic datasets and how to use them effectively.\nThis chapter teaches the fundamentals of CRSs, demonstrates the consequences of using different CRSs (including what can go wrong), and how to ‘reproject’ datasets from one coordinate system to another. In the next section we introduce CRSs in Python, followed by Section 6.4 which shows how to get and set CRSs associated with spatial objects. Section Section 6.5 demonstrates the importance of knowing what CRS your data is in with reference to a worked example of creating buffers. We tackle questions of when to reproject and which CRS to use in Section Section 6.6 and Section Section 6.7, respectively. We cover reprojecting vector and raster objects in sections Section 6.8 and Section 6.9 and modifying map projections in Section 6.10."
  },
  {
    "objectID": "07-reproj.html#sec-coordinate-reference-systems",
    "href": "07-reproj.html#sec-coordinate-reference-systems",
    "title": "6  Reprojecting geographic data",
    "section": "6.3 Coordinate Reference Systems",
    "text": "6.3 Coordinate Reference Systems\nMost modern geographic tools that require CRS conversions, including Python packages and desktop GIS software such as QGIS, interface with PROJ, an open source C++ library that “transforms coordinates from one coordinate reference system (CRS) to another”. CRSs can be described in many ways, including the following.\n\nSimple yet potentially ambiguous statements such as “it’s in lon/lat coordinates”.\nFormalized yet now outdated ‘proj4 strings’ such as +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs.\nWith an identifying ‘authority:code’ text string such as EPSG:4326.\n\nEach refers to the same thing: the ‘WGS84’ coordinate system that forms the basis of Global Positioning System (GPS) coordinates and many other datasets. But which one is correct?\nThe short answer is that the third way to identify CRSs is correct: EPSG:4326 is understood by geopandas and rasterio packages covered in this book, plus many other software projects for working with geographic data including QGIS and PROJ. EPSG:4326 is future-proof. Furthermore, although it is machine readable, unlike the proj-string representation “EPSG:4326” is short, easy to remember and highly ‘findable’ online (searching for EPSG:4326 yields a dedicated page on the website epsg.io, for example). The more concise identifier 4326 is also understood by geopandas and rasterio, but we recommend the more explicit AUTHORITY:CODE representation to prevent ambiguity and to provide context.\nThe longer answer is that none of the three descriptions are sufficient and more detail is needed for unambiguous CRS handling and transformations: due to the complexity of CRSs, it is not possible to capture all relevant information about them in such short text strings. For this reason, the Open Geospatial Consortium (OGC, which also developed the simple features specification that the sf package implements) developed an open standard format for describing CRSs that is called WKT (Well Known Text). This is detailed in a 100+ page document that “defines the structure and content of a text string implementation of the abstract model for coordinate reference systems described in ISO 19111:2019” (Open Geospatial Consortium 2019…to add citation!). The WKT representation of the WGS84 CRS, which has the identifier EPSG:4326 is as follows:\n\ncrs = pyproj.CRS.from_string('EPSG:4326') # or '.from_epsg(4326)'\nprint(crs.to_wkt(pretty=True))\n\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nThe output of the command shows how the CRS identifier (also known as a Spatial Reference Identifier or SRID) works: it is simply a look-up, providing a unique identifier associated with a more complete WKT representation of the CRS. This raises the question: what happens if there is a mismatch between the identifier and the longer WKT representation of a CRS? On this point Open Geospatial Consortium (2019… to add citation!) is clear, the verbose WKT representation takes precedence over the identifier:\n\nShould any attributes or values given in the cited identifier be in conflict with attributes or values given explicitly in the WKT description, the WKT values shall prevail.\n\nThe convention of referring to CRSs identifiers in the form AUTHORITY:CODE, which is also used by geographic software written in other languages, allows a wide range of formally defined coordinate systems to be referred to.26 The most commonly used authority in CRS identifiers is EPSG, an acronym for the European Petroleum Survey Group which published a standardized list of CRSs (the EPSG was taken over by the oil and gas body the Geomatics Committee of the International Association of Oil & Gas Producers (…to add citation!) in 2005). Other authorities can be used in CRS identifiers. ESRI:54030, for example, refers to ESRI’s implementation of the Robinson projection, which has the following WKT string:\n\ncrs = pyproj.CRS.from_string('ESRI:54030')\nprint(crs.to_wkt(pretty=True))\n\nPROJCRS[\"World_Robinson\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"World_Robinson\",\n        METHOD[\"Robinson\"],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Not known.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"ESRI\",54030]]\n\n\nWKT strings are exhaustive, detailed, and precise, allowing for unambiguous CRSs storage and transformations. They contain all relevant information about any given CRS, including its datum and ellipsoid, prime meridian, projection, and units.\nRecent PROJ versions (6+) still allow use of proj-strings to define coordinate operations, but some proj-string keys (+nadgrids, +towgs84, +k, +init=epsg:) are either no longer supported or are discouraged. Additionally, only three datums (i.e., WGS84, NAD83, and NAD27) can be directly set in proj-string. Longer explanations of the evolution of CRS definitions and the PROJ library can be found in Bivand (2021), Chapter 2 of Pebesma and Bivand (2022), and a blog post by Floris Vanderhaeghe (…to add citations!). As outlined in the PROJ documentation there are different versions of the WKT CRS format including WKT1 and two variants of WKT2, the latter of which (WKT2, 2018 specification) corresponds to the ISO 19111:2019 (Open Geospatial Consortium 2019…to add citations!)."
  },
  {
    "objectID": "07-reproj.html#sec-querying-and-setting-coordinate-systems",
    "href": "07-reproj.html#sec-querying-and-setting-coordinate-systems",
    "title": "6  Reprojecting geographic data",
    "section": "6.4 Querying and setting coordinate systems",
    "text": "6.4 Querying and setting coordinate systems\nLet’s look at how CRSs are stored in Python spatial objects and how they can be queried and set. First we will look at getting and setting CRSs in vector geographic data objects. Consider the GeoDataFrame object named world, imported from a file world.gpkg. The object world represents countries worldwide. Its CRS can be retrieved using the .crs property:\n\nworld.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe output specifies the following pieces of information:\n\nThe CRS type (Geographic 2D CRS) and EPSG code (EPSG:4326)\nThe CRS name (WGS 84)\nThe axes (latitude, longitude) and their units (degree)\nThe applicable area name (World) and bounding box ((-180.0, -90.0, 180.0, 90.0))\nThe datum (WGS 84)\n\nThe WKT representation, which is internally used when saving the object to a file or doing any coordinate operations, can be extracted using .crs.to_wkt() as shown above (Section 6.3). Above, we can see that the world object has the WGS84 ellipsoid, uses the Greenwich prime meridian, and the latitude and longitude axis order. We also have the suitable suitable area specification for the use of this CRS, and CRS identifier: EPSG:4326.\nThe CRS specification object, such as world.crs, has several other useful properties and methods to retrieve additional information about the used CRS. For example, try to run:\n\nworld.crs.is_geographic to check is the CRS is geographic or not\nworld.crs.axis_info[0].unit_name and world.crs.axis_info[1].unit_name to find out the CRS units of both axes (typically having identical units)\nworld.crs.to_authority() extracts the authority (e.g., EPSG) and the identifier (e.g., 4326)\nworld.crs.to_proj4() returns the proj-string representation\n\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .set_crs method can be used on a GeoSeries or a GeoDataFrame to set it. The CRS can be specified using an EPSG code as the first argument. In case the object already has a different CRS definition, we must also specify allow_override=True to replace it (otherwise we get an error). For example, here we set the EPSG:4326 CRS, which has no effect because world already has that exact CRS definition:\n\nworld2 = world.set_crs(4326)\n\nand here we replace the definition from the existing EPSG:4326, to a new definition EPSG:3857:\n\nworld3 = world.set_crs(3857, allow_override=True)\n\nA number is interpreted as an EPSG code. We can also use strings, as in 'EPSG:4326', which is useful to make the code more clear and when using other authorities:\n\nworld4 = world.set_crs('ESRI:54009', allow_override=True)\n\nIn rasterio, the CRS information is stored as part of a raster file connection metadata (Section 1.3.2). Replacing the CRS definition for a rasterio file connection is typically not necessary, because it is not considered in any operation, only the transformation matrix and coordinates are. One exception is when writing the raster, in which case we need to construct the metadata of the raster file to be written, and therein specify the CRS anyway (Section 1.3.3). However, if we do for some reason need to change the CRS definition in the file connection metadata, we can do that when opening the file in r+ (reading and writing) mode. To demonstrate, we will create a copy of the nlcd.tif file, named nlcd2.tif:\n\nshutil.copy('data/nlcd.tif', 'output/nlcd_modified_crs.tif')\n\n'output/nlcd_modified_crs.tif'\n\n\nNow, let’s examine the existing CRS:\n\nsrc_nlcd2 = rasterio.open('output/nlcd_modified_crs.tif', 'r+')\nsrc_nlcd2.crs\n\nCRS.from_epsg(26912)\n\n\nHere is how we replace the definition with a new one, such as EPSG:3857:\n\nsrc_nlcd2.crs = 3857\nsrc_nlcd2.crs\n\nCRS.from_epsg(3857)\n\n\nExamining the file connection demonstrates that the CRS was indeed changed:\n\nrasterio.open('output/nlcd_modified_crs.tif').crs\n\nCRS.from_epsg(26912)\n\n\nImportantly, the .set_crs (for vector layers) or the assignment to .crs (for rasters), as shown above, do not alter coordinates’ values or geometries. Their role is only to set a metadata information about the object CRS. Consequently, the objects we created, world3, world4, and src_nlcd2 are “incorrect”, in the sense that the geometries are in fact given in a different CRS than specified in the associated CRS definition.\nIn some cases the CRS of a geographic object is unknown, as is the case in the london dataset created in the code chunk below, building on the example of London introduced in Section 1.2.6:\n\nlnd_point = shapely.Point(-0.1, 51.5)\nlnd_geom = gpd.GeoSeries([lnd_point])\nlnd_layer = gpd.GeoDataFrame({'geometry': lnd_geom})\nlnd_layer\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (-0.10000 51.50000)\n\n\n\n\n\n\n\n\nlnd_layer.crs\n\nNothing is printed as a result of the last expression, because the value of the CRS definition in .crs is None. This implies that geopandas does not know what the CRS is and is unwilling to guess. Unless a CRS is manually specified or is loaded from a source that has CRS metadata, geopandas does not make any explicit assumptions about which coordinate systems, other than to say “I don’t know”. This behavior makes sense given the diversity of available CRSs but differs from some approaches, such as the GeoJSON file format specification, which makes the simplifying assumption that all coordinates have a lon/lat CRS: EPSG:4326.\nA CRS can be added to GeoSeries or GeoDataFrame objects using the .set_crs method, as mentioned above. Since the definition is missing, we do not need to specify allow_override=True, as there is nothing to override. For example:\n\nlnd_layer = lnd_layer.set_crs(4326)\nlnd_layer.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIn general, all geographic coordinates have a coordinate system and software can only make good decisions around plotting and and geometry operations if it knows what type of CRS it is working with. When working with geopandas and rasterio, datasets without a specified CRS are not an issue in most workflows, since only the coordinates are considered. It is up to the user to make sure that, when working with more than one layer, all of the coordinates are given in the same CRS (whether specified or not). When exporting the results, though, it is important to keep the CRS definition in place, because other software typically do use, and require, the CRS definition in calculation. It should also be mentioned that, in some cases the CRS specification is left unspecified on purpose, for example when working with layers in arbitrary or non-geographic space (simulations, internal building plans, analysis of plot-scale ecological patterns, etc.)."
  },
  {
    "objectID": "07-reproj.html#sec-geometry-operations-on-projected-and-unprojected-data",
    "href": "07-reproj.html#sec-geometry-operations-on-projected-and-unprojected-data",
    "title": "6  Reprojecting geographic data",
    "section": "6.5 Geometry operations on projected and unprojected data",
    "text": "6.5 Geometry operations on projected and unprojected data\nThe geopandas package, through its dependency shapely, assumes planar geometry and works with distance/area values assumed to be in CRS units. In fact, the CRS definition is typically ignored, and the respective functions (such as in plotting and distance calculations) are applied on the “bare” shapely geometries. Accordingly, it is crucial to make sure that:\n\nGeometric calculations are only applied in projected CRS\nIf there is more than one layer involved—all layers have to be in the same (projected) CRS\nThe input distance and area values are passed in CRS units (and the returned values—interpreted in CRS units)\n\nFor example, to calculate a buffer of 100 \\(km\\) around London, we need to:\n\nWork with a layer representing London in a projected CRS (e.g., EPSG:27700)\nPass the distance value in the CRS units (e.g., 100000 \\(m\\))\n\nIn the following code section we create, from scratch, a point layer lnd_layer_proj with a point representing London (compare to lnd_layer, in a geographical CRS which we created above, see Section 6.4):\n\nlnd_point_proj = shapely.Point(530000, 180000)\nlnd_geom_proj = gpd.GeoSeries([lnd_point_proj], crs=27700)\nlnd_layer_proj = gpd.GeoDataFrame({'geometry': lnd_geom_proj})\nlnd_layer_proj\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (530000.000 180000.000)\n\n\n\n\n\n\n\nNow we can use the .buffer method (Section 4.3.3) to calculate the buffer:\n\nlnd_layer_proj_buff = lnd_layer_proj.buffer(100000)\nlnd_layer_proj_buff\n\n0    POLYGON ((630000.000 180000.000...\ndtype: geometry\n\n\nThe resulting buffer is shown in the left panel of Figure 6.1.\nCalculating a 100-\\(km\\) buffer lnd_layer, which is in a geographical CRS, is impossible. Since the lnd_layer is in decimal degrees, the closest thing to a 100-\\(km\\) buffer would be to use a distance of 1 degree, which is roughly equivalent to 100 \\(km\\) (1 degree is about 111 \\(km\\) at the equator):\n\nlnd_layer_buff = lnd_layer.buffer(1)\nlnd_layer_buff\n\n/tmp/ipykernel_365/855451079.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  lnd_layer_buff = lnd_layer.buffer(1)\n\n\n0    POLYGON ((0.90000 51.50000, 0.8...\ndtype: geometry\n\n\nHowever, this is incorrect, as told by the warning message and shown in the right panel of Figure 6.1. The association between degrees and true distance varies over the surface of the earth and we cannot assume it is fixed (such as 1 degree = 111 \\(km\\)).\n\nuk = world[world['name_long'] == 'United Kingdom']\nuk_proj = uk.to_crs(27700)\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nuk_proj.plot(color='none', edgecolor='darkgrey', ax=axes[0])\nlnd_layer_proj.plot(color='red', ax=axes[0])\nlnd_layer_proj_buff.plot(color='none', ax=axes[0])\nuk.plot(color='none', edgecolor='darkgrey', ax=axes[1])\nlnd_layer.plot(color='red', ax=axes[1])\nlnd_layer_buff.plot(color='none', ax=axes[1])\naxes[0].set_title('100 km buffer\\ncorrect')\naxes[1].set_title('1 degree buffer\\nincorrectly approximating 100 km');\n\n\n\n\nFigure 6.1: Buffers around London, around a projected point and distance of 100 \\(km\\) (left), and around a point in lon/lat using distance of 1 degree (right) which is incorrect.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distance between two lines of longitude, called meridians, is around 111 \\(km\\) at the equator (execute import geopy.distance;geopy.distance.geodesic((0,0),(0,1)) to find the precise distance). This shrinks to zero at the poles. At the latitude of London, for example, meridians are less than 70 \\(km\\) apart (challenge: execute code that verifies this). Lines of latitude, by contrast, are equidistant from each other irrespective of latitude: they are always around 111 \\(km\\) apart, including at the equator and near the poles (see Figure…).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe spherely package, which is in early stages of development, is aimed at providing a spherical-geometry counterpart to shapely, so that true distances (in \\(m\\)) and areas (in \\(m^2\\)) can be calculated on geometries in geographic CRS."
  },
  {
    "objectID": "07-reproj.html#sec-when-to-reproject",
    "href": "07-reproj.html#sec-when-to-reproject",
    "title": "6  Reprojecting geographic data",
    "section": "6.6 When to reproject?",
    "text": "6.6 When to reproject?\nThe previous section showed how to set the CRS manually, with lnd_layer.set_crs(4326). In real world applications, however, CRSs are usually set automatically when data is read-in. In many projects the main CRS-related task is to transform objects, from one CRS into another. But when should data be transformed? And into which CRS? There are no clear-cut answers to these questions and CRS selection always involves trade-offs (Maling 1992, add reference…). However, there are some general principles provided in this section that can help you decide.\nFirst it’s worth considering when to transform. In some cases transformation to a geographic CRS is essential, such as when publishing data online (for example, a Leaflet-based map using Python package folium). Another case is when two objects with different CRSs must be compared or combined, as shown when we try to find the distance between two objects with different CRSs:\n\nlnd_layer.distance(lnd_layer_proj)\n\n/tmp/ipykernel_365/2145313019.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  lnd_layer.distance(lnd_layer_proj)\n/tmp/ipykernel_365/2145313019.py:1: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:4326\nRight CRS: EPSG:27700\n\n  lnd_layer.distance(lnd_layer_proj)\n\n\n0    559715.614087\ndtype: float64\n\n\nWe got a meaningless result and a warning.\nTo make the lnd_layer and lnd_layer_proj objects geographically comparable one of them must be transformed into the CRS of the other. But which CRS to use? The answer depends on context: many projects, especially those involving web mapping, require outputs in EPSG:4326, in which case it is worth transforming the projected object. If, however, the project requires geometric calculations, implying planar geometry, e.g., to calculating buffers (Section 6.5), it is necessary to transform data with a geographic CRS into an equivalent object with a projected CRS, such as the British National Grid (EPSG:27700). That is the subject of Section 6.9."
  },
  {
    "objectID": "07-reproj.html#sec-which-crs-to-use",
    "href": "07-reproj.html#sec-which-crs-to-use",
    "title": "6  Reprojecting geographic data",
    "section": "6.7 Which CRS to use?",
    "text": "6.7 Which CRS to use?\nThe question of which CRS is tricky, and there is rarely a “right” answer: “There exist no all-purpose projections, all involve distortion when far from the center of the specified frame” (Bivand, Pebesma, and Gómez-Rubio 2013, add citation…). Additionally, you should not be attached just to one projection for every task. It is possible to use one projection for some part of the analysis, another projection for a different part, and even some other for visualization. Always try to pick the CRS that serves your goal best!\nWhen selecting geographic CRSs, the answer is often WGS84. It is used not only for web mapping, but also because GPS datasets and thousands of raster and vector datasets are provided in this CRS by default. WGS84 is the most common CRS in the world, so it is worth knowing its EPSG code: 4326. This “magic number” can be used to convert objects with unusual projected CRSs into something that is widely understood.\nWhat about when a projected CRS is required? In some cases, it is not something that we are free to decide: “often the choice of projection is made by a public mapping agency” (Bivand, Pebesma, and Gómez-Rubio 2013, add citation…). This means that when working with local data sources, it is likely preferable to work with the CRS in which the data was provided, to ensure compatibility, even if the official CRS is not the most accurate. The example of London was easy to answer because:\n\nthe British National Grid (with its associated EPSG code 27700) is well known, and\nthe original dataset (lnd_layer) already had that CRS.\n\nA commonly used default is Universal Transverse Mercator (UTM), a set of CRSs that divides the Earth into 60 longitudinal wedges and 20 latitudinal segments. The transverse Mercator projection used by UTM CRSs is conformal but distorts areas and distances with increasing severity with distance from the center of the UTM zone. Documentation from the GIS software Manifold therefore suggests restricting the longitudinal extent of projects using UTM zones to 6 degrees from the central meridian (source: manifold.net). Therefore, we recommend using UTM only when your focus is on preserving angles for relatively small area!\nAlmost every place on Earth has a UTM code, such as “60H” which refers to northern New Zealand. UTM EPSG codes run sequentially from 32601 to 32660 for northern hemisphere locations and from 32701 to 32760 for southern hemisphere locations.\nTo show how the system works, let’s create a function, lonlat2UTM to calculate the EPSG code associated with any point on the planet as follows:\n\nimport math\ndef lonlat2UTM(lon, lat):\n    utm = (math.floor((lon + 180) / 6) % 60) + 1\n    if lat &gt; 0:\n        utm += 32600\n    else:\n        utm += 32700\n    return utm\n\nThe following command uses this function to identify the UTM zone and associated EPSG code for Auckland:\n\nlonlat2UTM(174.7, -36.9)\n\n32760\n\n\nHere is another example for London (where we “unpack” the coordinates of the 1st geometry in lnd_layer into the lonlat2UTM function arguments):\n\nlonlat2UTM(*lnd_layer['geometry'].iloc[0].coords[0])\n\n32630\n\n\nCurrently, we also have tools helping us to select a proper CRS. For example, the webpage https://jjimenezshaw.github.io/crs-explorer/ lists CRSs based on selected location and type. Important note: while these tools are helpful in many situations, you need to be aware of the properties of the recommended CRS before you apply it.\nIn cases where an appropriate CRS is not immediately clear, the choice of CRS should depend on the properties that are most important to preserve in the subsequent maps and analysis. All CRSs are either equal-area, equidistant, conformal (with shapes remaining unchanged), or some combination of compromises of those (Section 1.4.2). Custom CRSs with local parameters can be created for a region of interest and multiple CRSs can be used in projects when no single CRS suits all tasks. “Geodesic calculations” can provide a fall-back if no CRSs are appropriate (see proj.org/geodesic.html). Regardless of the projected CRS used, the results may not be accurate for geometries covering hundreds of kilometers.\nWhen deciding on a custom CRS, we recommend the following:\n\nA Lambert azimuthal equal-area (LAEA) projection for a custom local projection (set latitude and longitude of origin to the center of the study area), which is an equal-area projection at all locations but distorts shapes beyond thousands of kilometers\nAzimuthal equidistant (AEQD) projections for a specifically accurate straight-line distance between a point and the center point of the local projection\nLambert conformal conic (LCC) projections for regions covering thousands of kilometers, with the cone set to keep distance and area properties reasonable between the secant lines\nStereographic (STERE) projections for polar regions, but taking care not to rely on area and distance calculations thousands of kilometers from the center\n\nOne possible approach to automatically select a projected CRS specific to a local dataset is to create an azimuthal equidistant (AEQD) projection for the center-point of the study area. This involves creating a custom CRS (with no EPSG code) with units of meters based on the center point of a dataset. Note that this approach should be used with caution: no other datasets will be compatible with the custom CRS created and results may not be accurate when used on extensive datasets covering hundreds of kilometers.\nThe principles outlined in this section apply equally to vector and raster datasets. Some features of CRS transformation however are unique to each geographic data model. We will cover the particularities of vector data transformation in Section 6.8 and those of raster transformation in Section 6.9. Next, the last section, shows how to create custom map projections (Section 6.10)."
  },
  {
    "objectID": "07-reproj.html#sec-reprojecting-vector-geometries",
    "href": "07-reproj.html#sec-reprojecting-vector-geometries",
    "title": "6  Reprojecting geographic data",
    "section": "6.8 Reprojecting vector geometries",
    "text": "6.8 Reprojecting vector geometries\nChapter 1 demonstrated how vector geometries are made-up of points, and how points form the basis of more complex objects such as lines and polygons. Reprojecting vectors thus consists of transforming the coordinates of these points, which form the vertices of lines and polygons.\nSection 6.5 contains an example in which at least one GeoDataFrame object must be transformed into an equivalent object with a different CRS to calculate the distance between two objects.\n\nlnd_layer2 = lnd_layer.to_crs(27700)\n\nNow that a transformed version of lnd_layer has been created, using the .distance method, the distance between the two representations of London can be found. It may come as a surprise that lnd_layer and lnd_layer2 are just over 2 km apart! The difference in location between the two points is not due to imperfections in the transforming operation (which is in fact very accurate) but the low precision of the manually-created coordinates that created lnd_layer and lnd_layer_proj:\n\nlnd_layer2.distance(lnd_layer_proj)\n\n0    2017.949587\ndtype: float64\n\n\nFunctions for querying and reprojecting CRSs are demonstrated below with reference to cycle_hire_osm, a point layer that represents ‘docking stations’ where you can hire bicycles in London. The CRS of GeoSeries and GeoDataFrame objects can be queried—as we learned in Section 6.4 set—using the .crs property and the .set_crs method, respectively. The output is printed as multiple lines of text containing information about the coordinate system:\n\ncrs_lnd = lnd_layer.crs\ncrs_lnd\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nAs mentioned in Section 6.3, a CRS object has many useful properties to examing the CRS characteristics and details, including .name, .to_proj4() and .to_epsg():\n\ncrs_lnd.name\n\n'WGS 84'\n\n\n\ncrs_lnd.to_proj4()\n\n/usr/local/lib/python3.11/site-packages/pyproj/crs/crs.py:1296: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n  proj = self._crs.to_proj4(version=version)\n\n\n'+proj=longlat +datum=WGS84 +no_defs +type=crs'\n\n\n\ncrs_lnd.to_epsg()\n\n4326\n\n\nAs mentioned in Section 6.3, WKT representation, accessible through .to_wkt() of the crs_lnd object is the ultimate source of truth. This means that the outputs of the previous code chunk are queries from the WKT representation provided by PROJ, rather than inherent attributes of the object and its CRS.\nThe contents of the CRS object associated with a given geometry column is changed when the object’s CRS is transformed. In the code chunk below, we create a new version of cycle_hire_osm with a projected CRS:\n\ncycle_hire_osm_projected = cycle_hire_osm.to_crs(27700)\ncycle_hire_osm_projected.crs\n\n&lt;Derived Projected CRS: EPSG:27700&gt;\nName: OSGB36 / British National Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\n- bounds: (-9.0, 49.75, 2.01, 61.01)\nCoordinate Operation:\n- name: British National Grid\n- method: Transverse Mercator\nDatum: Ordnance Survey of Great Britain 1936\n- Ellipsoid: Airy 1830\n- Prime Meridian: Greenwich\n\n\nThe resulting object has a new CRS with an EPSG code 27700. But how to find out more details about this EPSG code, or any code? One option is to search for it online. Another option is to create a standalone CRS object within the Python environment (using pyproj.CRS.from_string or pyproj.CRS.from_epsg, see Section 6.3), and then query its properties:\n\ncrs_lnd_new = pyproj.CRS.from_epsg(27700)\ncrs_lnd_new.name, crs_lnd_new.to_proj4(), crs_lnd_new.to_wkt()\n\n('OSGB36 / British National Grid',\n '+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs',\n 'PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45\\'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9,61.01,2.01]],ID[\"EPSG\",27700]]')\n\n\nThe result shows that the EPSG code 27700 represents the British National Grid, a result that could have been found by searching online for “EPSG 27700”."
  },
  {
    "objectID": "07-reproj.html#sec-reprojecting-raster-geometries",
    "href": "07-reproj.html#sec-reprojecting-raster-geometries",
    "title": "6  Reprojecting geographic data",
    "section": "6.9 Reprojecting raster geometries",
    "text": "6.9 Reprojecting raster geometries\nThe projection concepts described in the previous section apply equally to rasters. However, there are important differences in reprojection of vectors and rasters: transforming a vector object involves changing the coordinates of every vertex but this does not apply to raster data. Rasters are composed of rectangular cells of the same size (expressed by map units, such as degrees or meters), so it is usually impracticable to transform coordinates of pixels separately. Raster reprojection involves creating a new raster object, often with a different number of columns and rows than the original. The attributes must subsequently be re-estimated, allowing the new pixels to be ‘filled’ with appropriate values. In other words, raster reprojection can be thought of as two separate spatial operations: a vector reprojection of the raster extent to another CRS (Section 6.8), and computation of new pixel values through resampling (Section 4.4.4). Thus in most cases when both raster and vector data are used, it is better to avoid reprojecting rasters and reproject vectors instead.\n\n\n\n\n\n\nNote\n\n\n\nReprojection of the regular rasters is also known as warping. Additionally, there is a second similar operation called “transformation”. Instead of resampling all of the values, it leaves all values intact but recomputes new coordinates for every raster cell, changing the grid geometry. For example, it could convert the input raster (a regular grid) into a curvilinear grid. The rasterio, like common raster file formats (such as GeoTIFF), does not support curvilinear grids (?).\n\n\nThe raster reprojection process is done using two functions from the rasterio.warp sub-package:\n\nrasterio.warp.calculate_default_transform\nrasterio.warp.reproject\n\nThe first function, calculate_default_transform, is used to calculate the new transformation matrix in the destination CRS, according to the source raster dimensions and bounds. Alternatively, the destination transformation matrix can be obtained from an existing raster; this is common practice when we need to align one raster with another, for instance to be able to combine them in raster algebra operations (Section 3.4.3) (see below). The second function rasterio.warp.reproject then actually calculates cell values in the destination grid, using the user-selected resampling method (such as nearest neighbor, or bilinear).\nLet’s take a look at two examples of raster transformation: using categorical and continuous data. Land cover data are usually represented by categorical maps. The nlcd.tif file provides information for a small area in Utah, USA obtained from National Land Cover Database 2011 in the NAD83 / UTM zone 12N CRS, as shown in the output of the code chunk below (only first line of output shown). We already created a connection to the nlcd.tif file, named src_nlcd:\n\nsrc_nlcd\n\n&lt;open DatasetReader name='data/nlcd.tif' mode='r'&gt;\n\n\nRecall that the raster transformation matrix and dimensions are accessible from the file connection as follows. This information will be required to calculate the destination transformation matrix (hereby printed collectively in a tuple):\n\nsrc_nlcd.transform, src_nlcd.width, src_nlcd.height\n\n(Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415),\n 1073,\n 1359)\n\n\nFirst, let’s define the destination CRS. In this case, we choose WGS84 (EPSG code 4326):\n\ndst_crs = 'EPSG:4326'\n\nNow, we are ready to claculate the destination raster transformation matrix (dst_transform), and the destination dimensions (dst_width, dst_height), as follows:\n\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_nlcd.crs,\n    dst_crs,\n    src_nlcd.width,\n    src_nlcd.height,\n    *src_nlcd.bounds\n)\ndst_transform, dst_width, dst_height\n\n(Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022),\n 1244,\n 1246)\n\n\nNote that *, in *src_nlcd.bounds, is used to unpack src_nlcd.bounds to four separate arguments, which calculate_default_transform requires:\n\nsrc_nlcd.bounds\n\nBoundingBox(left=301903.344386758, bottom=4111244.46098842, right=335735.354381954, top=4154086.47216415)\n\n\nNext, we will create the metadata file used for writing the reprojected raster to file. For convenience, we are taking the metadata of the source raster (src_nlcd.meta), making a copy (dst_kwargs), and then updating those specific properties that need to be changed. Note that the reprojection process typically creates “No Data” pixels, even when there were none in the input raster, since the raster orientation changes and the edges need to be “filled” to get back a rectangular extent. We need to specify a “No Data” value of our choice, if there is none, or use the existing source raster setting, such as 255 in this case:\n\ndst_kwargs = src_nlcd.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_kwargs\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': 'EPSG:4326',\n 'transform': Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022)}\n\n\nWe are ready to create the reprojected raster. Here, reprojection takes place between two file connections, meaning that the raster value arrays are not being read into memory at once. It is also possible to reproject into an in-memory ndarray object, see the documentation.\nTo write the reprojected raster, we first create a destination file connection dst_nlcd, pointing at the output file path of our choice (output/nlcd_4326.tif), using the updated metadata object created earlier (dst_kwargs):\nThen, we use the rasterio.warp.reproject function to calculate and write the reprojection result into the dst_nlcd file connection. Note that the source and destination accept a “band” object, created using rasterio.band. In this case, there is just one band. If there were more bands, we would have to repeat the procedure for each band, using i instead of 1 inside a loop:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.enums.Resampling.nearest\n)\n\n(Band(ds=&lt;open DatasetWriter name='output/nlcd_4326.tif' mode='w'&gt;, bidx=1, dtype='uint8', shape=(1246, 1244)),\n Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022))\n\n\nFinally, we close the file connection so that the data are actually written:\n\ndst_nlcd.close()\n\nMany properties of the new object differ from the previous one, including the number of columns and rows (and therefore number of cells), resolution (transformed from meters into degrees), and extent, as summarized again below (note that the number of categories increases from 8 to 9 because of the addition of NA values, not because a new category has been created—the land cover classes are preserved).\n\nsrc_nlcd.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1073,\n 'height': 1359,\n 'count': 1,\n 'crs': CRS.from_epsg(26912),\n 'transform': Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415)}\n\n\n\nsrc_nlcd_4326 = rasterio.open('output/nlcd_4326.tif')\nsrc_nlcd_4326.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022)}\n\n\nExamining the unique raster values tells us that the new raster has the same categories, plus the value 255 representing “No Data”:\n\nnp.unique(src_nlcd.read(1))\n\narray([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint8)\n\n\n\nnp.unique(src_nlcd_4326.read(1))\n\narray([  1,   2,   3,   4,   5,   6,   7,   8, 255], dtype=uint8)\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nshow(src_nlcd, ax=axes[0], cmap='Set3')\nshow(src_nlcd_4326, ax=axes[1], cmap='Set3')\naxes[0].set_title('Original (EPSG:26912)')\naxes[1].set_title('Reprojected (EPSG:4326)');\n\n\n\n\nFigure 6.2: Reprojecting a categorical raster using nearest neighbor resampling\n\n\n\n\nIn the above example, we automatically calculated an optimal (i.e., most information preserving) destination grid using rasterio.warp.calculate_default_transform. This is appropriate when there are no specific requirements for the destination raster spatial properties. Namely, we are not required to otain a specific origin and resolution, but just wish to preserve the raster values as much as possible. To do that, calculate_default_transform “tries” to keep the extent and resolution of the destination raster as similar as possible to the source. In other situations, however, we need to reproject a raster into a specific “template”, so that it corresponds, for instance, with other rasters we use in the analysis. In the following code section, we reproject the nlcd.tif raster, again, buit this time using the nlcd_4326.tif reprojection result as the “template” to demonstrate this alternative workflow.\nFirst, we create a connection to our “template” raster to read its metadata:\n\ntemplate = rasterio.open('output/nlcd_4326.tif')\ntemplate.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1244,\n 'height': 1246,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022)}\n\n\nThen, we create a write-mode connection to our destination raster, using this metadata, meaning that as the resampling result is going to have identical metadata as the “template”:\nNow, we can resample and write the result:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd_2, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_nlcd_2.transform,\n    dst_crs=dst_nlcd_2.crs,\n    resampling=rasterio.enums.Resampling.nearest\n)\n\n(Band(ds=&lt;open DatasetWriter name='output/nlcd_4326_2.tif' mode='w'&gt;, bidx=1, dtype='uint8', shape=(1246, 1244)),\n Affine(0.00031506316853514724, 0.0, -113.24138811813536,\n        0.0, -0.00031506316853514724, 37.51912722777022))\n\n\n\ndst_nlcd_2.close()\n\nNaturally, in this case, the outputs nlcd_4326.tif and nlcd_4326_2.tif are identical, as we used the same “template” and the same source data:\n\nd = rasterio.open('output/nlcd_4326.tif').read(1) == rasterio.open('output/nlcd_4326_2.tif').read(1)\nd\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])\n\n\n\nnp.all(d)\n\nTrue\n\n\nThe difference is that in the first example we calculate the template automatically, using rasterio.warp.calculate_default_transform, while in the second example we used an existing raster as the “template”.\nImportantly, when the template raster has much more “coarse” resolution than the source raster, the:\n\nrasterio.enums.Resampling.average (for continuous rasters), or\nrasterio.enums.Resampling.mode (for categorical rasters)\n\nresampling method should be used, instead of rasterio.enums.Resampling.nearest. Otherwise, much of the data will be lost, as the “nearest” method can capture one pixel value only for each destination raster pixel.\nReprojecting continuous rasters (with numeric or, in this case, integer values) follows an almost identical procedure. This is demonstrated below with srtm.tif from the Shuttle Radar Topography Mission (SRTM), which represents height in meters above sea level (elevation) with the WGS84 CRS.\nWe will reproject this dataset into a projected CRS, but not with the nearest neighbor method which is appropriate for categorical data. Instead, we will use the bilinear method which computes the output cell value based on the four nearest cells in the original raster. The values in the projected dataset are the distance-weighted average of the values from these four cells: the closer the input cell is to the center of the output cell, the greater its weight. The following code section create a text string representing WGS 84 / UTM zone 12N, and reproject the raster into this CRS, using the bilinear method. The code is practically the same, except for changing the source and destination file names, and replacing nearest with bilinear:\n\ndst_crs = 'EPSG:32612'\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_srtm.crs,\n    dst_crs,\n    src_srtm.width,\n    src_srtm.height,\n    *src_srtm.bounds\n)\ndst_kwargs = src_srtm.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_srtm = rasterio.open('output/srtm_32612.tif', 'w', **dst_kwargs)\nrasterio.warp.reproject(\n    source=rasterio.band(src_srtm, 1),\n    destination=rasterio.band(dst_srtm, 1),\n    src_transform=src_srtm.transform,\n    src_crs=src_srtm.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.enums.Resampling.bilinear\n)\ndst_srtm.close()\n\nFigure 6.3 shows the input and the reprojected SRTM rasters.\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nshow(src_srtm, ax=axes[0])\nshow(rasterio.open('output/srtm_32612.tif'), ax=axes[1])\naxes[0].set_title('Original (EPSG:4326)')\naxes[1].set_title('Reprojected (EPSG:32612)');\n\n\n\n\nFigure 6.3: Reprojecting a continuous raster using bilinear resampling"
  },
  {
    "objectID": "07-reproj.html#sec-custom-map-projections",
    "href": "07-reproj.html#sec-custom-map-projections",
    "title": "6  Reprojecting geographic data",
    "section": "6.10 Custom map projections",
    "text": "6.10 Custom map projections\nEstablished CRSs captured by AUTHORITY:CODE identifiers such as EPSG:4326 are well suited for many applications. However, it is desirable to use alternative projections or to create custom CRSs in some cases. Section 6.7 mentioned reasons for using custom CRSs, and provided several possible approaches. Here, we show how to apply these ideas in Python.\nOne is to take an existing WKT definition of a CRS, modify some of its elements, and then use the new definition for reprojecting, using the reprojection methods shown above for vector layers (Section 6.8) and rasters (Section 6.9).\nFor example, let’s transforms the zion.gpkg vector layer to a custom azimuthal equidistant (AEQD) CRS. Using a custom AEQD CRS requires knowing the coordinates of the center point of a dataset in degrees (geographic CRS). In our case, this information can be extracted by calculating a centroid of the zion area and transforming it into WGS84:\n\nzion_centr = zion.centroid\nzion_centr_wgs84 = zion_centr.to_crs(4326)\ncoords = list(zion_centr_wgs84.iloc[0].coords)\ncoords\n\n[(-113.02630562788148, 37.298176835277424)]\n\n\nNext, we can use the newly obtained lon/lat coordinates to update the WKT definition of the azimuthal equidistant (AEQD) CRS seen below. Notice that we modified just two values below—\"Central_Meridian\" to the longitude and \"Latitude_Of_Origin\" to the latitude of our centroid:\n\nmy_wkt = '''PROJCS[\"Custom_AEQD\",\n GEOGCS[\"GCS_WGS_1984\",\n  DATUM[\"WGS_1984\",\n   SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],\n  PRIMEM[\"Greenwich\",0.0],\n  UNIT[\"Degree\",0.0174532925199433]],\n PROJECTION[\"Azimuthal_Equidistant\"],\n PARAMETER[\"Central_Meridian\",-113.0263],\n PARAMETER[\"Latitude_Of_Origin\",37.29818],\n UNIT[\"Meter\",1.0]]'''\n\nThis approach’s last step is to transform our original object (zion) to our new custom CRS (zion_aeqd):\n\nzion_aeqd = zion.to_crs(my_wkt)\n\nCustom projections can also be made interactively, for example, using the Projection Wizard web application (Šavrič, Jenny, and Jenny 2016 to add citation…). This website allows you to select a spatial extent of your data and a distortion property, and returns a list of possible projections. The list also contains WKT definitions of the projections that you can copy and use for reprojections. See Open Geospatial Consortium (2019) for details on creating custom CRS definitions with WKT strings.\nPROJ strings can also be used to create custom projections, accepting the limitations inherent to projections, especially of geometries covering large geographic areas, mentioned in Section 6.3. Many projections have been developed and can be set with the +proj= element of PROJ strings, with dozens of projects described in detail on the PROJ website alone.\nWhen mapping the world while preserving area relationships the Mollweide projection, illustrated in Figure 6.4, is a popular and often sensible choice (Jenny et al. 2017 to add citation…). To use this projection, we need to specify it using the proj-string element, '+proj=moll', in the .to_crs method:\n\nworld.to_crs('+proj=moll').plot(color='none', edgecolor='black');\n\n\n\n\nFigure 6.4: Mollweide projection of the world\n\n\n\n\nIt is often desirable to minimize distortion for all spatial properties (area, direction, distance) when mapping the world. One of the most popular projections to achieve this is Winkel tripel, illustrated in Figure 6.5:\n\nworld.to_crs('+proj=wintri').plot(color='none', edgecolor='black');\n\n\n\n\nFigure 6.5: Winkel tripel projection of the world\n\n\n\n\nMoreover, proj-string parameters can be modified in most CRS definitions, for example the center of the projection can be adjusted using the +lon_0 and +lat_0 parameters. The below code transforms the coordinates to the Lambert azimuthal equal-area projection centered on the longitude and latitude of New York City (Figure 6.6).\n\nworld.to_crs('+proj=laea +x_0=0 +y_0=0 +lon_0=-74 +lat_0=40') \\\n    .plot(color='none', edgecolor='black');\n\n\n\n\nFigure 6.6: Lambert azimuthal equal-area projection of the world centered on New York City\n\n\n\n\nMore information on CRS modifications can be found in the Using PROJ documentation."
  },
  {
    "objectID": "07-reproj.html#exercises",
    "href": "07-reproj.html#exercises",
    "title": "6  Reprojecting geographic data",
    "section": "6.11 Exercises",
    "text": "6.11 Exercises"
  },
  {
    "objectID": "08-read-write-plot.html#prerequisites",
    "href": "08-read-write-plot.html#prerequisites",
    "title": "7  Geographic data I/O",
    "section": "7.1 Prerequisites",
    "text": "7.1 Prerequisites\nLet’s import the required packages:\n\nimport urllib.request\nimport zipfile\nimport numpy as np\nimport fiona\nimport geopandas as gpd\nimport shapely\nimport rasterio\nimport rasterio.plot\nimport cartopy\nimport osmnx as ox\n\nand load the sample data for this chapter:\n\nnz = gpd.read_file('data/nz.gpkg')\nnz_elev = rasterio.open('data/nz_elev.tif')"
  },
  {
    "objectID": "08-read-write-plot.html#introduction",
    "href": "08-read-write-plot.html#introduction",
    "title": "7  Geographic data I/O",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nThis chapter is about reading and writing geographic data. Geographic data import is essential for geocomputation: real-world applications are impossible without data. Data output is also vital, enabling others to use valuable new or improved datasets resulting from your work. Taken together, these processes of import/output can be referred to as data I/O.\nGeographic data I/O is often done with few lines of code at the beginning and end of projects. It is often overlooked as a simple one step process. However, mistakes made at the outset of projects (e.g. using an out-of-date or in some way faulty dataset) can lead to large problems later down the line, so it is worth putting considerable time into identifying which datasets are available, where they can be found and how to retrieve them. These topics are covered in Section 7.3, which describes various geoportals, which collectively contain many terabytes of data, and how to use them. To further ease data access, a number of packages for downloading geographic data have been developed, as described in Section 7.4.\nThere are many geographic file formats, each of which has pros and cons, described in Section 7.6. The process of reading and writing files in formats efficiently is covered in Sections Section 7.7 and Section 7.8, respectively."
  },
  {
    "objectID": "08-read-write-plot.html#sec-retrieving-open-data",
    "href": "08-read-write-plot.html#sec-retrieving-open-data",
    "title": "7  Geographic data I/O",
    "section": "7.3 Retrieving open data",
    "text": "7.3 Retrieving open data\nA vast and ever-increasing amount of geographic data is available on the internet, much of which is free to access and use (with appropriate credit given to its providers).1 In some ways there is now too much data, in the sense that there are often multiple places to access the same dataset. Some datasets are of poor quality. In this context, it is vital to know where to look, so the first section covers some of the most important sources. Various ‘geoportals’ (web services providing geospatial datasets such as Data.gov) are a good place to start, providing a wide range of data but often only for specific locations (as illustrated in the updated Wikipedia page on the topic).\nSome global geoportals overcome this issue. The GEOSS portal and the Copernicus Open Access Hub, for example, contain many raster datasets with global coverage. A wealth of vector datasets can be accessed from the SEDAC portal run by the National Aeronautics and Space Administration (NASA) and the European Union’s INSPIRE geoportal, with global and regional coverage.\nMost geoportals provide a graphical interface allowing datasets to be queried based on characteristics such as spatial and temporal extent, the United States Geological Survey’s EarthExplorer being a prime example. Exploring datasets interactively on a browser is an effective way of understanding available layers. Downloading data is best done with code, however, from reproducibility and efficiency perspectives. Downloads can be initiated from the command line using a variety of techniques, primarily via URLs and APIs (see the Sentinel API for example). Files hosted on static URLs can be downloaded with the following method, as illustrated in the code chunk below which accesses the Natural Earth Data website to download the world airports layer zip file and to extract the contained Shapefile. Note that the download code is complicated by the fact that the server checks the User-agent header of the request, basically to make sure that the download takes place through a browser. To overcome this, we add a header corresponding to a request coming from a browser (such as Firefox) in our code:\n\n# Set URL+filename\nurl = 'https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_airports.zip'\nfilename = 'output/ne_10m_airports.zip'\n\n# Download\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/116.0')]\nurllib.request.install_opener(opener)\nurllib.request.urlretrieve(url, filename)\n\n# Extract\nf = zipfile.ZipFile(filename, 'r')\nf.extractall('output')\nf.close()\n\nThe Shapefile that has been created in the output directory can then be imported and plotted (Figure 7.1) as follows:\n\nne = gpd.read_file(filename.replace('.zip', '.shp'))\nne.plot();\n\n\n\n\nFigure 7.1: World airports layer, downloaded using Python from the Natural Earth Data website"
  },
  {
    "objectID": "08-read-write-plot.html#sec-geographic-data-packages",
    "href": "08-read-write-plot.html#sec-geographic-data-packages",
    "title": "7  Geographic data I/O",
    "section": "7.4 Geographic data packages",
    "text": "7.4 Geographic data packages\nMany Python packages have been developed for accessing geographic data, some of which are presented in Table 7.1. These provide interfaces to one or more spatial libraries or geoportals and aim to make data access even quicker from the command line.\n\n\nTable 7.1: Selected Python packages for geographic data retrieval\n\n\nPackage\nDescription\n\n\n\n\ncartopy\nDownload layers from Natural Earth Data\n\n\nosmnx\nAccess to OpenStreetMap data and conversion to spatial networks\n\n\n...\n…\n\n\n...\n…\n\n\n\n\nEach data package has its own syntax for accessing data. This diversity is demonstrated in the subsequent code chunks, which show how to get data using three packages from Table 7.1. Country borders are often useful and these can be accessed with the cartopy.io.shapereader.natural_earth function from the cartopy package, as follows.\n\nfilename = cartopy.io.shapereader.natural_earth(\n    resolution='10m',\n    category='cultural',\n    name='admin_2_counties'\n)\ncounties = gpd.read_file(filename)\ncounties\n\n/usr/local/lib/python3.11/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/10m_cultural/ne_10m_admin_2_counties.zip\n  warnings.warn(f'Downloading: {url}', DownloadWarning)\n\n\n\n\n\n\n\n\n\nFEATURECLA\nSCALERANK\nADM2_CODE\n...\nNAME_ZH\nNAME_ZHT\ngeometry\n\n\n\n\n0\nAdmin-2 scale rank\n0\nUSA-53073\n...\n霍特科姆县\n霍特科姆縣\nMULTIPOLYGON (((-122.75302 48.9...\n\n\n1\nAdmin-2 scale rank\n0\nUSA-53047\n...\n奥卡诺根县\n奧卡諾根縣\nPOLYGON ((-120.85196 48.99251, ...\n\n\n2\nAdmin-2 scale rank\n0\nUSA-53019\n...\n费里县\n費里縣\nPOLYGON ((-118.83688 48.99251, ...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3221\nAdmin-2 scale rank\n0\nUSA-72149\n...\n維拉爾巴\n維拉爾巴\nPOLYGON ((-66.44407 18.17665, -...\n\n\n3222\nAdmin-2 scale rank\n0\nUSA-72121\n...\n大薩瓦納\n大薩瓦納\nPOLYGON ((-66.88464 18.02481, -...\n\n\n3223\nAdmin-2 scale rank\n0\nUSA-72093\n...\n馬里考\n馬里考\nPOLYGON ((-66.89856 18.18790, -...\n\n\n\n\n3224 rows × 62 columns\n\n\n\nThe layer of counties is plotted in Figure 7.2:\n\ncounties.plot();\n\n\n\n\nFigure 7.2: US counties, downloaded from the Natural Earth Data website using package cartopy\n\n\n\n\nOther layers can be accessed the same way:\n\nyou need to locate the resolution, category, and name of the requested dataset, then\nrun the cartopy.io.shapereader.natural_earth, which downloads the file(s) and returns the path, and\nread the file into the Python environment, e.g., using gpd.read_file\n\nThis is an alternative approach to “directly” downloading files (Section 7.3).\nThe second example uses the osmnx package to find parks from the OpenStreetMap (OSM) database. As illustrated in the code-chunk below, OpenStreetMap data can be obtained using the ox.features.features_from_place functin. The first argument is a string which is geocoded to a polygon (the ox.features.features_from_bbox and ox.features.features_from_polygon can be used to query a custom area of interest). The second argument specifies the OSM tag(s), selecting which OSM elements we’re interested in (parks, in this case), represented by key-value pairs:\n\nparks = ox.features.features_from_place(\n    query='leeds uk', \n    tags={'leisure': 'park'}\n)\nparks\n\n\n\n\n\n\n\n\n\ncreated_by\ngeometry\nbarrier\n...\ndesignation\nways\ntype\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\nnode\n389460215\nNaN\nPOINT (-1.55473 53.78279)\nNaN\n...\nNaN\nNaN\nNaN\n\n\n5293573719\nNaN\nPOINT (-1.49361 53.81912)\nNaN\n...\nNaN\nNaN\nNaN\n\n\n5610301653\nNaN\nPOINT (-1.38216 53.76826)\nNaN\n...\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrelation\n13390072\nNaN\nMULTIPOLYGON (((-1.36901 53.743...\nNaN\n...\nNaN\n[321795481, 997899209]\nmultipolygon\n\n\n13433407\nNaN\nMULTIPOLYGON (((-1.35251 53.906...\nNaN\n...\nNaN\n[177323345, 1001812511]\nmultipolygon\n\n\n14552313\nNaN\nPOLYGON ((-1.42512 53.80263, -1...\nNaN\n...\nNaN\n[636081594, 1092557003]\nmultipolygon\n\n\n\n\n528 rows × 58 columns\n\n\n\nThe result is a GeoDataFrame with numerous properties. The following expression plots the geometries with the name property in the tooltips (Figure 7.3):\n\nparks[['name', 'geometry']].explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 7.3: Parks in Leeds, based on OpenStreetMap data, downloaded using package osmnx\n\n\n\nIt should be noted that the osmnx package downloads OSM data from the Overpass API, which is rate limited and therefore unsuitable for queries covering very large areas. To overcome this limitation, you can download OSM data extracts, such as in Shapefile format from Geofabrik, and then load them from the file into the Python environment.\nOpenStreetMap is a vast global database of crowd-sourced data, is growing daily, and has a wider ecosystem of tools enabling easy access to the data, from the Overpass turbo web service for rapid development and testing of OSM queries to osm2pgsql for importing the data into a PostGIS database. Although the quality of datasets derived from OSM varies, the data source and wider OSM ecosystems have many advantages: they provide datasets that are available globally, free of charge, and constantly improving thanks to an army of volunteers. Using OSM encourages ‘citizen science’ and contributions back to the digital commons (you can start editing data representing a part of the world you know well at www.openstreetmap.org).\nSometimes, packages come with built-in datasets. These can be accessed just like any other object (e.g., function) that is imported as part of the package, or in other ways as specified in the package documentation. For example, package geopandas comes with few built-in datasets (see gpd.datasets.available for a list of names). Using the gpd.datasets.get_path function and the dataset name, we can obtain the path to the location of the dataset file on our computer. For example, 'naturalearth_lowres' is a vector layer of world countries (from Natural Earth Data, which we’ve alredy met before):\n\nfilename = gpd.datasets.get_path('naturalearth_lowres')\nfilename\n\n'/usr/local/lib/python3.11/site-packages/geopandas/datasets/naturalearth_lowres/naturalearth_lowres.shp'\n\n\nThen, we can import the dataset, just like from any other file:\n\ngpd.read_file(filename)\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n889953.0\nOceania\nFiji\nFJI\n5496\nMULTIPOLYGON (((180.00000 -16.0...\n\n\n1\n58005463.0\nAfrica\nTanzania\nTZA\n63177\nPOLYGON ((33.90371 -0.95000, 34...\n\n\n2\n603253.0\nAfrica\nW. Sahara\nESH\n907\nPOLYGON ((-8.66559 27.65643, -8...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n174\n1794248.0\nEurope\nKosovo\n-99\n7926\nPOLYGON ((20.59025 41.85541, 20...\n\n\n175\n1394973.0\nNorth America\nTrinidad and Tobago\nTTO\n24269\nPOLYGON ((-61.68000 10.76000, -...\n\n\n176\n11062113.0\nAfrica\nS. Sudan\nSSD\n11998\nPOLYGON ((30.83385 3.50917, 29....\n\n\n\n\n177 rows × 6 columns\n\n\n\nAnother way to obtain spatial information is to perform geocoding—transform a description of a location, usually an address, into its coordinates. This is usually done by sending a query to an online service and getting the location as a result. Many such services exist that differ in the used method of geocoding, usage limitations, costs, or API key requirements. Nominatim is a popular free service, based on OpenStreetMap data. It can be accessed in Python uisng the osmnx.geocoder.geocode function. The function returns a tuple of the form (lat,lon). The example below searches for John Snow blue plaque coordinates located on a building in the Soho district of London:\n\nox.geocoder.geocode('54 Frith St, London W1D 4SJ, UK')\n\n(51.5138297, -0.1317359)\n\n\nIf the query returns no results, an InsufficientResponseError is raised, a scenario that the user can deal with using try/except.\nThe alternative function osmnx.geocoder.geocode_to_gdf can be used to automatically geocode multiple addresses (accepting a list of strings) and transforming them into a GeoDataFrame. This function also returns Polygon geometries. For example:\n\nresult = ox.geocoder.geocode_to_gdf(['54 Frith St, London W1D 4SJ, UK'])\nresult\n\n\n\n\n\n\n\n\ngeometry\nbbox_north\nbbox_south\n...\naddresstype\nname\ndisplay_name\n\n\n\n\n0\nPOLYGON ((-0.13193 51.51376, -0...\n51.513845\n51.513696\n...\nbuilding\n\n54, Frith Street, Soho, Islingt...\n\n\n\n\n1 rows × 17 columns\n\n\n\nThe result is visualized below using .explore (Figure 7.4):\n\nresult[['display_name', 'geometry']].explore(color='red')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 7.4: Specific address in London, geocoded into a GeoDataFrame using package osmnx"
  },
  {
    "objectID": "08-read-write-plot.html#geographic-web-services",
    "href": "08-read-write-plot.html#geographic-web-services",
    "title": "7  Geographic data I/O",
    "section": "7.5 Geographic web services",
    "text": "7.5 Geographic web services\nTo complete…"
  },
  {
    "objectID": "08-read-write-plot.html#sec-file-formats",
    "href": "08-read-write-plot.html#sec-file-formats",
    "title": "7  Geographic data I/O",
    "section": "7.6 File formats",
    "text": "7.6 File formats\nGeographic datasets are usually stored as files or in spatial databases. File formats can either store vector or raster data, while spatial databases such as PostGIS can store both. The large variety of file formats may seem bewildering, but there has been much consolidation and standardization since the beginnings of GIS software in the 1960s when the first widely distributed program (SYMAP) for spatial analysis was created at Harvard University [@coppock_history_1991].\nGDAL (which should be pronounced “goo-dal”, with the double “o” making a reference to object-orientation), the Geospatial Data Abstraction Library, has resolved many issues associated with incompatibility between geographic file formats since its release in 2000. GDAL provides a unified and high-performance interface for reading and writing of many raster and vector data formats. Many open and proprietary GIS programs, including GRASS, ArcGIS and QGIS, use GDAL behind their GUIs for doing the legwork of ingesting and spitting out geographic data in appropriate formats.\nGDAL provides access to more than 200 vector and raster data formats. Table 7.2 presents some basic information about selected and often used spatial file formats.\n\n\nTable 7.2: Commonly used spatial data file formats\n\n\n\n\n\n\n\n\n\nName\nExtension\nInfo\nType\nModel\n\n\n\n\nESRI Shapefile\n.shp (the main file)\nPopular format consisting of at least three files. No support for: files &gt; 2GB;mixed types; names &gt; 10 chars; cols &gt; 255.\nVector\nPartially open\n\n\nGeoJSON\n.geojson\nExtends the JSON exchange format by including a subset of the simple feature representation; mostly used for storing coordinates in longitude and latitude; it is extended by the TopoJSON format\nVector\nOpen\n\n\nKML\n.kml\nXML-based format for spatial visualization, developed for use with Google Earth. Zipped KML file forms the KMZ format.\nVector\nOpen\n\n\nGPX\n.gpx\nXML schema created for exchange of GPS data.\nVector\nOpen\n\n\nFlatGeobuf\n.fgb\nSingle file format allowing for quick reading and writing of vector data. Has streaming capabilities.\nVector\nOpen\n\n\nGeoTIFF\n.tif/.tiff\nPopular raster format. A TIFF file containing additional spatial metadata.\nRaster\nOpen\n\n\nArc ASCII\n.asc\nText format where the first six lines represent the raster header, followed by the raster cell values arranged in rows and columns.\nRaster\nOpen\n\n\nSQLite/SpatiaLite\n.sqlite\nStandalone relational database, SpatiaLite is the spatial extension of SQLite.\nVector and raster\nOpen\n\n\nESRI FileGDB\n.gdb\nSpatial and nonspatial objects created by ArcGIS. Allows: multiple feature classes; topology. Limited support from GDAL.\nVector and raster\nProprietary\n\n\nGeoPackage\n.gpkg\nLightweight database container based on SQLite allowing an easy and platform-independent exchange of geodata\nVector and (very limited) raster\nOpen\n\n\n\n\nAn important development ensuring the standardization and open-sourcing of file formats was the founding of the Open Geospatial Consortium (OGC) in 1994. Beyond defining the simple features data model (see Section 1.2.4), the OGC also coordinates the development of open standards, for example as used in file formats such as KML and GeoPackage. Open file formats of the kind endorsed by the OGC have several advantages over proprietary formats: the standards are published, ensure transparency and open up the possibility for users to further develop and adjust the file formats to their specific needs.\nESRI Shapefile is the most popular vector data exchange format; however, it is not an open format (though its specification is open). It was developed in the early 1990s and has a number of limitations. First of all, it is a multi-file format, which consists of at least three files. It only supports 255 columns, column names are restricted to ten characters and the file size limit is 2 GB. Furthermore, ESRI Shapefile does not support all possible geometry types, for example, it is unable to distinguish between a polygon and a multipolygon. Despite these limitations, a viable alternative had been missing for a long time. In the meantime, GeoPackage emerged, and seems to be a more than suitable replacement candidate for ESRI Shapefile. GeoPackage is a format for exchanging geospatial information and an OGC standard. The GeoPackage standard describes the rules on how to store geospatial information in a tiny SQLite container. Hence, GeoPackage is a lightweight spatial database container, which allows the storage of vector and raster data but also of non-spatial data and extensions. Aside from GeoPackage, there are other geospatial data exchange formats worth checking out (Table 7.2).\nThe GeoTIFF format seems to be the most prominent raster data format. It allows spatial information, such as the CRS definition and the transformation matrix (see Section 1.3.2), to be embedded within a TIFF file. Similar to ESRI Shapefile, this format was firstly developed in the 1990s, but as an open format. Additionally, GeoTIFF is still being expanded and improved. One of the most significant recent addition to the GeoTIFF format is its variant called COG (Cloud Optimized GeoTIFF). Raster objects saved as COGs can be hosted on HTTP servers, so other people can read only parts of the file without downloading the whole file (see Sections 8.6.2 and 8.7.2…).\nThere is also a plethora of other spatial data formats that we do not explain in detail or mention in Table 7.2 due to the book limits. If you need to use other formats, we encourage you to read the GDAL documentation about vector and raster drivers. Additionally, some spatial data formats can store other data models (types) than vector or raster. It includes LAS and LAZ formats for storing lidar point clouds, and NetCDF and HDF for storing multidimensional arrays.\nFinally, spatial data is also often stored using tabular (non-spatial) text formats, including CSV files or Excel spreadsheets. This can be convenient to share spatial datasets with people who, or software that, struggle with spatial data formats."
  },
  {
    "objectID": "08-read-write-plot.html#sec-data-input",
    "href": "08-read-write-plot.html#sec-data-input",
    "title": "7  Geographic data I/O",
    "section": "7.7 Data input (I)",
    "text": "7.7 Data input (I)\nExecuting commands such as geopandas.read_file (the main function we use for loading vector data) or rasterio.open+.read (the main functions used for loading raster data) silently sets off a chain of events that reads data from files. Moreover, there are many Python packages containing a wide range of geographic data or providing simple access to different data sources. All of them load the data into the Python environment or, more precisely, assign objects to your workspace, stored in RAM and accessible within the Python session.\n\n7.7.1 Vector data\nSpatial vector data comes in a wide variety of file formats. Most popular representations such as .shp, .geojson, and .gpkg files can be imported and exported with geopandas functions read_file and to_file (covered in Section @ref(sec-data-output)), respectively.\ngeopandas uses GDAL to read and write data, via fiona (the default) or pyogrio packages (a recently developed alternative to fiona). After fiona is imported, the command fiona.supported_drivers can be used to list drivers available to GDAL, including whether they can (r), append (a), or write (w) data, or all three:\n\nfiona.supported_drivers\n\n{'DXF': 'rw',\n 'CSV': 'raw',\n 'OpenFileGDB': 'raw',\n 'ESRIJSON': 'r',\n 'ESRI Shapefile': 'raw',\n 'FlatGeobuf': 'raw',\n 'GeoJSON': 'raw',\n 'GeoJSONSeq': 'raw',\n 'GPKG': 'raw',\n 'GML': 'rw',\n 'OGR_GMT': 'rw',\n 'GPX': 'rw',\n 'MapInfo File': 'raw',\n 'DGN': 'raw',\n 'S57': 'r',\n 'SQLite': 'raw',\n 'TopoJSON': 'r'}\n\n\nOther, less common, drivers can be “activated” by manually supplementing fiona.supported_drivers. The first argument of the geopandas versatile data import function gpd.read_file is filename, which is typically a string, but can also be a file connection. The content of a string could vary between different drivers. In most cases, as with the ESRI Shapefile (.shp) or the GeoPackage format (.gpkg), the filename argument would be a path or a URL to an actual file, such as geodata.gpkg. The driver is automatically selected based on the file extension, as demonstrated for a .gpkg file below:\n\nworld = gpd.read_file('data/world.gpkg')\nworld\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.960000\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n1\nTZ\nTanzania\nAfrica\n...\n64.163000\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n2\nEH\nWestern Sahara\nAfrica\n...\nNaN\nNaN\nMULTIPOLYGON (((-8.66559 27.656...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nXK\nKosovo\nEurope\n...\n71.097561\n8698.291559\nMULTIPOLYGON (((20.59025 41.855...\n\n\n175\nTT\nTrinidad and Tobago\nNorth America\n...\n70.426000\n31181.821196\nMULTIPOLYGON (((-61.68000 10.76...\n\n\n176\nSS\nSouth Sudan\nAfrica\n...\n55.817000\n1935.879400\nMULTIPOLYGON (((30.83385 3.5091...\n\n\n\n\n177 rows × 11 columns\n\n\n\nFor some drivers, such as a File Geodatabase (OpenFileGDB), filename could be provided as a folder name. GeoJSON string can also be read from a character string:\n\ngpd.read_file('{\"type\":\"Point\",\"coordinates\":[34.838848,31.296301]}')\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (34.83885 31.29630)\n\n\n\n\n\n\n\nAlternatively, the gpd.read_postgis function can be used to read a vector layer from a PostGIS database.\nSome vector formats, such as GeoPackage, can store multiple data layers. By default, gpd.read_file automatically reads the first layer of the file specified in filename. However, using the layer argument you can specify any other layer.\nThe gpd.read_file function also allows for reading just parts of the file into RAM with two possible mechanisms. The first one is related to the where argument, which allows specifying what part of the data to read using an SQL WHERE expression. An example below extracts data for Tanzania only (Figure …). It is done by specifying that we want to get all rows for which name_long equals to \"Tanzania\":\n\ntanzania = gpd.read_file('data/world.gpkg', where='name_long=\"Tanzania\"')\ntanzania\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nTZ\nTanzania\nAfrica\n...\n64.163\n2402.099404\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n\n\n1 rows × 11 columns\n\n\n\nIf you do not know the names of the available columns, a good approach is to just read one row of the data using the rows argument, which can be used to read the first N rows, then use the .columns property to examine the column names:\n\ngpd.read_file('data/world.gpkg', rows=1).columns\n\nIndex(['iso_a2', 'name_long', 'continent', 'region_un', 'subregion', 'type',\n       'area_km2', 'pop', 'lifeExp', 'gdpPercap', 'geometry'],\n      dtype='object')\n\n\nThe second mechanism uses the mask argument to filter data based on intersection with an existing geometry. This argument expects a geometry (GeoDataFrame, GeoSeries, or shapely) representing the area where we want to extract the data. Let’s try it using a small example—we want to read polygons from our file that intersect with the buffer of 50,000 \\(m\\) of Tanzania’s borders. To do it, we need to (a) transform the geometry to a projected CRS (such as EPSG:32736), (b) prepare our “filter” by creating the buffer (Section 4.3.3), and (c) transform back to the original CRS to be used as a mask:\n\ntanzania_buf = tanzania.to_crs(32736).buffer(50000).to_crs(4326)\ntanzania_buf.iloc[0]\n\n\n\n\nNow, we can apply this “filter” using the mask argument.\n\ntanzania_neigh = gpd.read_file('data/world.gpkg', mask=tanzania_buf)\ntanzania_neigh\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nMZ\nMozambique\nAfrica\n...\n57.099\n1079.823866\nMULTIPOLYGON (((34.55999 -11.52...\n\n\n1\nZM\nZambia\nAfrica\n...\n60.775\n3632.503753\nMULTIPOLYGON (((30.74001 -8.340...\n\n\n2\nMW\nMalawi\nAfrica\n...\n61.932\n1090.367208\nMULTIPOLYGON (((32.75938 -9.230...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6\nBI\nBurundi\nAfrica\n...\n56.688\n803.172837\nMULTIPOLYGON (((30.46967 -2.413...\n\n\n7\nUG\nUganda\nAfrica\n...\n59.224\n1637.275081\nMULTIPOLYGON (((33.90371 -0.950...\n\n\n8\nRW\nRwanda\nAfrica\n...\n66.188\n1629.868866\nMULTIPOLYGON (((30.41910 -1.134...\n\n\n\n\n9 rows × 11 columns\n\n\n\nOur result, shown in Figure 7.5, contains Tanzania and every country within its 50,000 \\(m\\) buffer. Note that the last two expressions are used to add text labels with the name_long of each country, placed at the country centroid:\n\nfig, ax = plt.subplots(ncols=2, figsize=(9,5))\ntanzania.plot(ax=ax[0], color='lightgrey', edgecolor='grey')\ntanzania_neigh.plot(ax=ax[1], color='lightgrey', edgecolor='grey')\ntanzania_buf.plot(ax=ax[1], color='none', edgecolor='red')\nax[0].set_title('where')\nax[1].set_title('mask')\ntanzania.apply(lambda x: ax[0].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\ntanzania_neigh.apply(lambda x: ax[1].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\n\n\n\n\nFigure 7.5: Reading a subset of the vector data using a where query (left) and a mask (right)\n\n\n\n\nOften we need to read CSV files (or other tabular formats) which have x and y coordinate columns, and turn them into a GeoDataFrame with point geometries. To do that, we can import the file using pandas (e.g., pd.read_csv or pd.read_excel), then go from DataFrame to GeoDataFrame using the gpd.points_from_xy function, as shown earlier in the book (See Section 1.2.6 and Section 3.3.4). For example, the table cycle_hire_xy.csv, where the coordinates are stored in the X and Y columns in EPSG:4326, can be imported, converted to a GeoDataFrame, and plotted, as follows:\n\ncycle_hire = pd.read_csv('data/cycle_hire_xy.csv')\ngeom = gpd.points_from_xy(cycle_hire['X'], cycle_hire['Y'], crs=4326)\ngeom = gpd.GeoSeries(geom)\ncycle_hire_xy = gpd.GeoDataFrame(data=cycle_hire, geometry=geom)\ncycle_hire_xy.plot();\n\n\n\n\nInstead of columns describing ‘XY’ coordinates, a single column can also contain the geometry information. Well-known text (WKT), well-known binary (WKB), and the GeoJSON formats are examples of this. For instance, the world_wkt.csv file has a column named WKT representing polygons of the world’s countries. To import and convert it to a GeoDataFrame, we can apply the shapely.from_wkt function (Section 1.2.5) on WKT strings, to convert them into shapely geometries:\n\nworld_wkt = pd.read_csv('data/world_wkt.csv')\nworld_wkt['geometry'] = world_wkt['WKT'].apply(shapely.from_wkt)\nworld_wkt = gpd.GeoDataFrame(world_wkt)\nworld_wkt.plot();\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNot all of the supported vector file formats store information about their coordinate reference system. In these situations, it is possible to add the missing information using the .set_crs function. Please refer also to Section 6.4 for more information.\n\n\nAs a final example, we will show how geopandas also reads KML files. A KML file stores geographic information in XML format—a data format for the creation of web pages and the transfer of data in an application-independent way (Nolan and Lang 2014 …). Here, we access a KML file from the web. First, we need to “activate” the KML driver, which isn’t available by default (see above):\n\nfiona.supported_drivers['KML'] = 'r'\n\nThis file contains more than one layer. To list the available layers, we can use the fiona.listlayers function:\n\nu = 'https://developers.google.com/kml/documentation/KML_Samples.kml'\nfiona.listlayers(u)\n\n['Placemarks',\n 'Highlighted Icon',\n 'Paths',\n 'Google Campus',\n 'Extruded Polygon',\n 'Absolute and Relative']\n\n\nFinally, we can choose the first layer Placemarks and read it, using gpd.read_file with an additional layer argument:\n\nplacemarks = gpd.read_file(u, layer='Placemarks')\n\n\n\n7.7.2 Raster data\nSimilar to vector data, raster data comes in many file formats with some of them supporting multilayer files. rasterio.open is used to create a file connection to a raster file, which can be subsequently used to read the metadata and/or the values, as shown previously (Section 1.3.2). For example:\n\nsrc = rasterio.open('data/srtm.tif')\nsrc\n\n&lt;open DatasetReader name='data/srtm.tif' mode='r'&gt;\n\n\nAll of the previous examples read spatial information from files stored on your hard drive. However, GDAL also allows reading data directly from online resources, such as HTTP/HTTPS/FTP web resources. The only thing we need to do is to add a /vsicurl/ prefix before the path to the file. Let’s try it by connecting to the global monthly snow probability at 500 \\(m\\) resolution for the period 2000-2012 (T. Hengl 2021 add reference…). Snow probability for December is stored as a Cloud Optimized GeoTIFF (COG) file (see Section 7.6). To read an online file, we just need to provide its URL together with the /vsicurl/ prefix:\n\nurl = \"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif\"\nsrc = rasterio.open(url)\nsrc\n\n&lt;open DatasetReader name='/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif' mode='r'&gt;\n\n\nIn the example above rasterio.open creates a connection to the file without obtaining any values, as we did for the local srtm.tif file. The values can read, into an ndarray, using the .read method of the file connection (Section 1.3.2). This allows us also to just read a small portion of the data without downloading the entire file. This is very useful when working with large datasets hosted online from resource-constrained computing environments such as laptops.\nAnother option is to extract raster values at particular points, directly from the file connection, using the .sample method (see Section 3.4.1). For example, we can get the snow probability for December in Reykjavik (70%) by specifying its coordinates and applying .sample:\n\nvalues = src.sample([(-21.94, 64.15)])\nlist(values)\n\n[array([70], dtype=uint8)]\n\n\nThe example above efficiently extracts and downloads a single value instead of the entire GeoTIFF file, saving valuable resources. The /vsicurl/ prefix also works for vector file formats, enabling you to import datasets from online storage with geopandas just by adding it before the vector file URL.\nImportantly, /vsicurl/ is not the only prefix provided by GDAL—many more exist, such as /vsizip/ to read spatial files from ZIP archives without decompressing them beforehand or /vsis3/ for on-the-fly reading files available in AWS S3 buckets. You can learn more about it at https://gdal.org/user/virtual_file_systems.html.\n(To add example of reading rectangular extent…)"
  },
  {
    "objectID": "08-read-write-plot.html#sec-data-output",
    "href": "08-read-write-plot.html#sec-data-output",
    "title": "7  Geographic data I/O",
    "section": "7.8 Data output (O)",
    "text": "7.8 Data output (O)\nWriting geographic data allows you to convert from one format to another and to save newly created objects for permanent storage. Depending on the data type (vector or raster), object class (e.g., GeoDataFrame), and type and amount of stored information (e.g., object size, range of values), it is important to know how to store spatial files in the most efficient way. The next two sections will demonstrate how to do this.\n\n7.8.1 Vector data\nThe counterpart of gpd.read_file is the .to_file method that a GeoDataFrame has. It allows you to write GeoDataFrame objects to a wide range of geographic vector file formats, including the most common, such as .geojson, .shp and .gpkg. Based on the file name, .to_file decides automatically which driver to use. The speed of the writing process depends also on the driver.\n\nworld.to_file('output/world.gpkg')\n\nNote: if you try to write to the same data source again, the function will overwrite the file:\n\nworld.to_file('output/world.gpkg')\n\nInstead of overwriting the file, we could add a new layer to the file with mode='a' (“append” mode, as opposed to the default mode='w' for “write” mode). Appending is supported by several spatial formats, including GeoPackage. For example:\n\nworld.to_file('output/world_many_features.gpkg')\nworld.to_file('output/world_many_features.gpkg', mode='a')\n\nHere, world_many_features.gpkg will contain a polygonal layer named world with two “copies” of each country (that is 177×2=354 features, whereas the world layer has 177 features).\nAlternatively, you can create another, separate, layer, within the same file. The GeoPackage format also supports multiple layers within one file. For example:\n\nworld.to_file('output/world_many_layers.gpkg')\nworld.to_file('output/world_many_layers.gpkg', layer='world2')\n\nIn this case, world_many_layers.gpkg has two “layers”, world_many_layers (same as the file name, when layer is unspecified) and world2. Incidentally, the contents of the two layers is identical, but this doesn’t have to be so. Each layer from such a file can be imported separately, as in:\n\ngpd.read_file('output/world_many_layers.gpkg', layer='world_many_layers').head(1)\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.96\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n\n\n1 rows × 11 columns\n\n\n\n\ngpd.read_file('output/world_many_layers.gpkg', layer='world2').head(1)\n\n\n\n\n\n\n\n\niso_a2\nname_long\ncontinent\n...\nlifeExp\ngdpPercap\ngeometry\n\n\n\n\n0\nFJ\nFiji\nOceania\n...\n69.96\n8222.253784\nMULTIPOLYGON (((-180.00000 -16....\n\n\n\n\n1 rows × 11 columns\n\n\n\n\n\n7.8.2 Raster data\nTo write a raster file using rasterio, we need to pass a raster file path to rasterio.open, in writing ('w') mode. This implies creating a new empty file (or overwriting an existing one). As opposed to read ('r', the default) mode, the rasterio.open function needs quite a lot of information, in addition to the file path and mode:\n\nAn array with the raster values\nMetadata describing the raster format and spatial properties\n\nThe metadata needs to specify the following properties:\n\ndriver—The file format (The recommendation is 'GTiff' for GeoTIFF)\nheight—Number of rows\nwidth—Number of columns\ncount—Number of bands\nnodata—The value which represents “No Data”, if any\ndtype—The raster data type, one of numpy types (e.g., np.int64)\ncrs—The CRS, using an EPSG code (e.g., 4326)\ntransform—The transform matrix\ncompress—A compression method to apply, such as 'lzw'. This is optional and most useful for large rasters. Note that, at the time of writing, this doesn’t work well for writing multiband rasters.\n\nOnce the file connection with the right metadata is ready, we do the actual writing using the .write method of the file connection. If there are several bands we may execute the .write method several times, as in .write(a,n), where a is the array with band values and n is the band index (starting from 1, see below). When done, we close the file connection using the .close method. Some functions, such as rasterio.warp.reproject used for resampling and reprojecting, directly accept a file connection in 'w' mode, thus handling the writing (of a resampled or reprojected raster) for us.\nMost of the properties are either straightforward to choose, based on our aims, (e.g., driver, crs, compress, nodata), or directly derived from the array with the raster values itself (e.g., height, width, count, dtype). The most complicated property is the transform, which specifies the raster origin and resolution. The transform is typically either obtained from an existing raster (serving as a “template”), or created from scratch based on manually specified origin and resolution values (e.g., using rasterio.transform.from_origin), or calculate automatically (e.g., using rasterio.warp.calculate_default_transform).\nEarlier in the book, we have already demonstrated the four most common scenarios of writing rasters:\n\nCreating from scratch (Section 1.3.3)—We created and wrote two rasters from scratch by associating the elev and grain arrays with an arbitrary spatial extent. The custom arbitrary transform created using rasterio.transform.from_origin.\nAggregating (Section 4.4.3)—We wrote an aggregated a raster, by reading a resampled array from an exising raster, then updating the transform using .transform.scale.\nResampling (Section 4.4.4)—We resampled a raster into a custom grid, manually creating the transform using rasterio.transform.from_origin, then resampling and writing the output using rasterio.warp.reproject.\nReprojecting (Section 6.9)—We reprojected a raster into another CRS, by automatically calculating an optimal transform using rasterio.warp.calculate_default_transform, then resampling and writing the output using rasterio.warp.reproject.\n\nA miminal example of writing a raster file named r.tif from scratch (i.e., the 1st scenario), to remind some of these concepts, is given below:\n\n# An array with raster values\nr = np.array([1,2,3,4]).reshape(2,2).astype(np.int8)\nr\n\narray([[1, 2],\n       [3, 4]], dtype=int8)\n\n\n\n# Calculating the transform\nnew_transform = rasterio.transform.from_origin(\n    west=-0.5, \n    north=51.5, \n    xsize=2, \n    ysize=2\n)\nnew_transform\n\nAffine(2.0, 0.0, -0.5,\n       0.0, -2.0, 51.5)\n\n\n\n# Creating the file connection with the metadata\ndst = rasterio.open(\n    'output/r.tif', 'w', \n    driver = 'GTiff',\n    height = r.shape[0],\n    width = r.shape[1],\n    count = 1,\n    dtype = r.dtype,\n    crs = 4326,\n    transform = new_transform\n)\ndst\n\n&lt;open DatasetWriter name='output/r.tif' mode='w'&gt;\n\n\n\n# Writing the array values into the file\ndst.write(r, 1)\n\n\n# Closing the file\ndst.close()\n\nThis code section creates a new file output/r.tif, which is a \\(2 \\times 2\\) raster, having a 2 decimal degree resolution, with the top-left corner placed over London.\nTo summarize, the various scenarios differ in two aspects:\n\nThe way that the transform for the output raster is obtained:\n\nImported from an existing raster (see below)\nCreated from scratch, using rasterio.transform.from_origin (Section 1.3.3)\nCalculate automatically, using rasterio.warp.calculate_default_transform (Section 6.9)\n\nThe way that the raster is written:\n\nUsing the .write method, given an existing array (Section 1.3.3, Section 4.4.3)\nUsing rasterio.warp.reproject to calculate and write a resampled or reprojected array (Section 4.4.4, Section 6.9)\n\n\nTo make the picture of raster export complete, there are three important concepts we haven’t covered yet: array and raster data types, writing multiband rasters, and handling “No Data” values.\nArrays (i.e., ndarray objects defined in package numpy) are used to store raster values when reading them from file, using .read (Section 1.3.2). All values in an array are of the same type, whereas the numpy package supports numerous numeric data types of various precision (and, accordingly, memory footprint). Raster formats, such as GeoTIFF, support exactly the same data types, which means that reading a raster file uses as little RAM as possible. The most relevant types are summarized in Table 7.3.\n\n\nTable 7.3: Numeric numpy data which are commonly used for rasters\n\n\nData type\nDescription\n\n\n\n\nint8\nInteger in a single byte (-128 to 127)\n\n\nint16\nInteger in 16 bits (-32768 to 32767)\n\n\nint32\nInteger in 32 bits (-2147483648 to 2147483647)\n\n\nuint8\nUnsigned integer (0 to 255)\n\n\nuint16\nUnsigned integer (0 to 65535)\n\n\nuint32\nUnsigned integer (0 to 4294967295)\n\n\nfloat16\nHalf-precision (16 bit) float (-65504 to 65504)\n\n\nfloat32\nSingle-precision (32 bit) float (1e-38 to 1e38)\n\n\nfloat64\nDouble-precision (64 bit) float (1e-308 to 1e308)\n\n\n\n\nThe raster data type can be specified when writing a raster (see above). For an existing raster file, the data type is accessible through the .dtype property of the metadata:\n\nrasterio.open('output/r.tif').meta['dtype']\n\n'int8'\n\n\nThe file r.tif has the data type np.int8, which we specified when creating it according to the data type of the original array:\n\nr.dtype\n\ndtype('int8')\n\n\nWhen reading the data back into the Python session, the array with the same data type is recreated:\n\nrasterio.open('output/r.tif').read().dtype\n\ndtype('int8')\n\n\nWriting multiband rasters is similar to writing single-band rasters, only that we need to:\n\nDefine the number of layers (the count property in the metadata) that are going to be in the file we are creating\nExecute the .write method multiple times, once for each layer\n\nFor completeness, let’s demonstrate writing a multi-band raster named r3.tif, which is similar to r.tif, but having three bands with values r, r*2, and r*3 (i.e., the array r multiplied by 1, 2, or 3). Since most of the metadata is going to be the same, this is also a good opportunity to (re-)demonstrate updating an existing metadata object rather than creating one from scratch.\nFirst, let’s make a copy of the metadata we already have in r.tif:\n\ndst_kwds = rasterio.open('output/r.tif').meta.copy()\ndst_kwds\n\n{'driver': 'GTiff',\n 'dtype': 'int8',\n 'nodata': None,\n 'width': 2,\n 'height': 2,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(2.0, 0.0, -0.5,\n        0.0, -2.0, 51.5)}\n\n\nSecond, we update the count entry, replacing 1 (single-band) with 3 (three-band):\n\ndst_kwds.update(count=3)\ndst_kwds\n\n{'driver': 'GTiff',\n 'dtype': 'int8',\n 'nodata': None,\n 'width': 2,\n 'height': 2,\n 'count': 3,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(2.0, 0.0, -0.5,\n        0.0, -2.0, 51.5)}\n\n\nFinally, we can create a file connection using the updated metadata and then write the values of the three bands:\n\ndst = rasterio.open('output/r3.tif', 'w', **dst_kwds)\ndst.write(r,   1)\ndst.write(r*2, 2)\ndst.write(r*3, 3)\ndst.close()\n\nAs a result, a three-band raster named r3.tif is created.\nRasters often contain “No Data” values, representing missing data, e.g., unreliable measurement due to clouds or pixels outside of the photographed extent. In a numpy ndarray object, “No Data” values may be represented by the special np.nan value. However, due to computer memory limitations, only arrays of type float can contain np.nan, while arrays of type int cannot. For int rasters containing “No Data”, we typically mark missing data with a specific value beyond the valid range (e.g., -9999). The missing data “flag” is stored in the file (set through the nodata property of the file connection, see above). When reading an int raster with “No Data” back into Python, we need to be aware of these flags. Let’s demonstrate through examples.\nWe will start with the simpler case, rasters of type float. Since float arrays may contain the “native” value np.nan, representing “No Data” is straightforward. For example, suppose that we have a float array with np.nan:\n\nr = np.array([1.1,2.1,np.nan,4.1]).reshape(2,2)\nr\n\narray([[1.1, 2.1],\n       [nan, 4.1]])\n\n\n\nr.dtype\n\ndtype('float64')\n\n\nWhen writing the array to file, we do not need to specify any particular nodata value:\n\ndst = rasterio.open(\n    'output/r_nodata_float.tif', 'w', \n    driver = 'GTiff',\n    height = r.shape[0],\n    width = r.shape[1],\n    count = 1,\n    dtype = r.dtype,\n    crs = 4326,\n    transform = new_transform\n)\ndst.write(r, 1)\ndst.close()\n\nThis is equivalent to nodata=None:\n\nrasterio.open('output/r_nodata_float.tif').meta\n\n{'driver': 'GTiff',\n 'dtype': 'float64',\n 'nodata': None,\n 'width': 2,\n 'height': 2,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(2.0, 0.0, -0.5,\n        0.0, -2.0, 51.5)}\n\n\nReading from the raster back into the Python session reproduces the same exact array, with np.nan:\n\nrasterio.open('output/r_nodata_float.tif').read()\n\narray([[[1.1, 2.1],\n        [nan, 4.1]]])\n\n\nNow, suppose that we have an np.int32 array with missing data, which is inevitably flagged using a specific int value such as -9999 (remember that we can’t store np.nan in an int array!):\n\nr = np.array([1,2,-9999,4]).reshape(2,2).astype(np.int32)\nr\n\narray([[    1,     2],\n       [-9999,     4]], dtype=int32)\n\n\n\nr.dtype\n\ndtype('int32')\n\n\nWhen writing the array to file, we must specify nodata=-9999 to keep track of our “No Data” flag:\n\ndst = rasterio.open(\n    'output/r_nodata_int.tif', 'w', \n    driver = 'GTiff',\n    height = r.shape[0],\n    width = r.shape[1],\n    count = 1,\n    dtype = r.dtype,\n    nodata = -9999,\n    crs = 4326,\n    transform = new_transform\n)\ndst.write(r, 1)\ndst.close()\n\nExamining the metadata confirms that the nodata=-9999 setting was stored in the file r_nodata_int.tif.\n\nrasterio.open('output/r_nodata_int.tif').meta\n\n{'driver': 'GTiff',\n 'dtype': 'int32',\n 'nodata': -9999.0,\n 'width': 2,\n 'height': 2,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(2.0, 0.0, -0.5,\n        0.0, -2.0, 51.5)}\n\n\nIf you try to open the file in GIS software, such as QGIS, you will see the missing data interpreted (e.g., the pixel shown as blank), meaning that the software is aware of the flag. However, reading the data back into Python reproduces an int array with -9999, for the same reason stated before:\n\nsrc = rasterio.open('output/r_nodata_int.tif')\nr = src.read()\nr\n\narray([[[    1,     2],\n        [-9999,     4]]], dtype=int32)\n\n\nThe Python user must thefore be mindful of “No Data” int rasters, for example to avoid interpreting the value -9999 literally. For example, if we “forget” about the nodata flag, the literal calculation of the .mean would incorrectly include the value -9999:\n\nr.mean()\n\n-2498.0\n\n\nThere are two basic ways to deal with the situation:\n\nConverting the raster to float\nUsing “No Data” masks\n\nFirst, particularly with small rasters where memory constraints are irrelevant, it may be more convenient to go from int to float, to gain the ability of the natural np.nan representation. Here is how we can do this with r_nodata_int.tif. We detect the missing data flag, conver the raster to float, and assign np.nan into the cells that are supposed to be missing:\n\nmask = r == src.nodata\nr = r.astype(np.float64)\nr[mask] = np.nan\nr\n\narray([[[ 1.,  2.],\n        [nan,  4.]]])\n\n\nFrom there on, we deal with np.nan the usual way, such as using np.nanmean to calculate the mean excluding “No Data”:\n\nnp.nanmean(r)\n\n2.3333333333333335\n\n\nThe second approach is to read the values into a so-called “masked” array, using the argument masked=True. A masked array can be thought of as an extended ndarray, with two components: .data (the values) and .mask (a corresponding boolean array marking “No Data” values):\n\nr = src.read(masked=True)\nr\n\nmasked_array(\n  data=[[[1, 2],\n         [--, 4]]],\n  mask=[[[False, False],\n         [ True, False]]],\n  fill_value=-9999,\n  dtype=int32)\n\n\nUsing masked arrays is beyond the scope of this book. However, the basic idea is that many numpy operations “honor” the mask, so that the user does not have to keep track of the way that “No Data” values are marked, similarly to the natural np.nan representation. For example, the .mean of a masked array ignores the value -9999, because it is masked, taking into account just the valid values 1, 2, and 4:\n\nr.mean()\n\n2.3333333333333335\n\n\nKeep in mind that, somewhat confusingly, float rasters may represent “No Data” using a specific value (such as -9999.0), instead, or in addition to (!), the native np.nan representation. In such cases, the same considerations shown for int apply to float rasters as well."
  },
  {
    "objectID": "08-read-write-plot.html#exercises",
    "href": "08-read-write-plot.html#exercises",
    "title": "7  Geographic data I/O",
    "section": "7.9 Exercises",
    "text": "7.9 Exercises"
  },
  {
    "objectID": "08-read-write-plot.html#footnotes",
    "href": "08-read-write-plot.html#footnotes",
    "title": "7  Geographic data I/O",
    "section": "",
    "text": "For example, visit https://freegisdata.rtwilson.com/ for a list of websites with freely available geographic datasets.↩︎"
  },
  {
    "objectID": "09-mapping.html#prerequisites",
    "href": "09-mapping.html#prerequisites",
    "title": "8  Making maps with Python",
    "section": "8.1 Prerequisites",
    "text": "8.1 Prerequisites\nLet’s import the required packages:\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.plot\nimport contextily as ctx\n\nand load the sample data for this chapter:\n\nnz = gpd.read_file('data/nz.gpkg')\nnz_height = gpd.read_file('data/nz_height.gpkg')\nnz_elev = rasterio.open('data/nz_elev.tif')\ntanzania = gpd.read_file('data/world.gpkg', where='name_long=\"Tanzania\"')\ntanzania_buf = tanzania.to_crs(32736).buffer(50000).to_crs(4326)\ntanzania_neigh = gpd.read_file('data/world.gpkg', mask=tanzania_buf)"
  },
  {
    "objectID": "09-mapping.html#introduction",
    "href": "09-mapping.html#introduction",
    "title": "8  Making maps with Python",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\n\n\n\nA satisfying and important aspect of geographic research is communicating the results. Map making—the art of cartography—is an ancient skill that involves communication, intuition, and an element of creativity. In addition to being fun and creative, cartography also has important practical applications. A carefully crafted map can be the best way of communicating the results of your work, but poorly designed maps can leave a bad impression. Common design issues include poor placement, size and readability of text and careless selection of colors, as outlined in the style guide of the Journal of Maps. Furthermore, poor map making can hinder the communication of results (Brewer 2015, add citation…):\n\nAmateur-looking maps can undermine your audience’s ability to understand important information and weaken the presentation of a professional data investigation. Maps have been used for several thousand years for a wide variety of purposes. Historic examples include maps of buildings and land ownership in the Old Babylonian dynasty more than 3000 years ago and Ptolemy’s world map in his masterpiece Geography nearly 2000 years ago (Talbert 2014, add citation…).\n\nMap making has historically been an activity undertaken only by, or on behalf of, the elite. This has changed with the emergence of open source mapping software such as mapping packages in Python, R, and other languages, and the “print composer” in QGIS which enable anyone to make high-quality maps, enabling “citizen science”. Maps are also often the best way to present the findings of geocomputational research in a way that is accessible. Map making is therefore a critical part of geocomputation and its emphasis not only on describing, but also changing the world.\nBasic static display of vector layers in Python is done with the .plot method or the rasterio.plot.show function, for vector layers and rasters, as we saw in Sections Section 1.2.2 and Section 1.3.2, respectively. Other, more advaned uses of these methods, were also encountered in later chapters, when demonstrating the various outputs we got. In this chapter, we provide a comprehensive summary of the most useful workflows of these two methods for creating static maps (Section 8.3). Then, we move on to elaborate on the .explore method for creating interactive maps, which was also briefly introduced earlier (Section 1.2.2)."
  },
  {
    "objectID": "09-mapping.html#sec-static-maps",
    "href": "09-mapping.html#sec-static-maps",
    "title": "8  Making maps with Python",
    "section": "8.3 Static maps",
    "text": "8.3 Static maps\nStatic maps are the most common type of visual output from geocomputation. Standard formats include .png and .pdf for raster and vector outputs, respectively. Static maps can be easily shared and viewed (whether digitally or in print), however they can only convey as much information as a static image can. Interactive maps provide much more flexibilty in terms of user experience and amout of information, however they often require more work to design and effectively share.\n\n\nLet’s move on to the basics of static mapping with Python.\n\n8.3.1 Minimal example\nA vector layer (GeoDataFrame) or a geometry column (GeoSeries) can be displayed using their .plot method (Section 1.2.2). A minimal example of a vector layer map is obtained using .plot with nothing but the defaults (Figure 8.1):\n\nnz.plot();\n\n\n\n\nFigure 8.1: Minimal example of a static vector layer plot with .plot\n\n\n\n\nA rasterio raster file connection, or a numpy ndarray, can be displayed using rasterio.plot.show (Section 1.3.2). Here is a minimal example of a raster plot (Figure 8.2):\n\nrasterio.plot.show(nz_elev);\n\n\n\n\nFigure 8.2: Minimal example of a static raster plot with rasterio.plot.show\n\n\n\n\n\n\n8.3.2 Styling\nMost useful visual properties of the geometries that can be specified in .plot include color, edgecolor, and markersize (for points) (Figure 8.3):\n\nnz.plot(color='grey');\nnz.plot(color='none', edgecolor='blue');\nnz.plot(color='grey', edgecolor='blue');\n\n\n\n\n\n\n\n(a) Grey fill\n\n\n\n\n\n\n\n(b) No fill, blue edge\n\n\n\n\n\n\n\n(c) Grey fill, blue edge\n\n\n\n\nFigure 8.3: Setting color and edgecolor in static maps of a vector layer\n\n\n\nAnd here is an example of using markersize to get larger points (Figure 8.4). This example also demonstrated how to control the overall figure size, such as \\(4 \\times 4\\) \\(in\\) in this case:\n\nfig, ax = plt.subplots(figsize=(4,4))\nnz_height.plot(markersize=100, ax=ax);\n\n\n\n\nFigure 8.4: Setting markersize in a static map of a vector layer\n\n\n\n\n\n\n8.3.3 Symbology\nWe can set symbology in a .plot using the following parameters:\n\ncolumn—A column name\nlegend—Whether to show a legend\ncmap—Color map\n\nFor example, here we plot stops points colored according to their 'Median_income' attribute (Figure 8.5):\n\nnz.plot(column='Median_income', legend=True);\n\n\n\n\nFigure 8.5: Symbology in a static map created with .plot\n\n\n\n\nThe default color scale which you see in Figure 8.5 is cmap='viridis'. However, the cmap (“color map”) argument can be used to specify any of countless other color scales. A first safe choice is often the ColorBrewer collection of color scales, specifically chosen for mapping uses. Any color scale can be reversed, using the _r suffic. Finally, other color scales are available, see the matplotlib colormaps article for details. The following code sections demonstrates these color scale specifications (Figure 8.6):\n\nnz.plot(column='Median_income', legend=True, cmap='Reds');\nnz.plot(column='Median_income', legend=True, cmap='Reds_r');\nnz.plot(column='Median_income', legend=True, cmap='spring');\n\n\n\n\n\n\n\n(a) The 'Reds' color scale from ColorBrewer\n\n\n\n\n\n\n\n(b) Reversed 'Reds' color scale\n\n\n\n\n\n\n\n(c) The 'spring' color scale from matplotlib\n\n\n\n\nFigure 8.6: Symbology in a static map of a vector layer, created with .plot\n\n\n\nCategorical symbology is also supported, such as when column points to a string attribute. For example, the following expression sets symbology according to the 'Island' column. In this case, it makes sense to use a qualitative color scale, such as 'Set1' from ColorBrewer (Figure 8.7):\n\nnz.plot(column='Island', legend=True, cmap='Set1');\n\n\n\n\nFigure 8.7: Symbology for a categorical variable\n\n\n\n\nIn case the legend interferes with the contents (such as in Figure 8.7), we can modify the legend position as follows (Figure 8.8):\n\nnz.plot(column='Island', legend=True, cmap='Set1', legend_kwds={'loc': 4});\n\n\n\n\nFigure 8.8: Setting legend position in .plot\n\n\n\n\nThe rasterio.plot.show function, based on matplotlib as well, supports the same kinds of cmap arguments. For example (Figure 8.9):\n\nrasterio.plot.show(nz_elev, cmap='BrBG');\nrasterio.plot.show(nz_elev, cmap='BrBG_r');\nrasterio.plot.show(nz_elev, cmap='nipy_spectral');\n\n\n\n\n\n\n\n(a) The 'BrBG' color scale from ColorBrewer\n\n\n\n\n\n\n\n(b) Reversed 'BrBG_r' color scale\n\n\n\n\n\n\n\n(c) The 'nipy_spectral' color scale from matplotlib\n\n\n\n\nFigure 8.9: Symbology in a static map of a raster, with rasterio.plot.show\n\n\n\nUnfortunately, there is no built-in option to display a legend in rasterio.plot.show. The following workaround, going back to matplotlib methods, can be used to acheive it instead (Figure 8.10):\n\nfig, ax = plt.subplots(figsize=(5, 5))\ni = ax.imshow(nz_elev.read()[0], cmap='BrBG')\nrasterio.plot.show(nz_elev, cmap='BrBG', ax=ax);\nfig.colorbar(i, ax=ax);\n\n\n\n\nFigure 8.10: Adding a legend in rasterio.plot.show\n\n\n\n\n\n\n8.3.4 Layers\nYou can combine the raster and vector plotting methods shown above into a single visualisation with multiple layers, which we already used earlier when explaining masking and cropping (Figure 5.1). For example, Figure 8.11 demonstrated plotting a raster with increasingly complicated additions:\n\nThe left panel shows a raster (New Zealand elevation) and a vector layer (New Zealand administrative division)\nThe center panel shows the raster with a buffer of 22.2 \\(km\\) around the dissolved administrative borders, representing New Zealand’s territorial waters (see Section 3.4.6)\nThe right panel shows the raster with two vector layers: the territorial waters (in red) and elevation measurement points (in yellow)\n\n\n# Raster + vector layer\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\nnz.to_crs(nz_elev.crs).plot(ax=ax, facecolor='none', edgecolor='red');\n\n# Raster + computed vector layer\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\ngpd.GeoSeries(nz.unary_union, crs=nz.crs) \\\n    .to_crs(nz_elev.crs) \\\n    .buffer(22200) \\\n    .boundary \\\n    .plot(ax=ax, color='red');\n\n# Raster + two vector layers\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\ngpd.GeoSeries(nz.unary_union, crs=nz.crs) \\\n    .to_crs(nz_elev.crs) \\\n    .buffer(22200) \\\n    .exterior \\\n    .plot(ax=ax, color='red')\nnz_height.to_crs(nz_elev.crs).plot(ax=ax, color='yellow');\n\n\n\n\n\n\n\n(a) Raster + vector layer\n\n\n\n\n\n\n\n(b) Raster + computed vector layer\n\n\n\n\n\n\n\n(c) Raster + two vector layers\n\n\n\n\nFigure 8.11: Combining a raster and vector layers in the same plot\n\n\n\n\n\n8.3.5 Basemaps\n\nnzw = nz.to_crs(epsg=3857)\n\nFigure 8.12:\n\n# Default basemap\nfig, ax = plt.subplots(figsize=(7, 7))\nax = nzw.plot(color='none', edgecolor='k', ax=ax)\nctx.add_basemap(ax);\n\n# Specific basemap\nfig, ax = plt.subplots(figsize=(7, 7))\nax = nzw.plot(column='Median_income', legend=True, alpha=0.5, ax=ax)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron);\n\n\n\n\n\n\n\n(a) Default basemap from contextily\n\n\n\n\n\n\n\n(b) Custom basemap (CartoDB Positron)\n\n\n\n\nFigure 8.12: Adding a basemap to a static map\n\n\n\n\n\n8.3.6 Faceted maps\nTo complete…\n\n\n8.3.7 Exporting static maps\nStatic maps can be exported to a file using the matplotlib.pyplot.savefig function. For example, the following code section recreates Figure 7.5 (see previous Chapter), but this time the last expression saves the image to a JPG image named plot_geopandas.jpg:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\ntanzania.plot(ax=axes[0], color='lightgrey', edgecolor='grey')\ntanzania_neigh.plot(ax=axes[1], color='lightgrey', edgecolor='grey')\ntanzania_buf.plot(ax=axes[1], color='none', edgecolor='red')\naxes[0].set_title('where')\naxes[1].set_title('mask')\ntanzania.apply(lambda x: axes[0].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\ntanzania_neigh.apply(lambda x: axes[1].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\nplt.savefig('output/plot_geopandas.jpg')\n\nFigures with rasters can be exported exactly the same way. For example, the following code section (Section 8.3.4) creates an image of a raster and a vector layer, which is then exported to a file named plot_rasterio.jpg:\n\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\nnz.to_crs(nz_elev.crs).plot(ax=ax, facecolor='none', edgecolor='r');\nplt.savefig('output/plot_rasterio.jpg')\n\nImage file properties can be controlled through the plt.subplots and plt.savefig parameters. For example, the following code section exports the same raster plot to a file named plot_rasterio2.svg, which has different dimensions (width = 5 \\(in\\), height = 7 \\(in\\)), a different format (SVG), and different resolution (300 \\(DPI\\):)\n\nfig, ax = plt.subplots(figsize=(5, 7))\nrasterio.plot.show(nz_elev, ax=ax)\nnz.to_crs(nz_elev.crs).plot(ax=ax, facecolor='none', edgecolor='r');\nplt.savefig('output/plot_rasterio2.svg', dpi=300)"
  },
  {
    "objectID": "09-mapping.html#interactive-maps",
    "href": "09-mapping.html#interactive-maps",
    "title": "8  Making maps with Python",
    "section": "8.4 Interactive maps",
    "text": "8.4 Interactive maps\n\n8.4.1 Minimal example\nAn interactive map of a GeoSeries or GeoDataFrame can be created with .explore (Section 1.2.2). Here is a minimal example:\n\nnz.explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 8.13: Minimal example of an interactive vector layer plot with .explore\n\n\n\n\n\n8.4.2 Layers\nTo display multiple layers, one on top of another, with .explore, use the m argument, which stands for the previous map (Figure 8.14):\n\nm = nz.explore()\nnz_height.explore(m=m, color='red')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 8.14: Displaying multiple layers in an interactive map with .explore\n\n\n\n\n\n8.4.3 Symbology\nTo complete…\n\n\n8.4.4 Using other basemaps\nTo complete…\n\n\n8.4.5 Controls\nTo complete…\n\n\n8.4.6 Publishing interactive maps\nTo complete…\n\n…"
  },
  {
    "objectID": "09-mapping.html#exercises",
    "href": "09-mapping.html#exercises",
    "title": "8  Making maps with Python",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises"
  }
]